% Chapter 3. Estimation, variability and confidence ...............................................
\chapter{Estimation, variability and confidence}
\label{chapter3}

\epigraph{\textit{Numbers have an important story to tell, if given a voice.}}{â€” Florence Nightingale}

In the previous chapters, we described data using summary statistics and introduced probability models to represent uncertainty. In this chapter, we connect these two perspectives. Estimation is the process through which data are used to learn about unknown features of a population, while probability provides the language to quantify how reliable such learning is.

\medskip

A crucial distinction must be made between \textit{prediction} and \textit{inference}. Prediction focuses on forecasting future observations, whereas inference aims to draw conclusions about underlying parameters or mechanisms that generate the data. Estimation lies at the heart of inference: it transforms random samples into numerical statements about unknown quantities.

\medskip

Because data are inherently variable, different samples lead to different estimates. Understanding how estimates behave across repeated samples is therefore essential. The main goal of this chapter is to explain how probability theory allows us to quantify this variability and to construct principled measures of uncertainty such as confidence intervals.

% The Law of Large Numbers ........................................................................
\section{The Law of Large Numbers}

The Law of Large Numbers formalizes the intuitive idea that averages stabilize when more data are collected. While individual observations may be highly variable, their average becomes increasingly predictable as the sample size grows.

\medskip

Historically, this result was first articulated by Jacob Bernoulli in the early eighteenth century. It provided the first rigorous justification for interpreting probability as long-run relative frequency and established a bridge between theoretical probability and empirical observation.

\medskip

Let $X_1,X_2,\dots$ be independent and identically distributed random variables with expected value $\mu$. The Law of Large Numbers states that the sample mean
\begin{equation}
\bar{X}_n = \frac{1}{n}\sum_{i=1}^n X_i
\end{equation}
converges to $\mu$ as the sample size $n$ increases.

\medskip

\textit{Example.} When repeatedly tossing a fair coin, individual outcomes are unpredictable, but the proportion of heads approaches $1/2$ as the number of tosses grows.

% The Central Limit Theorem .......................................................................
\section{The Central Limit Theorem}

While the Law of Large Numbers explains where averages converge, it does not describe how they fluctuate around their limiting value. The Central Limit Theorem answers this question by describing the distribution of fluctuations.

\medskip

One of the most remarkable results in probability theory, the Central Limit Theorem explains why the Gaussian distribution appears so frequently in statistical practice. It shows that many different random processes give rise to approximately normal behavior when aggregated.

\medskip

Let $X_1,\dots,X_n$ be independent and identically distributed with mean $\mu$ and variance $\sigma^2$. The Central Limit Theorem states that the standardized sample mean
\begin{equation}
\frac{\bar{X}_n - \mu}{\sigma/\sqrt{n}}
\end{equation}
approaches a standard normal distribution as $n$ becomes large.

\medskip

\textit{Example.} Even if individual measurements are skewed, the distribution of their average over many observations is often approximately Gaussian.

% Bias, variance and Mean Squared Errors ..........................................................
\section{Bias, variance and Mean Squared Error}

An estimator is a rule that assigns a numerical value to an unknown parameter based on observed data. Because estimators depend on random samples, they are themselves random variables.

\medskip

The \textit{bias} of an estimator measures systematic error: it quantifies whether the estimator tends to overestimate or underestimate the true parameter. The \textit{variance} measures how much the estimator fluctuates from sample to sample.

\medskip

If $\hat{\theta}$ is an estimator of a parameter $\theta$, its bias is defined as
\begin{equation}
\operatorname{Bias}(\hat{\theta}) = \mathbb{E}[\hat{\theta}] - \theta,
\end{equation}
and its variance is
\begin{equation}
\operatorname{Var}(\hat{\theta}).
\end{equation}

\medskip

A common way to combine these two sources of error is the mean squared error (MSE):
\begin{equation}
\operatorname{MSE}(\hat{\theta}) = \mathbb{E}\!\left[(\hat{\theta}-\theta)^2\right]
= \operatorname{Var}(\hat{\theta}) + \operatorname{Bias}(\hat{\theta})^2.
\end{equation}

\medskip

\textit{Example.} The sample mean is an unbiased estimator of the population mean. Increasing the sample size reduces its variance, making the estimate more precise.

% Confidence intervals and critical regions .......................................................
\section{Confidence intervals and critical regions}

Point estimates summarize data with a single number, but they do not convey uncertainty. Confidence intervals address this limitation by providing a range of plausible values for an unknown parameter.

\medskip

A confidence interval is constructed so that, in repeated sampling, it contains the true parameter a fixed proportion of the time. This proportion is called the confidence level and is typically expressed as a percentage.

\medskip

For a population with mean $\mu$ and known variance $\sigma^2$, an approximate $100(1-\alpha)\%$ confidence interval for $\mu$ is given by
\begin{equation}
\bar{X}_n \pm z_{\alpha/2}\frac{\sigma}{\sqrt{n}},
\end{equation}
where $z_{\alpha/2}$ is a quantile of the standard normal distribution.

\medskip

Confidence intervals are closely related to hypothesis testing. The set of parameter values not rejected by a statistical test forms a confidence region. This duality provides a unified framework for estimation and decision-making.

\medskip

\textit{Example.} A $95\%$ confidence interval for the mean expresses the range of values that are consistent with the observed data under repeated sampling.



% Exercises ............................................................................................................

\newpage

\subsection*{Exercises}

\textbf{1.} Exercise [...].\\

\textbf{2.} Exercise [...].\\

\textbf{3.} Exercise [...].\\
