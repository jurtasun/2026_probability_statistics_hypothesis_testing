% Chapter - Introduction to conditional probability ...................................................................
\chapter{Modeling, dependency and correlation}

\epigraph{\textit{The theory of probabilities is at bottom nothing but common sense reduced to calculation.}}{— Pierre-Simon Laplace}

% \section{Introduction and Philosophy}
% Statistical models provide simplified representations of how variables relate while acknowledging randomness.
% The philosophical idea traces back to Gauss (1809), who introduced probabilistic error models for astronomical observations, and to Fisher (1922), who gave modelling a modern parametric interpretation.

% \subsection*{Mathematical Formulation}
% We view observations as realizations of random variables linked by
% \[
% Y = f(X) + \varepsilon,
% \]
% where $\varepsilon$ captures unexplained variation.

% \subsection*{Numerical Example}
% Suppose we observe $(X,Y) = (2,70),(5,85)$. The upward trend suggests $f$ is increasing, motivating a simple linear model.

% \subsection*{Historical Note}
% Gauss’ work (\emph{Theoria Motus}, 1809) introduced the normal error model, while Legendre (1805) independently formulated least squares.
% Fisher (1922, \emph{Phil. Trans. Royal Society A}) established the parametric likelihood framework, shaping modern modelling philosophy.

% \section{Linear Models in One Dimension}
% A linear model assumes that the expected value of $Y$ depends linearly on $X$.

% \subsection*{Mathematical Formulation}
% \[
% Y_i = \beta_0 + \beta_1 X_i + \varepsilon_i,\qquad i=1,\dots,n.
% \]

% \subsection*{Numerical Example}
% Given $(1,3)$ and $(3,7)$,
% \[
% \beta_1 = \frac{7-3}{3-1} = 2,\qquad
% \beta_0 = 3 - 2\cdot 1 = 1.
% \]
% Thus $\widehat{Y} = 1 + 2X$.

% \subsection*{Historical Note}
% Linear regression emerged in the biometrical work of Francis Galton (1886), who coined “regression toward mediocrity.”  
% Karl Pearson formalized correlation and regression mathematically (\emph{Phil. Trans. R. Soc.}, 1896).

% \section{Linear Models in Matrix Form}
% Matrix notation provides a unified and scalable representation for multivariate modelling.

% \subsection*{Mathematical Formulation}
% \[
% \mathbf{Y} = X\beta + \varepsilon,\qquad
% X\in\mathbb{R}^{n\times p}.
% \]

% \subsection*{Numerical Example}
% With
% \[
% X = \begin{pmatrix}
% 1 & 2\\
% 1 & 5
% \end{pmatrix},
% \qquad
% \beta = \begin{pmatrix}1\\2\end{pmatrix},
% \]
% we obtain predictions
% \[
% X\beta = \begin{pmatrix}5\\11\end{pmatrix}.
% \]

% \subsection*{Historical Note}
% Matrix-based linear modelling was systematized in the mid-20th century, notably in the work of C. R. Rao (1945, \emph{Bulletin of the Calcutta Mathematical Society}), who developed the Cramér–Rao bound and unified estimation in linear models.

% \section{Estimation and Inference}
% Model estimation chooses parameter values that best describe the data; inference quantifies uncertainty around these estimates.

% \subsection*{Mathematical Formulation}
% The ordinary least squares estimator is
% \[
% \widehat{\beta} = (X^\top X)^{-1}X^\top\mathbf{Y},
% \]
% with residuals $\widehat{\varepsilon}=\mathbf{Y}-X\widehat{\beta}$.
% Under the normal-error model,
% \[
% \widehat{\beta}\sim\mathcal{N}\bigl(\beta,\sigma^2(X^\top X)^{-1}\bigr).
% \]

% \subsection*{Numerical Example}
% For
% \[
% X=\begin{pmatrix}
% 1&1\\
% 1&2\\
% 1&3
% \end{pmatrix},
% \qquad
% Y=\begin{pmatrix}2\\3\\5\end{pmatrix},
% \]
% one obtains
% \[
% \widehat{\beta}=
% \begin{pmatrix}0.333\\1.5\end{pmatrix},
% \]
% so $\widehat{Y}=0.333+1.5X$.

% \subsection*{Historical Note}
% Fisher (1922, 1925) connected least squares, likelihood, and sampling distributions, establishing the foundations of modern inference.
% Neyman (1937) formalized confidence intervals as frequentist procedures, contributing to philosophical debates on inference that continue today.


% \newpage

% \subsection*{Exercises}

% \textbf{1.} Exercise [...].\\

% \textbf{2.} Exercise [...].\\

% \textbf{3.} Exercise [...].\\

% \newpage

% \subsection*{Solutions}

% \textbf{1.} Solution [...].\\

% \textbf{2.} Solution [...].\\

% \textbf{3.} Solution [...].\\