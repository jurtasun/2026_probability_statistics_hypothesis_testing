% Chapter - Introduction to conditional probability ...................................................................
\chapter{Modeling, dependency and correlation}
\label{chapter5}

\epigraph{\textit{The theory of probabilities is at bottom nothing but common sense reduced to calculation.}}{— Pierre-Simon Laplace}

\section{Introduction and Philosophy}

Matrix-based linear modelling was systematized in the mid-20th century, notably in the work of C. R. Rao (1945, \emph{Bulletin of the Calcutta Mathematical Society}), who developed the Cramér–Rao bound and unified estimation in linear models.

\section{Estimation and Inference}
Model estimation chooses parameter values that best describe the data; inference quantifies uncertainty around these estimates.

\subsection*{Mathematical Formulation}
The ordinary least squares estimator is
\[
\widehat{\beta} = (X^\top X)^{-1}X^\top\mathbf{Y},
\]
with residuals $\widehat{\varepsilon}=\mathbf{Y}-X\widehat{\beta}$.
Under the normal-error model,
\[
\widehat{\beta}\sim\mathcal{N}\bigl(\beta,\sigma^2(X^\top X)^{-1}\bigr).
\]

\subsection*{Numerical Example}
For
\[
X=\begin{pmatrix}
1&1\\
1&2\\
1&3
\end{pmatrix},
\qquad
Y=\begin{pmatrix}2\\3\\5\end{pmatrix},
\]
one obtains
\[
\widehat{\beta}=
\begin{pmatrix}0.333\\1.5\end{pmatrix},
\]
so $\widehat{Y}=0.333+1.5X$.

\newpage

\subsection*{Exercises}

\textbf{1.} Exercise [...].\\

\textbf{2.} Exercise [...].\\

\textbf{3.} Exercise [...].\\

\newpage

\subsection*{Solutions}

\textbf{1.} Solution [...].\\

\textbf{2.} Solution [...].\\

\textbf{3.} Solution [...].\\