% Chapter - Modelling, dependency and correlation ...................................................................
\chapter{Modelling, dependency, and correlation}
\label{chapter5}

\epigraph{\textit{Numbers have an important story to tell, if given a voice.}}{— Florence Nightingale}

% In earlier chapters we focused on describing data and estimating unknown quantities, while in this chapter we move toward understanding how quantities relate to one another and how uncertainty enters these relationships through modelling, shifting attention from isolated variables to structured relationships among them. 

% \medskip

% A statistical model is not a literal description of reality but a simplified representation designed to capture essential patterns while acknowledging uncertainty and variability, allowing us to reason systematically even when information is incomplete or noisy. 

% \medskip

% Probability provides a language for uncertainty, while statistics provides tools for learning from data, and modelling forms the conceptual bridge between them by combining assumptions, randomness, and observed evidence into a coherent framework, an idea that matured during the late nineteenth and early twentieth centuries with the work of Galton, Pearson, and Fisher \cite{pearson_1895_evolution,fisher_1925_statistical}. 



% Section .............................................................................................................
\section{Models, variables, and functions}

% A fundamental mathematical idea underlying statistical modelling is the notion of a \textit{function}, which describes how one quantity depends on another and provides a structured way to translate inputs into outputs.

% \medskip

% In its simplest form, a function associates an input variable with an output variable, expressing how changes in one are reflected in changes of the other in a consistent and interpretable manner. 

% \medskip

% In statistics, we often write this relationship symbolically as
% \begin{equation}
%     Y = f(X),
% \end{equation}
% where \(X\) represents an input variable and \(Y\) an output variable. 

% \medskip

% When several inputs influence an outcome, this idea naturally extends to multiple variables, which can be written abstractly as
% \begin{equation}
%     Y = f(X_1, X_2, \dots, X_p).
% \end{equation}

% \medskip

% Because real-world systems are influenced by many unobserved factors, this relationship is rarely exact and must be understood as an approximation rather than a deterministic rule. 

% \medskip

% The explicit use of mathematical functions to represent empirical relationships developed gradually, particularly in astronomy and biology, where Gauss introduced systematic modelling of observational error and Pearson formalized functional relationships between variables \cite{gauss_1809_astronomy,pearson_1895_evolution}. 



% Section .............................................................................................................
\section{Error, noise, and uncertainty}

% The discrepancy between what a model predicts and what is actually observed is called \textit{error}, and it arises naturally even when a model is well chosen and carefully constructed. 

% \medskip

% Rather than viewing error as a failure, statistics treats it as an intrinsic feature of data arising from measurement limitations, natural variability, and incomplete information about complex systems. 

% \medskip

% To reflect this uncertainty, models are commonly written in the form
% \begin{equation}
%     Y = f(X) + \varepsilon,
% \end{equation}
% where the term \(\varepsilon\) represents random variation not explained by the model. 

% \medskip

% This error term is typically assumed to fluctuate around zero,
% \begin{equation}
%     \mathbb{E}[\varepsilon] = 0,
% \end{equation}
% expressing the idea that errors do not systematically favor over- or under-prediction. 

% \medskip

% The separation between systematic structure and random error originates in Gauss’s theory of errors and was later refined by Fisher, who emphasized probabilistic noise as a central object of inference rather than a nuisance \cite{gauss_1809_astronomy,fisher_1925_statistical}. 



% Section .............................................................................................................
\section{Estimation and inference}

% Once a model structure has been chosen, its unknown components must be determined from data, a process known as \textit{estimation}, which translates observed information into numerical summaries. 

% \medskip

% Estimation produces numerical values, but these values are themselves uncertain because they depend on finite and noisy observations rather than complete knowledge of the underlying process. 

% \medskip

% For example, estimating a parameter \(\theta\) from data produces a value \(\hat{\theta}\),
% \begin{equation}
%     \hat{\theta} = \hat{\theta}(X_1, \dots, X_n),
% \end{equation}
% highlighting that estimates are functions of the observed sample. 

% \medskip

% Statistical inference addresses this uncertainty by providing methods to assess reliability, variability, and the strength of conclusions drawn from estimated models. 

% \medskip

% Competing interpretations of inference were developed in the early twentieth century by Fisher, Neyman, and Pearson, whose debates shaped modern views on estimation, testing, and scientific reasoning \cite{fisher_1925_statistical,neyman_pearson_1933_efficient}. 



% Section .............................................................................................................
\section{Linear dependence and correlation}

% The simplest and most widely used models assume a \textit{linear} relationship between variables, in which changes in one variable are associated with proportional changes in another over a meaningful range. 

% \medskip

% A linear model with one input variable can be written as
% \begin{equation}
%     Y = a + bX + \varepsilon,
% \end{equation}
% where the constants \(a\) and \(b\) describe the intercept and slope of the relationship. 

% \medskip

% The slope \(b\) quantifies the expected change in \(Y\) associated with a one-unit change in \(X\),
% \begin{equation}
%     \Delta Y \approx b \, \Delta X,
% \end{equation}
% providing a direct interpretation of linear dependence. 

% \medskip

% Correlation provides a numerical summary of linear dependence, measuring the strength and direction of association without implying causation. 

% \medskip

% The concept of correlation was introduced by Galton and formalized by Pearson, who emphasized both its descriptive power and its potential for misuse when interpreted causally \cite{pearson_1895_evolution,spearman_1904_association}. 



% Section .............................................................................................................
\section{Non-linear dependence and Generalized Linear Models}

% Many relationships in nature and society are not linear, and forcing a linear model onto such data can obscure important structure and lead to misleading conclusions. 

% \medskip

% Non-linear models allow the function \(f\) to take more flexible forms, while still retaining a clear separation between systematic structure and random variation. 

% \medskip

% Symbolically, such models retain the same basic structure,
% \begin{equation}
%     Y = f(X) + \varepsilon,
% \end{equation}
% but differ in the choice of the function \(f\). 

% \medskip

% Generalized Linear Models reflect a historical movement toward flexible yet interpretable modelling frameworks grounded in probability theory, building on axiomatic foundations introduced by Kolmogorov \cite{kolmogorov_1933_foundations}. 



% % Exercises ............................................................................................................

% \newpage

% \subsection*{Exercises}

% \textbf{1.} Describe in words a situation where one quantity depends on another and identify possible sources of error.\\

% \textbf{2.} Explain the meaning of the term \(\varepsilon\) in a statistical model.\\

% \textbf{3.} Give an example of a real-world relationship that is unlikely to be linear.
