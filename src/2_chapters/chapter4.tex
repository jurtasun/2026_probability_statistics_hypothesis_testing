% Chapter4. Introduction to hypothesis testing ........................................................................
\chapter{Introduction to hypothesis testing}
\label{chapter4}

\epigraph{\textit{The object of statistical science is the reduction of data to relevant information.}}{— Ronald A. Fisher}

The term \textit{hypothesis testing} lies on top of the two pillars we have mentioned in previous chapters. From statistical analysis, as discussed in Chapter \ref{chapter1}, we will use sampling, estimators, and graphical summaries to describe data. From probability theory, as discussed in Chapter \ref{chapter2}, we will rely on probability distributions and expected values to model random variation and uncertainty. Through this chapter, we will mostly work in a \textit{parametric} setting, where data is assumed to follow a simple, smooth and well-behaved distribution—most often the Gaussian distribution. Under these assumptions, and supported by the Law of Large Numbers and the Central Limit Theorem, estimators such as the sample mean and variance provide reliable information about the true population parameters. Confidence intervals and critical regions, introduced in Chapter \ref{chapter3}, formalize this idea of reliability.

\medskip

Within this framework, we introduce the notion of a \textit{statistic}, or \textit{statistic test}: a numerical quantity computed from data, designed to measure how close our observations are to what we would expect under a given hypothesis. Statistical tests are built by comparing such statistics to their behavior under an assumed model. In the modern Neyman--Pearson framework, this comparison is summarized through the \textit{P-value}, which quantifies how unusual the observed data would be if the null hypothesis were true \cite{pearson_neyman_1933_efficient}.

\medskip

Let us begin with a brief historical note, often skipped. The ideas behind hypothesis testing did not emerge all at once. While the general notion of testing expectations against observations is very old, the mathematical tools required for modern statistical testing are relatively recent. Systematic use of estimators and probability models developed gradually during the nineteenth and early twentieth centuries, and the axiomatic foundations of probability were only formalized in 1933 with Kolmogorov's work \cite{kolmogorov_1933_grundbegriffe}. The first statistical tests were developed in the early twentieth century by researchers such as Pearson and Fisher, among many other, and were originally designed to measure discrepancies between data and theoretical expectations, not to support automatic accept-reject decisions. The formal structure of hypothesis testing, including null and alternative hypotheses, error rates, and P-values, was later introduced by Neyman and Pearson. Because these ideas developed in stages, their interpretation requires some care. Throughout this chapter, we will emphasize both the practical use of statistical tests and the assumptions on which they are based.

% Prediction vs inference revisited ...................................................................................
\section{Prediction vs inference revisted}

When formulating hypotheses about natural phenomena, it is important to recall the distinction between population and sampling. On the one hand, we consider an idealized and generally inaccessible population, characterized by true but unknown quantities such as the mean and variance. Mathematical prediction, in this context, refers to the computation of expected values—also known as statistical moments—defined for a random variable together with its probability distribution.

\medskip

Given a random variable, expected values (or moments) can be computed directly from its distribution.

\medskip

Population mean for a discrete random variable \(x\) with support \(\{x_i\}\)
\begin{equation}
    \mu = \mathbb{E}[x] = \sum_{i} x_i \, \mathbb{P}(x = x_i)
    \label{population_mean_discrete}
\end{equation}

Population variance for a discrete random variable \(x\)
\begin{equation}
    \sigma^2 = \mathbb{E}\!\left[(x - \mu)^2\right] = \sum_{i} (x_i - \mu)^2 \, \mathbb{P}(x = x_i)
    \label{population_var_discrete}
\end{equation}

Population mean for a continuous random variable \(x\) with density \(f(x)\)
\begin{equation}
    \mu = \mathbb{E}[x] = \int_{-\infty}^{\infty} x \, f(x)\, dx
    \label{population_mean_continuous}
\end{equation}

Population variance for a continuous random variable \(x\)
\begin{equation}
    \sigma^2 = \mathbb{E}\!\left[(x - \mu)^2\right] = \int_{-\infty}^{\infty} (x - \mu)^2 \, f(x)\, dx
    \label{population_var_continuous}
\end{equation}

\medskip

Hypotheses are therefore always formulated in terms of mathematical predictions about population-level parameters. For example, under Newtonian mechanics, a hypothesis may consist of Newton’s second law, from which predictions can be made about the position of a falling object as a function of time. In biology, a hypothesis may concern the effect of a gene on a disease or on stress response, formulated in terms of expected expression levels or counts. In epidemiology, one may hypothesize a relationship between smoking prevalence and lung cancer risk. In all such cases, hypotheses concern population properties, while access to them is obtained only through finite samples of observations, collectively referred to as \textit{data}.

\medskip

Observed sample mean for a sample \(\mathcal{X} = \{x_1, x_2, \dots, x_n\}\)
\begin{equation}
    \bar{x} = \frac{1}{n}\sum_{i = 1}^{n} x_i
    \label{sample_mean}
\end{equation}

Observed sample variance for the same sample
\begin{equation}
    s^2 = \frac{1}{n - 1}\sum_{i = 1}^{n} (x_i - \bar{x})^2
    \label{sample_variance}
\end{equation}

\medskip

Hypothesis testing formalizes this comparison between population-level predictions and sample-based estimates by quantifying how compatible the observed data are with a hypothesized model.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\textwidth]{5_figures/chapter1/prediction_vs_inference.png}
    \caption{Representation of the predictive (from theory, or model, to experimental verification) and inferential (from data, measurement, observation to underlying truth) approaches to natural phenomena. As an example of the predictive branch of mathematics dealing with uncertainty we would find the theory of probability, while the descriptive way of addressing the same problem is normally regarded as statistical inference.}
    \label{fig:prediction_vs_inference2}
\end{figure}

% General approach to hypothesis testing ..............................................................................
\section{General approach to hypothesis testing}

When dealing with hypotheses, predictions, experiments, and data, there exist many approaches and formulations, as many as instruments, scales, and fields of study. These approaches change from one field to another, and they also change over time. Our very ideas of hypothesis, prediction, measurement, and scientific law have evolved historically. Nowadays, when people refer to \textit{hypothesis testing}, they usually mean a very specific approach: an almost algorithmic set of rules applied broadly to inference and data analysis problems. In this chapter, we will define such an approach as the \textit{modern} or \textit{general} approach to hypothesis testing. It assumes basic notions of probability theory, randomness, and probability distributions, together with statistical concepts such as estimators and sample-based descriptions. The core ideas of statistical tests, p-values, and significance that we discuss here are relatively recent, tracing back to the work of Pearson, Fisher, and Neyman in the early twentieth century.

\medskip

The general approach to hypothesis testing can be summarized in the following steps:

\begin{itemize}
    \item \textbf{Formulate hypotheses.} One specifies a \textit{null hypothesis} \(H_0\), representing the expected or reference case, and an \textit{alternative hypothesis} \(H_1\), representing a departure from \(H_0\) that would be considered surprising. These hypotheses are always formulated in terms of true population parameters, and are often expressed through expected values or related quantities discussed in Chapter~\ref{chapter2}.
    \item \textbf{Experiment, measurement, observation.} Any process—regardless of instrumentation or field of study—that produces measurements or observations, resulting in one or more samples of data.
    \item \textbf{Compute a statistic or test statistic.} From the observed data, one computes an informative quantity, which may be a simple estimator such as the sample mean or variance, or a more elaborate statistic designed to quantify how far the observed data deviate from what is expected under \(H_0\).
    \item \textbf{Compute a p-value.} The p-value is the probability that, assuming the null hypothesis and its associated population parameters are true, one would obtain a value of the statistic at least as extreme as the one observed.
    \item \textbf{Interpret the result.} The p-value is compared to a chosen significance level, leading to a decision or conclusion, commonly phrased as rejecting or not rejecting the null hypothesis.
\end{itemize}

\medskip

A few remarks are in order regarding this general roadmap. A statistic may be as simple as an estimator, such as the sample mean, or a more abstract quantity derived from the data. Fisher originally introduced the p-value as a continuous measure of evidence against the null hypothesis, rather than as a strict decision rule. The widespread practice of fixed significance thresholds and accept--reject decisions reflects a later synthesis of Fisher’s ideas with the Neyman--Pearson framework. As a result, the modern approach to hypothesis testing combines elements of both traditions, a point we will return to later in this chapter. \textit{Warning.} A p-value does not measure the probability that a hypothesis is true or false, but rather how compatible the observed data are with the null hypothesis under the assumed model.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{5_figures/chapter1/estimators.png}
    \caption{Representation of the \textit{true} population mean $\mu$, in black, and the observed \textit{sample} mean $\bar{x}$. The true mean is and ideal and unaccessible quantity, while the sample mean can be computed as an estimator of the finite sample.}
    \label{fig:estimators2}
\end{figure}

% Statistic tests: some examples ......................................................................................
\section{Statistic tests: common examples}

% Subsection ..........................................................................................................
\subsection{One sample \(t\)-test: Compare sample mean with hypothesized value}

The \(t\)-test is arguably the simplest example of a statistical test we will discuss. It was developed in 1908 by William S.~Gosset, a statistician working at the Guinness factory in Dublin, who was trying to accurately estimate the error of the mean when the population variance is unknown, as part of the brewing process. Due to his affiliation with the Guinness company, he was not allowed to publicly share his work, and therefore submitted it to the journal \textit{Biometrika} under the pseudonym \textit{Student}. For this reason, it remains nowadays known as the \textit{Student’s \(t\)-test} \cite{student_1908_mean}.

\medskip

The test begins by formulating a null hypothesis about the true population mean, \textit{prior to any sampling or data collection}. Normally, the null hypothesis is written as \textit{the true population mean is expected to take the value \(\mu\)}. It is important here to stop and think carefully about what physical quantity we are actually going to measure. Recall that such a quantity, inaccessible in principle, can be either predicted through the computation of an expected value or estimated from data, as discussed in Chapter~\ref{chapter2}.

\medskip

Now take a series of observations or measurements and group them into a sample \(\chi = \{x_1, x_2, \dots, x_n\}\). From this sample, we can compute the sample mean \(\bar{x}\) as an estimator of the true population mean \(\mu\), and the sample standard deviation \(s\) as an estimator of the true population standard deviation \(\sigma\).

\medskip

Given these three elements (the expected value \(\mu\), given by our null hypothesis \(H_0\), together with the sample mean \(\bar{x}\) and the sample standard deviation \(s\), obtained from data) we can compute the \(t\)-\textit{statistic}, or \(t\)-\textit{test statistic}, defined as
\begin{equation}
    t = \dfrac{\bar{x} - \mu}{s/\sqrt{n}} \; .
    \label{t_statistic_one_sample}
\end{equation}
Let us pause for a moment to examine this quantity. As the sample mean \(\bar{x}\) approaches the expected value \(\mu\), the \(t\)-statistic tends to zero. It was designed precisely for this purpose: to quantify how different—or how similar—our observed data are with respect to the hypothesized value, so that in the ideal case \(\bar{x} \to \mu\), we have \(t \to 0\).

\medskip

It is important to note that \(t\) is computed from a set of random observations. If we repeat the same measurements using a different sample, or at a different time, or under different conditions, we may obtain different values of \(\bar{x}\) and \(s\), and therefore a different value of \(t\). This means that \textit{\(t\) is itself a real-valued random variable}, and it follows some probability distribution. The mathematical form of this distribution, introduced in Gosset’s original work, is known as the \textit{Student’s \(t\)-distribution},
\begin{equation}
    f(t; \nu) = \dfrac{\Gamma\!\left(\dfrac{\nu + 1}{2}\right)}{\sqrt{\pi\nu}\,\Gamma\!\left(\dfrac{\nu}{2}\right)} \left( 1 + \dfrac{t^2}{\nu} \right)^{-(\nu + 1)/2} \; ,
    \label{t_distribution}
\end{equation}
where the parameter \(\nu\) is referred to as the \textit{degrees of freedom} and is simply related to the sample size by \(\nu = n - 1\). Some textbooks write the statistic as \(t_\nu\), which is standard notation, but here we reserve the symbol \(t\) for the statistic itself and \(f(t;\nu)\) for its distribution. It can be shown that as the degrees of freedom increase, the \(t\)-distribution converges to the standard normal distribution, written as \(f(t;\nu) \to \mathcal{N}(0,1)\) as \(\nu \to \infty\); the explicit proof is beyond the scope of this course.

\medskip

We now arrive at the final step: the computation of the p-value. Returning to Fisher’s definition of the p-value as \textit{the probability of obtaining a value at least as extreme as the one observed for the statistic, assuming the null hypothesis \(H_0\) is true}, we see that once the distribution of the \(t\)-statistic is known, the p-value can be computed by evaluating an appropriate cumulative probability.

\medskip

If we ask for the probability of obtaining a value strictly greater (or strictly smaller) than the observed statistic, \(\mathbb{P}(t \ge t_{\text{obs}})\), we compute the integral of one tail of the distribution; this is called a \textit{one-sided} (or one-tailed) p-value. If instead we ask for the probability of obtaining a value more extreme than the observed one, regardless of direction, \(\mathbb{P}(|t| \ge |t_{\text{obs}}|)\), we integrate both tails of the distribution; this is called a \textit{two-sided} (or two-tailed) p-value. Due to the symmetry of the \(t\)-distribution, the two-sided p-value is simply twice the one-sided p-value.
\[
    p_{\text{one-sided}} = \mathbb{P}(t \ge t_{\text{obs}}) = \int_{t_{\text{obs}}}^{\infty} f(t; \nu)\, dt \; ,
\]
\[
    p_{\text{two-sided}} = \mathbb{P}(|t| \ge |t_{\text{obs}}|) = 2 \int_{|t_{\text{obs}}|}^{\infty} f(t; \nu)\, dt \; .
\]

For a review of cumulative probabilities and probability integrals, see Chapter~\ref{chapter2}, and for a brief refresher on integral calculus, consult Appendix~\ref{appendix3}. In practice, numerical computation of p-values is typically carried out using statistical software, which evaluates both the test statistic and the corresponding tail probabilities; common examples include the \texttt{scipy} library in \texttt{Python} and the \texttt{stats} package in \texttt{R}.

\medskip

\textbf{Example.} For a sample of size \(n = 10\), observed mean \(\bar{x} = 5.2\), sample standard deviation \(s = 1.0\), and hypothesized value \(\mu = 5\), we obtain
\[
    t_{\text{obs}} = \frac{5.2 - 5}{1/\sqrt{10}} \approx 0.63.
\]
With \(\nu = 9\) degrees of freedom, the corresponding two-sided p-value is approximately \(p \approx 0.54\).

\begin{figure}[ht]

    \centering

    % Subfigure 1 .............................................................
    \begin{subfigure}{0.8\textwidth}
        \centering
        \includegraphics[width=0.8\textwidth]{5_figures/chapter4/t_distribution.png}
        \caption{The Student's t distribution of the t-statistic, given different values of the degrees of freedom $\nu$.}
        \label{fig:t_distribution_dof}
    \end{subfigure}

    \vspace{1.2em}

    % Subfigure 2 .............................................................
    \begin{subfigure}{0.8\textwidth}
        \centering
        \includegraphics[width=0.8\textwidth]{5_figures/chapter4/t_test_1_sample_p_one_tailed.png}
        \caption{Representation of the 1-sided P-value, computed as the integral of the right tail of the t distribution.}
        \label{fig:t_distribution_1_sided_p}
    \end{subfigure}

    \vspace{1.2em}

    % Subfigure 3 .............................................................
    \begin{subfigure}{0.8\textwidth}
        \centering
        \includegraphics[width=0.8\textwidth]{5_figures/chapter4/t_test_1_sample_p_two_tailed.png}
        \caption{Representation of the 2-sided P-value. Given the symmetry of the t-distribution, it can be obtained as double in size of the 1-sided integral.}
        \label{fig:t_distribution_2_sided_p}
    \end{subfigure}

    \label{fig:t_test_comparison}

\end{figure}

% Subsection ..........................................................................................................
\subsection{Two sample \(t\)-test: Compare sample means of two independent groups}

The two-sample \(t\)-test is an extension of the one-sample case, hence the general approach will be almost identical as the one discussed in previous section. It is used to test two independent samples, \(chi_1\) and \(\chi_2\), of lengths \(n_1\) and \(n_2\). The null hypothesis is still formulated about the true population means, as \textit{both observations come—are sampled from—the same distribution, with an expected true mean \(\mu\)}. Remember again that such quantity, unaccessible in theory, can be either fitted from previous data, or predicted through the computation of an expected value, as we discussed in Chapter \ref{chapter2}.

\medskip

Now take a series of measurements for both samples, and compute the sample means \(\bar{x}_1\), \(\bar{x}_2\), as estimators of the true population mea \(\mu\), and the sample variances \(s_1^2\), \(s_2^2\), as estimators of the true population variance \(\sigma^2\).

\medskip

In the same way we proceeded for the one-sample case, we combine the expectation given by our \(H_0\) and the estimators computed out of our data, into the \textit{two-sample \(t\)-statistic}, or \textit{two-sample \(t\)-statistic test}, defined as
\begin{equation}
    t = \dfrac{\bar{x}_1 - \bar{x}_2}{\sqrt{\dfrac{s_1^2}{n_1} + \dfrac{s_2^2}{n_2 }}} \; .
    \label{t_statistic_two_sample}
\end{equation}
The denominator is normally called \textit{pooled standard deviation}, and denoted by \(s_p\). We can see that this quantity behaves in the same way as the one-sample case, tending to zero, as the sample means of both groups tend to each other, \(t \rightarrow 0\) as \(\bar{x}_1 \rightarrow \bar{x}_2\),

\medskip

Again, \(t\) is a real-valued random variable, and it follows the same Student's \(t\)-distribution of eq. \eqref{t_distribution}. The only difference is that the degrees of freedom \(\nu\) are now obtained by combining the lengths of both samples \textit{\(\nu = n_1 + n_2 - 2\)}.

\medskip

The computation of the P-value, is identical to the one-sample case, as we just need to integrate the \(t\)-distribution. Again, given the symmetry of the \(t\)-distribution, the two-sided P-value is just double the size of the one-sided case. For a review of cumulative probabilities and integrating probability distributions, go back to Chapter \ref{chapter2}, and for a review on integral calculus and some warm-up examples, see Appendix \ref{appendix3}.

\medskip

\textbf{Example.} For sample of size \(n_1 = 10\), observed average \(\bar{x} = 5.2\), variance \(s = 1.0\), and \(\mu_0 = 5\), then
\[
    t_{\text{obs}} = \frac{5.2-5}{1/\sqrt{10}} \approx 0.63,
\]

Given this observed value, and the degrees of freedom \(\nu=9\), the two-sided p-value would be \(p\approx0.54\).

% Subsection ..........................................................................................................
\subsection{Fisher's \(F\) test: Compare variation of two independent groups}

The Fisher's variance-ratio test, or \(F\)-test for short, was introduced by Fisher in the 1920s and formally developed in his works \textit{Statistical methods for research workers} and \textit{The design of experiments}, in 1925 and 1935. The impact of Fisher's work, not only in statistics but also in evolutionary biology, experimental design, hypothesis tesing and mathematical modelling is credited still nowadays as one of the greatest among the twentieth century, by far \cite{fisher_1924_distribution, fisher_1925_statistical, fisher_1935_design}.

\medskip

The \(F\) test begins by formulating some null hypothesis about the true population variance, \textit{prior to any sampling or data collection}. Normally, the null hypothesis is simply written as \textit{both samples under study come from the same distribution, with true population variance \(\sigma^2\)}. As we did in previous cases, remember that such quantity is hypothesized value about the true population, and it can be either fitted from data, or predicted through the computation of an expected value, as we discussed in Chapter \ref{chapter2}.

\medskip

Now take a series of measurements for two independent samples \(\chi_1\), \(chi_2\) of sizes \(n_1\) and \(n_2\), compute the sample variances \(s_1^2\), \(s_2^2\), as estimators of the true population variances \(\sigma_1^2\) and \(\sigma_2^2\). Then the Fisher \(F\)-\textit{statistic}, or \(F\)-\textit{statistic test}, is defined just as the ratio
\begin{equation}
    F = \dfrac{s_1^2}{s_2^2} \; .
    \label{f_statistic_exact}
\end{equation}
Similarly to what happend in previous cases, we can notice that as the sample variances \(s_1^2\), \(s_2^2\) approach each other, the \(F\)-variable tends to one. It was designed for this precise purpose, to quantify how different—or similar—two independent groups are from each other, and in the ideal case \(s_1^2 \rightarrow s_2^2\), then \(F \rightarrow 1\). Recall the definition of the \(t\)-statistic, as here we can start noticing that, in general, statistic tests are normally defined such that, in the case of \(H_0\) being true, they reduce to a small, simple value.

\medskip

If we pay close attention, we can notice that unlike the \(t\)-test, where the null hypothesis was stated directly in terms of a \(\mu\) parameter, appearing explicitly in the definition of the \(t\)-statistic, there is no explicit trace of \(H_0\) in the definition of our \(F\), which is computed just as the ratio of two sample variances. There is a historical reason for this, that we will revisit further in this chapter, related to how the very idea of hypotheis, parameter and statistic test were used in Fisher's time, different from modern usage. As we will see, the classical \(F\)-test encodes the null hypothesis through the \textit{condition under which the ratio of \(F\) of sample variances follow a specific distribution}. In practice, this reflects the fact that the \(t\)-test is formulated around an explicit parameter hypothesis, whereas the \(F\)-test arose from Fisher's analysis of sampling distributions.

\medskip

Same as in the two previous examples, the \(F\)-statistic is obtained out of a set of random observations. If I repeat the same measurements in a different sample, or a different day, or under different conditions, they may lead to different values \(s_1\) and \(s_2\), hence producing a different \(F\). This means that \textit{\(F\) is a real-valued random variable itself}, and it will follow \textit{some} distribution. The mathematical definition of such distribution—or density—of the \(F\)-variable, as introduced in Fisher's [...], is called the Fisher's \textit{\(F\)-distribution},
\begin{equation}
    f(F; \nu_1, \nu_2) = \dfrac{1}{B \bigg( \dfrac{\nu_1}{2}, \dfrac{\nu_2}{2} \bigg)} \; \bigg( \dfrac{\nu_1}{\nu_2} \bigg)^{\frac{\nu_1}{2}}\; F^{\frac{\nu_1}{2} - 1} \;  \bigg( 1 + \dfrac{\nu_1}{\nu_2} F \bigg)^{-\frac{(\nu_1 + \nu_2)}{2}} \; ,
    \label{f_distribution}
\end{equation}
where the parameters \(\nu_1\), \(\nu_2\) represent the \textit{degrees of freedom}, and they are related to the length of the samples \(\nu_1 = n_1 - 1\), \(\nu_2 = n_2 - 1\). You may see that some textbooks and literature sources write it as \(F_{\nu_1, \nu_2}\), which is perfectly fine and common standard, but we prefere to use here the letter \(F\) just for the statistic, and \(f(F; \nu_1, \nu_2)\) for the distribution of \(F\) given \(\nu_1\) and \(\nu_2\) degrees of freedom, rather than referring to both elements with the same symbol. It can be demonstrated that as \(\nu_1,\nu_2 \to \infty\), \(f(t; \nu_1, \nu_2)\) concentrates at \(1\) and \(\log f(F; \nu_1, \nu_2)\) becomes approximately Gaussian. The explicit demonstration is out of the scope of this course, but it can be found at [...].

As we mentioned alread, it is under the null hypothesis \(H_0: \sigma_1^2 = \sigma_2^2\) that the \(F\)-statistic follows the Fisher distribution. Modern Wald and likelihood-ratio tests provide a unified parameter-based framework.

\medskip

Back to the computation of the P-value, given Fisher's definition, \textit{the probability of obtaining a value at least as extreme as the one observed for our statistic, asuming the expected value—or values—given by our null hypothesis \(H_0\) and our random data}, we just need to compute the cumulative probability—that is, the integral—of the \(F\) distribution. 

\medskip

If we ask for the probability of obtaining a value \textit{strictly greater or strictly smaller than the one we observed for our statistic}, \(\mathbb{P}(F \ge F_{\text{obs}})\), we just need to compute the integral of one of the distribution tales, and it give the one-sided P-value. But the two-sided case, the probability of obtaining a value \textit{more extreme, regardless of the direction} would require the integral of both tails, and given the asymmetry of the \(F\)-distribution, becomes a non-trivial task. The common formulation is written as follows
\[
    P_{\text{one-sided}} = \mathbb{P}(F \ge F_{\text{obs}}) = \int_{F_{\text{obs}}}^{\infty} f(F; \nu_1, \nu_2) \; df \; ,
\]

\[
    P_{\text{two-sided}} = 2 \min \!\left\{ \int_{0}^{F_{\text{obs}}} f(F; \nu_1, \nu_2) \; df, \; \int_{F_{\text{obs}}}^{\infty} f(F; \nu_1, \nu_2) \; df \right\}.
\]

For a review of cumulative probabilities and integrating probability distributions, go back to Chapter \ref{chapter2}, and for a review on integral calculus and some warm-up examples, see Appendix \ref{appendix3}. If this feels a bit heavy, this is where computer softwares and libraries become particularly useful, as they not only implement the calculation of the statistic but the numerical integration of such distribution, yealding to the P-value without the need for manual integral calculus. Examples of these will be \texttt{scipy} library of \texttt{Python}, and the \texttt{stats} package of \texttt{R}, among many others [...].

\medskip

\textbf{Example.}
If \(n_1=n_2=10\), \(S_1^2=4\), \(S_2^2=2\), then
\[
f_{\text{obs}}=2,\qquad \nu_1=\nu_2=9,
\]
yielding a one-sided p-value \(p\approx0.12\).

\begin{figure}[ht]

    \centering

    % Subfigure 1 .............................................................
    \begin{subfigure}{0.8\textwidth}
        \centering
        \includegraphics[width=0.8\textwidth]{5_figures/chapter4/f_distribution.png}
        \caption{The Fisher's \(F\) distribution of the \(F\)-statistic, given different values of the degrees of freedom $\nu$.}
        \label{fig:f_distribution_dof}
    \end{subfigure}

    \vspace{1.2em}

    % Subfigure 2 .............................................................
    \begin{subfigure}{0.8\textwidth}
        \centering
        \includegraphics[width=0.8\textwidth]{5_figures/chapter4/f_test_one_tailed.png}
        \caption{Representation of the 1-sided P-value, computed as the integral of the right tail of the t distribution.}
        \label{fig:f_distribution_1_sided_p}
    \end{subfigure}

    \vspace{1.2em}

    % Subfigure 3 .............................................................
    \begin{subfigure}{0.8\textwidth}
        \centering
        \includegraphics[width=0.8\textwidth]{5_figures/chapter4/f_test_two_tailed.png}
        \caption{Representation of the 2-sided P-value. Given the symmetry of the t-distribution, it can be obtained as double in size of the 1-sided integral.}
        \label{fig:f_distribution_2_sided_p}
    \end{subfigure}

    \label{fig:f_test_comparison}

\end{figure}

% Subsection ..........................................................................................................
\subsection{Fisher's ANOVA: Compare variation of multiple groups}

In the same way the two-sample \(t\)-test is an extension of the one-sample case, Fisher’s analysis of variance (ANOVA) can be seen as a natural extension of the variance-ratio \(F\)-test to more than two groups. It was developed by Fisher in the 1920s as part of his investigation of sampling distributions under normality and of experimental design, aiming to assess whether several sources of variability could plausibly be attributed to the same underlying population variance \cite{fisher_1925_statistical}.

\medskip

In short, ANOVA tests whether the variability between group means is large relative to the variability within groups.

\medskip

Now consider \(k\) independent samples \(\chi_1, \chi_2, \dots, \chi_k\), with sizes \(n_1, n_2, \dots, n_k\), sample means \(\bar{x}_1, \bar{x}_2, \dots, \bar{x}_k\), and an overall mean \(\bar{x}\). The null hypothesis is formulated as \textit{all groups come from populations with the same true mean}, that is \(H_0:\mu_1=\mu_2=\cdots=\mu_k\).

\medskip

The ANOVA \(F\)-statistic is defined as the ratio of the variance between groups to the variance within groups,
\begin{equation}
    F = \dfrac{s_{\text{between}}^2}{s_{\text{within}}^2} \; ,
    \label{f_statistic_anova}
\end{equation}
where
\[
    s_{\text{between}}^2 = \frac{1}{k-1}\sum_{i=1}^{k} n_i(\bar{x}_i-\bar{x})^2,
    \qquad
    s_{\text{within}}^2 = \frac{1}{N-k}\sum_{i=1}^{k}\sum_{j=1}^{n_i}(x_{ij}-\bar{x}_i)^2,
\]
and \(N=\sum_{i=1}^k n_i\). As in the previous cases, the statistic is constructed so that, when the null hypothesis is true and group means are similar, the ratio remains close to one.

\medskip

The \(F\)-statistic is again a real-valued random variable, and under the null hypothesis it follows a Fisher \(F\)-distribution with degrees of freedom \(\nu_1=k-1\) and \(\nu_2=N-k\). P-values are computed by integrating the right tail of this distribution, exactly as in the two-sample \(F\)-test.

%% Subsection ..........................................................................................................
\subsection{Pearson's $\chi^{2}$ test: Compare distributions and testing for normality}

The chi-square test was introduced by Karl Pearson in 1900 as part of his work on goodness-of-fit and contingency tables \cite{pearson_1900_chisquare}. Pearson’s original formulation was oriented toward measuring discrepancy between observed and expected frequencies, rather than toward formal decision-based hypothesis testing. Given observed counts \(\{O_1,O_2,\dots,O_k\}\) and expected counts \(\{E_1,E_2,\dots,E_k\}\), the \(\chi^2\)-statistic
\begin{equation}
    \chi^2 = \sum_{i=1}^{k} \frac{(O_i - E_i)^2}{E_i}
    \label{chi2_statistic}
\end{equation}
was conceived as a numerical measure of lack of agreement between data and model, with larger values indicating poorer fit.\footnote{Pearson’s original formulation predates the modern null/alternative hypothesis framework and the notion of Type~I and Type~II errors.}

\medskip

As in the previous examples, the \(\chi^2\)-statistic is computed from random data, and repeating the experiment would generally lead to different observed counts and hence a different value of \(\chi^2\). This means that \textit{\(\chi^2\) is itself a random variable}, and it follows a probability distribution known as the Pearson \(\chi^2\)-distribution,
\begin{equation}
    f(\chi^2;\nu) = \frac{(\chi^2)^{\nu/2-1}e^{-\chi^2/2}}{2^{\nu/2}\Gamma(\nu/2)} \; ,
    \label{chi2_distribution}
\end{equation}
defined for \(\chi^2>0\).

\medskip

Unlike the \(t\)-test, where the degrees of freedom are always \(n-1\), the degrees of freedom \(\nu\) of the \(\chi^2\)-test depend on the structure of the data and on how many parameters are estimated under the null hypothesis. In particular:
\begin{itemize}
    \item For a goodness-of-fit test with \(k\) categories, \(\nu = k - 1 - p\), where \(p\) is the number of parameters estimated from the data.
    \item For a test of independence in an \(r\times c\) contingency table, \(\nu = (r-1)(c-1)\).
\end{itemize}

\medskip

In the modern framework, the chi-square test is formulated with an explicit null hypothesis and a p-value. One specifies a null hypothesis \(H_0\) describing the expected distribution or independence structure, derives the asymptotic distribution \(\chi^2\sim\chi^2_\nu\) under \(H_0\), and computes the p-value as the right-tail probability
\[
    p = \mathbb{P}(\chi^2 \ge \chi^2_{\text{obs}}) = \int_{\chi^2_{\text{obs}}}^{\infty} f(\chi^2;\nu)\,d\chi^2.
\]

\medskip

A deeper theoretical connection appears through likelihood-based testing: Pearson’s \(\chi^2\)-statistic is asymptotically equivalent to the likelihood-ratio statistic \(-2\log\Lambda\), a result formalized by Wilks in the 1930s. Both converge to a chi-square distribution under the null hypothesis, providing a unifying asymptotic framework together with Wald and score tests.

\begin{figure}[ht]

    \centering

    % Subfigure 1 .............................................................
    \begin{subfigure}{0.8\textwidth}
        \centering
        \includegraphics[width=0.8\textwidth]{5_figures/chapter4/chi2_distribution.png}
        \caption{The Pearson's \(\chi^2\) distribution of the \(\chi^2\)-statistic, given different values of the degrees of freedom $\nu$.}
        \label{fig:chi2_distribution_dof}
    \end{subfigure}

    \vspace{1.2em}

    % Subfigure 2 .............................................................
    \begin{subfigure}{0.8\textwidth}
        \centering
        \includegraphics[width=0.8\textwidth]{5_figures/chapter4/chi2_test_one_tailed.png}
        \caption{Representation of the 1-sided P-value, computed as the integral of the right tail of the t distribution.}
        \label{fig:chi2_distribution_1_sided_p}
    \end{subfigure}

    \vspace{1.2em}

    % Subfigure 3 .............................................................
    \begin{subfigure}{0.8\textwidth}
        \centering
        \includegraphics[width=0.8\textwidth]{5_figures/chapter4/chi2_test_two_tailed.png}
        \caption{Representation of the 2-sided P-value. Given the symmetry of the t-distribution, it can be obtained as double in size of the 1-sided integral.}
        \label{fig:chi2_distribution_2_sided_p}
    \end{subfigure}

    \label{fig:chi2_test_comparison}

\end{figure}

% Subsection ..........................................................................................................
\subsection{The Wald test: asymptotic behavior}

The Wald test was developed by Abraham Wald in the 1930s as part of a general asymptotic theory of hypothesis testing for parametric models with large samples \cite{wald_1943_tests}. Its purpose is to test hypotheses about finite-dimensional parameters in statistical models using large-sample approximations. Unlike classical tests tailored to specific settings, the Wald test provides a general framework applicable to linear models, generalized linear models, and maximum likelihood estimation.

\medskip

Conceptually, the Wald test generalizes classical parametric tests such as the \(t\)- and \(F\)-tests by formulating hypotheses directly in terms of model parameters. In this sense, it unifies earlier procedures within a common asymptotic theory.

\medskip

Let \(\hat\theta\) be an estimator of a parameter \(\theta\). The Wald statistic is defined as
\[
    W = (\hat\theta - \theta_0)^\top \widehat{\mathrm{Var}}(\hat\theta)^{-1} (\hat\theta - \theta_0).
\]
Under the null hypothesis and suitable regularity conditions,
\[
    W \xrightarrow{d} \chi^2_p,
\]
where \(p\) is the number of tested constraints. P-values are computed from the upper tail of the chi-square distribution.

% Parametric and non-parametric tests .................................................................................
\section{Parametric and non-parametric tests}

Statistical tests are often described as \emph{parametric} or \emph{non-parametric}, a distinction that reflects both historical development and underlying philosophical views about statistical modeling. Parametric tests were developed first, at the beginning of the twentieth century, in a context where probability models were seen as idealized descriptions of data. These tests assume that observations come from a distribution belonging to a family described by a small number of parameters, such as the mean and variance. Statistical inference then focuses directly on these parameters.

\medskip

In practice, parametric tests are frequently associated with the Gaussian (normal) distribution. This historical association arises because many of the foundational procedures of classical statistics—such as the \(t\)-test, the \(F\)-test, and analysis of variance—are exact under normality. As a result, the parametric versus non-parametric distinction is often informally described as “Gaussian versus non-Gaussian.”  This simplification is useful pedagogically, but it should be remembered that parametric models also include many non-Gaussian distributions, such as the binomial or Poisson.

\medskip

Non-parametric tests emerged later, largely in the 1940s and 1950s, motivated by the recognition that real data often deviate from idealized models. Rather than assuming a specific distributional form, these methods aim to remain valid under broad and unspecified distributions.  They typically rely on ranks, signs, or empirical distributions, and therefore make fewer assumptions about the shape of the data.

\medskip

From a historical perspective, the development of statistical testing can be roughly organized as a timeline. Early work by Pearson and Fisher between 1900 and 1930 established the foundations of parametric inference, including the chi-square test, the \(t\)-test, and analysis of variance. In the mid-twentieth century, researchers such as Wilcoxon, Mann, and Whitney introduced distribution-free methods to address practical limitations of these classical procedures. Later developments, including asymptotic theory and robust statistics, provided a unifying framework that connects parametric and non-parametric approaches.

\medskip

To organize the tests presented in this section, it is helpful to focus on their \emph{goal} rather than on their label. Some tests are designed to compare central tendencies between samples, others to assess variability, and others to evaluate the overall agreement between data and a theoretical model. Viewed this way, non-parametric tests are not simply substitutes for parametric ones, but complementary tools that reflect different assumptions, historical traditions, and inferential aims.

% Subsection ..........................................................................................................
\subsection{Wilcoxon signed-rank test}

The Wilcoxon signed-rank test was introduced by Frank Wilcoxon in 1945 as a distribution-free alternative to the one-sample and paired-sample \(t\)-tests \cite{wilcoxon_1945_ranks}. It was motivated by situations in which normality could not be assumed but a test of central location was still desired. The parametric analogue is the one-sample or paired \(t\)-test, which tests a hypothesis about a mean. By contrast, the Wilcoxon signed-rank test targets symmetry of the distribution about a specified location.

\medskip

The test statistic is based on the ranks of the absolute deviations \(|x_i - \theta_0|\), with signs retained. Under the null hypothesis of symmetry, the statistic has a known finite-sample distribution; for moderate to large samples, it is commonly approximated by a normal distribution, from which p-values are obtained.

% Subsection ..........................................................................................................
\subsection{Mann--Whitney \(U\) test}

The Mann--Whitney \(U\) test, introduced by Mann and Whitney in 1947, provides a non-parametric alternative to the two-sample \(t\)-test for independent samples \cite{mann_whitney_1947_test}. It was designed to compare two populations without assuming normality or equal variances. The parametric analogue is the two-sample \(t\)-test, which compares population means. The Mann--Whitney test instead assesses whether one distribution tends to produce larger observations than the other.

\medskip

The test statistic \(U\) is constructed from the ranks of the pooled samples. Under the null hypothesis that the two distributions are identical, \(U\) has a known exact distribution and, asymptotically, a normal distribution. P-values are computed either exactly or via the normal approximation.

% Subsection ..........................................................................................................
\subsection{Levene median-based test}

Levene's test was proposed by Howard Levene in 1960 as a robust alternative to Fisher’s variance-ratio test \cite{levene_1960_variances}. The median-based version, later emphasized by Brown and Forsythe, improves robustness against non-normality. The parametric analogue is the classical \(F\)-test for equality of variances, which is highly sensitive to departures from normality. Levene’s test replaces variances by absolute deviations from group centers.

\medskip

The statistic is computed by applying a one-way ANOVA to the transformed data \(|x_{ij} - \tilde{x}_i|\), where \(\tilde{x}_i\) is the group median. Under the null hypothesis of equal spreads, the test statistic follows approximately an \(F\) distribution, from which p-values are obtained.

% Subsection ..........................................................................................................
\subsection{Kruskal--Wallis test}

The Kruskal--Wallis test was introduced in 1952 by Kruskal and Wallis as a non-parametric extension of one-way ANOVA.  It was motivated by the need to compare more than two groups without assuming normality. The parametric analogue is Fisher’s one-way ANOVA, which tests equality of group
means. The Kruskal--Wallis test instead evaluates whether the group distributions are identical \cite{kruskal_wallis_1952_use}.

\medskip

The test statistic is based on the ranks of all observations:
\[
    H = \frac{12}{N(N+1)} \sum_{i=1}^k n_i (\bar R_i - \bar R)^2 \; .
\]
Under the null hypothesis, \(H\) converges in distribution to \(\chi^2_{k-1}\). P-values are computed from the chi-square distribution.

% Subsection ..........................................................................................................
\subsection{The Kolmogorov--Smirnov test}

The Kolmogorov--Smirnov test was developed in the 1930s by Kolmogorov and later extended by Smirnov as a general goodness-of-fit procedure \cite{kolmogorov_1933_sulla,smirnov_1948_table}. It compares an empirical distribution to a fully specified theoretical distribution. The parametric analogue is the chi-square goodness-of-fit test, which relies on binning and asymptotic approximations. The Kolmogorov--Smirnov test instead measures the maximum discrepancy between distribution functions.

The test statistic is
\[
    D = \sup_x |F_n(x) - F_0(x)| \; .
\]
Under the null hypothesis, \(D\) has a known distribution independent of \(F_0\). P-values are computed from this distribution or its asymptotic form.

% Subsection ..........................................................................................................
\subsection{The Shapiro--Wilk test}

The Shapiro--Wilk test was introduced by Shapiro and Wilk in 1965 as a powerful goodness-of-fit test specifically designed to assess normality \cite{shapiro_wilk_1965_normality}. It was motivated by the low power of general-purpose tests when applied to normal models. The parametric analogue is not a test of means or variances, but rather the assumption of normality underlying \(t\)-tests, \(F\)-tests, and ANOVA. The Shapiro--Wilk test directly targets this assumption.

The test statistic is
\[
    W = \frac{\left(\sum_{i=1}^n a_i x_{(i)}\right)^2}{\sum_{i=1}^n (x_i - \bar{x})^2} \; ,
\]
where the coefficients \(a_i\) depend on normal order statistics. The distribution of \(W\) under the null hypothesis is obtained via approximation or simulation, and p-values are computed accordingly.

% Error types in hypothesis testing ...................................................................................
\section{Error types in hypothesis testing}

Modern hypothesis testing emerged in the early twentieth century as an attempt to formalize uncertainty, error, and decision making in empirical science.  Three major approaches—Fisherian significance testing, Neyman–Pearson hypothesis testing, and Bayesian inference—address these issues in fundamentally different ways, while later philosophical analyses by Reichenbach and Popper clarified their distinct aims.  Subsequent commentators such as Cox, Mayo, and Lehmann have emphasized both the strengths of each framework and the conceptual tensions created by their later amalgamation in textbook practice.

\medskip

Ronald A.\ Fisher introduced significance tests in the 1920s as tools for assessing the \emph{strength of evidence} against a null hypothesis \cite{fisher_1925_statistical,fisher_1935_design}.  In Fisher’s view, a null hypothesis \(H_0\) is a reference model, and the p-value is defined as the probability, under \(H_0\), of observing data at least as extreme as those obtained.  Small p-values indicate discordance between data and model, but Fisher rejected fixed decision thresholds and did not formalize Type~II errors or power.  Type~I error appears implicitly as the tail probability under \(H_0\), not as a long-run operating characteristic.  Hypothesis testing, for Fisher, is evidential rather than decisional: it informs scientific judgment but does not prescribe action.

\medskip

Jerzy Neyman and Egon Pearson developed a sharply different framework in the 1930s, motivated by repeated decision making \cite{neyman_pearson_1933_efficient}.  Here, hypotheses \(H_0\) and \(H_1\) are competing models, and tests are designed to control error rates in the long run.  Type~I error (\(\alpha\)) and Type~II error (\(\beta\)) are central primitives, and optimal tests maximize power subject to a fixed \(\alpha\).  P-values play no essential role; instead, decisions are based on pre-specified critical regions.  This approach interprets hypothesis testing as a rule for action under uncertainty rather than as a measure of evidential support.

\medskip

Bayesian inference, originating in Bayes’s posthumous essay \cite{bayes_1763_doctrine} and developed by Laplace and later subjectivists such as de~Finetti \cite{definetti_1974_probability}, rejects Type~I and Type~II errors as fundamental concepts.  Probability is interpreted as rational degree of belief, and hypotheses themselves are assigned probabilities.  Inference proceeds by updating prior beliefs via Bayes’ theorem to obtain posterior probabilities or Bayes factors.  Hypothesis testing becomes model comparison, and decisions—if required—are made by minimizing expected loss.  The Bayesian framework thus dissolves the classical error dichotomy by reframing uncertainty epistemically rather than behaviorally.

\medskip

Hans Reichenbach provided the clearest philosophical articulation of the frequentist stance underlying Neyman–Pearson theory \cite{reichenbach_1938_prediction}. He distinguished \emph{prediction}—statements about long-run frequencies—from \emph{inference}—claims about truth or belief.  Statistical tests, on this view, justify actions and predictions through their error properties, not through probabilistic assertions about hypotheses.  This position sharply contrasts with Bayesian epistemology and clarifies why frequentist testing can function without assigning probabilities to hypotheses.

\medskip

Karl Popper rejected probabilistic confirmation altogether, arguing that science advances through bold conjectures and severe attempts at falsification \cite{popper_1934_logic}.  Statistical tests, in his view, contribute by formulating risky predictions whose failure can refute theories, not by accumulating evidence or controlling long-run errors.  Popper’s philosophy is incompatible with Bayesian confirmation and only partially aligned with frequentist testing, insofar as both emphasize error and refutation rather than belief.

\medskip

Erich Lehmann, in his definitive treatment of hypothesis testing \cite{lehmann_1959_testing}, emphasized the formal coherence and optimality of Neyman–Pearson theory while explicitly distinguishing it from Fisher’s evidential approach.  D.\,R.~Cox later argued that the routine combination of p-values with fixed significance thresholds conflates logically distinct inferential goals \cite{cox_2006_principles}.  Deborah Mayo further developed an error-statistical philosophy in which evidential interpretation is grounded in the severity with which hypotheses are tested \cite{mayo_1996_error,mayo_2018_severe}.  Together, these authors converge on a common diagnosis: the modern textbook procedure of hypothesis testing is a pragmatic but conceptually hybrid construct, blending incompatible foundations.

\medskip

The coexistence of Fisherian evidence, Neyman–Pearson decision rules, Bayesian belief updating, Popperian falsification, and Reichenbach’s predictive frequentism reflects not confusion but plurality.  Each framework answers a different question—about evidence, action, belief, or prediction—and Type~I and Type~II errors acquire meaning only within the Neyman–Pearson decision-theoretic context.  Understanding these distinctions is essential for the principled use and interpretation of hypothesis tests in modern statistics.
