% Chapter4. Introduction to hypothesis testing ........................................................................
\chapter{Introduction to hypothesis testing}
\label{chapter4}

\epigraph{\textit{The object of statistical science is the reduction of data to relevant information.}}{— Ronald A. Fisher}

The term \textit{hypothesis testing} lies on top of the two pillars we have mentioned in previous chapters. On the one hand, we will use the basic statistical analysis tools we described in Chapter \ref{chapter1}, such as sampling, estimators and general of data visualization. At the same time, we will rely on probability theory to predict expected values about the true popuplation parameters, assuming certain distributions, etc, following what we discussed in Chapter \ref{chapter2}. Most of our examples will assume that our data is simple, smooth, and Gaussian distributed—what people normality refer to as \textit{parametric}—given the Law of Large Numbers and the Central Limit Theorem, and building confidence intervals and critical regions to ensure our estimators—sample mean and variance—reliably represent the population under study, quantifying their central tendency and variation. All these have been discussed in Chapter \ref{chapter3}. 

\medskip

With all these properly covered, we can now start formulating and testing hypotheses. With a predictive mathematical theory, such as probability and combinatorics, we can compute expected values for the true population mean or variance of a given population. With statistical analysis we can build estimators that quantify central tendency and variation, and visualize distribution and outlier behaviors. Finally, we can rely on the results given by the LLN and CLT, and confidence intervals to ensure that bout our expectations and data is smooth and simple to address. With all these, we will be able to to define a new type of \textit{informative quantity} normally referred to as \textit{statistic}, or \textit{statistic test}, that quantifies how much our observed data approaches—or differs from–the expected or hypothesized value. Finally, following the so-called modern, or Pearson-Neyman approach to hypotheis testing, we will learn how to quantify significance through the computaiton of the Pearson value—or P-value, for short.

\medskip 

Before rushing to mathematical or technical definitions, we would like to pencil a brief historical note, that we hope will shed some light on this topic, sometimes rather obscure. The very idea of hypothesis, and verification against experimental observations, is indeed old, and can be traced back to [...]. But quantities such as the arithmetic mean or other estimators we discussed in previous chapters were not properly defined or become a standard in scientific research until quite later [...]. Similarly, the very foundations of probability theory, including the idea of random variable, unitarity, or distributions—Uniform, Binomial, Gaussian—, were not properly until the work of Kolmogorov in 1933 [...]. The problem of mathematically quantify how much an expectation matches or approaches empirical data, is indeed even younger. It is only since the early 1920s with the works of Karl Pearson and Ronald Fisher, among others, that the very idea of statistic tests were developed, and indeed quite different that the modern approach we are used to nowadays. It was Fisher in the (1922, 1925) connected least squares, likelihood, and sampling distributions, establishing the foundations of modern inference. The original development of either the $\chi^2$, the $t$-test, or the Fisher $F$-test, \textit{was not linked to the acceptance / rejectance of hypotheis based on P-values}. The whole philosophy of relying on P-values to accept / reject null hypotheses, and the very formulation of null / alternative hypotheis on the first place, traces back to Egon Pearson and Neyman (1937), as part of their work on formalizing the idea of confidence intervals as frequentist procedures. The meaning of all these quantifies and their interpretation remains nowadays an open philosophical debate, hence we will define them carefully and walk throug each of them with clear examples. We will revisit the interpretation and historical discussion further in this chapter, and hope this brief disclamer will help to start dismantling now preconceived notions, and approach the topic carefully.

% Prediction vs inference revisited ...................................................................................
\section{Prediction vs inference revisted}

When formulating hypothesis about natural phenomena, we shall remember once again the difference betwen population and smapling. On the one hand, we have an idealized, unaccessible population with unaccessible true mean, variance, etc. The way I can compute a mathematical prediction for a random variable, whatever it represents, is through the calculation of an expected value—also referred to \textit{momentum}—given a random variable and its distribution.

\medskip

Given a random variable, we can compute expected values, or momenta of the distriutions:

\medskip

Population mean for a discrete random variable \(x_i\)
\begin{equation}
    \mu = \mathbb{E}[x] = \frac{1}{N}\sum_{i = 1}^{N} x_i \; \mathbb{P}(x_i)
    \label{population_mean_discrete}
\end{equation}

Population variance for a discrete random variable \(x_i\)
\begin{equation}
    \sigma^2 = \mathbb{E}[x - \mu] = \frac{1}{N}\sum_{i = 1}^{N} (x_i - \mu)^2 \; \mathbb{P}(x_i)
    \label{population_var_discrete}
\end{equation}

Population mean for a continous random variable \(x\)
\begin{equation}
    \mu = \mathbb{E}[x] = \int_{i = -\infty}^{\infty} x_i * \; f(x) \; dx
    \label{population_mean_continous}
\end{equation}

Population variance for a continous random variable \(x\)
\begin{equation}
    \mu = \mathbb{E}[x - \mu] = \int_{i = -\infty}^{\infty} (x_i - \mu) * \; f(x) \; dx
    \label{population_var_continous}
\end{equation}

if \(x\) is a continous random variable.

\medskip

Hence hypotheses will \textit{always be formulated in terms of a mathematical predictions about the population parameters}. If I believe in Newtonian mechanics, a hypothesis could be to write down Newont's second law and use it to predict where and when a stone would fall when dropped from a certain height—its position and time. If my hypothesis is that a gene has a cerain impact in a known disease, or in response to stress, —its expression level, or counts. Or, if I am studying the relation between smokers in the UK and their probability to develop lung cancer [...]. In any of these cases, upon hypothesis. I would need samples, or groups, of measurements, normally referred to simply as \textit{data}.

\medskip

Observed sample mean for a sample \(\chi = \{x_1, x_2, ..., x_n\}\)
\begin{equation}
    \bar{x} = \frac{1}{n}\sum_{i = 1}^{\infty} x_i
    \label{sample_mean}
\end{equation}

Observed sample variance for a sample \(\chi = \{x_1, x_2, ..., x_n\}\)
\begin{equation}
    s^2 = \frac{1}{n - 1}\sum_{i = 1}^{\infty} (x_i - \bar{x})^2
    \label{sample_variance}
\end{equation}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\textwidth]{5_figures/chapter1/prediction_vs_inference.png}
    \caption{Representation of the predictive (from theory, or model, to experimental verification) and inferential (from data, measurement, observation to underlying truth) approaches to natural phenomena. As an example of the predictive branch of mathematics dealing with uncertainty we would find the theory of probability, while the descriptive way of addressing the same problem is normally regarded as statistical inference.}
    \label{fig:prediction_vs_inference2}
\end{figure}

% General approach to hypothesis testing ..............................................................................
\section{General approach to hypothesis testing}

When dealing with hypothesis, predictions, experiments and data, there is plenty of approaches and formulations, as many as instruments, scales and fields of study. These do change form one field to another, and they do change in time. Our very idea of hypothsis, prediction, measurement, and law [...]. Nowadays, when people refer to \textit{hypothesis testing} they mean very specific approach, almost and algorithmc-wise set of rules, that is applied in general to inference and data science problems. We will define such approach as the "modern", or "general" approach to hypothesis testing, that asumes some basic notions of probability theory, distributions and randomness, with some of statistics, estimators and sample description [...]. The whole idea of statistic test, P-value and significance, that we will discuss now, ranges indeed from quite recent times, back to Pearson, Fisher, and Neyman in the early 1900s.

\begin{itemize}
    \item Formulate hypothesis. Normally referred to as \textit{null} hypothesis \(H_0\), as the expectation that our prediction or expectation will follow, and \textit{alternative} hypothesis \(H_1\), representing the case of finding a surpsing observation, that deviates from \(H_0\). These hypothesis will \textit{always} be made about the \textit{true population parameters}, and commonly formulated as the computation of an expected value, that we discussed in Chapter \ref{chapter2}.
    \item Experiment, measurement, observation. Any process, regardless of instrumentation and object of study, that involves a measurement, an observation, or data collection of any kind from one or more samples.
    \item Compute statistic, or statistic test. Out of our random data we can caompute any \textit{informative quantity}, which can be an estimator like the sample mean, the variance, etc, or a more abstract quantity that represents how close are the these mean and variance from their expected values, given \(H_0\) 
    \item Copute P-value: the probability that, given a certain assumtion for our true population parameters and our random data, we obtained a value at least as extreme as the one we got for our statistic.
    \item interpretation of the result, normally accept / reject the null hypothesis based on the P-value and some significance threshold.
\end{itemize}

A couple of notes about this general roadmap. A statistic can be just an estimator, like the sample mean [...]. Fisher's definition of P-value as extream [...]. The approach is a mixed of Fisher and Pearson-Neyman [...].

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{5_figures/chapter1/estimators.png}
    \caption{Representation of the \textit{true} population mean $\mu$, in black, and the observed \textit{sample} mean $\bar{x}$. The true mean is and ideal and unaccessible quantity, while the sample mean can be computed as an estimator of the finite sample.}
    \label{fig:estimators2}
\end{figure}

% Statistic tests: some examples ......................................................................................
\section{Statistic tests: common examples}

% Subsection ..........................................................................................................
\subsection{One sample \(t\)-test: Compare sample mean with hypothesized value}

The \(t\)-test is arguably the simplest example of statistic test we will discuss. It was developed in 1908 by William S. Gosset, a statistician working at the Guinness factory in Dublin, trying to accurately estimate the error of the mean when the population variance is unknown, as part of the brewing process. Due to his affiliation to the Guinness company he has not allowed to publicly share his work and hence he submitted it to the \textit{Biometrika} statistics journal under the pseudonym \textit{Student}. This is why it remains nowadays known as the \textit{Student's \(t\)-test}.

\medskip

The test begins by formulating some null hypothesis about the true population mean, \textit{prior to any sampling or data collection}. Normally, the null hypothesis is simply written as \textit{the true population mean is expected to take the value \(\mu\)}. It is important here to stop and think carefully about what is the physical quantity that we are actually going to measure. Remember that such quantity, unaccessible in theory, can be either fitted from data, or predicted through the computation of an expected value, as we discussed in Chapter \ref{chapter2}.

\medskip

Now take a series of observations, or measurements, and group them in a given sample \(\chi = \{x_1, x_2, \dots, x_n\}\). Out of them, we can compute the sample mean \(\bar{x}\), as an estimator of the true population mea \(\mu\), and the sample standard deviation \(s\), as an estimator of the true population standard deviation \(\sigma\).

\medskip

Given these three elements (the expected value \(\mu\), given by our null hypothesis \(H_0\), the sample mean \(\bar{x}\) and the sample standard deviation \(s\), as our estimators) we can compute the \(t\)-\textit{statistic}, or \(t\)-\textit{statistic test}, defined as
\begin{equation}
    t = \dfrac{\bar{x} - \mu}{s/\sqrt{n}} \; .
    \label{t_statistic_one_sample}
\end{equation}
Let's look at this quantity for a second. We will notice that as the sample mean \(\bar{x}\) approaches the expected value \(\mu\), the \(t\)-variable tends to zero. It was designed for this precise purpose, to quantify how different—or similar—our data is from the hipothesized value, and in the ideal case \(\bar{x} \rightarrow \mu\), then \(t \rightarrow 0\). 

\medskip

It is important to note here that \(t\) is obtained out of a set of random observations. If I repeat the same measurements in a different sample, or a different day, or under different conditions, they may lead to different values of \(\bar{x}\) and \(s\), hence producing a different \(t\). This means that \textit{\(t\) is a real-valued random variable itself}, and it will follow \textit{some} distribution. The mathematical definition of such distribution—or density—of the \(t\)-variable, as introduced in Gosset's work [...], is called the \textit{Student's \(t\)-distribution},
\begin{equation}
    f(t; \nu) = \dfrac{\Gamma \bigg( \dfrac{\nu + 1}{2} \bigg)}{\sqrt{\pi\nu} \; \Gamma \big(\dfrac{\nu}{2} \big)} \; \bigg( 1 + \dfrac{t^2}{\nu} \bigg)^{(\nu + 1) / 2} \; ,
    \label{t_distribution}
\end{equation}
where the parameter \(\nu\) is referred to as the \textit{degrees of freedom}, and it is simply related to the length of the sample \textit{\(\nu = n - 1\)}. You may see that some textbooks and literature sources write it as \(t_\nu\), which is perfectly fine and common standard, but we prefere to use here the letter \(t\) just for the statistic, and \(f(t; \nu)\) for the distribution of \(t\) given \(\nu\) degrees of freedom, rather than referring to both elements with the same symbol. It can be demonstrated that as the degrees of freedom increase, the \(t\)-distribution tends asymptotically to a Gaussian distribution, normally written as \(f(t; \nu) \to \mathcal N(0,1)\) for \(\nu\to\infty\). The explicit demonstration is out of the scope of this course, but it can be found at [...].

\medskip

We arrive now to the final step; the computation of the P-value. Back to the well known Fisher's definition of P-value, \textit{the probability of obtaining a value at least as extreme as the one observed for our statistic, asuming the expected value—or values—given by our null hypothesis \(H_0\) and our random data}, we see that once known the distribution of the \(t\)-statistic, we just need to compute the cumulative probability—that is, the integral—of such distribution. 

\medskip

If we ask for the probability of obtaining a value \textit{strictly greater or strictly smaller than the one we observed for our statistic}, \(\mathbb{P}(t \ge t_{\text{obs}})\), we just need to compute the integral of one of the distribution tales, and it is referred to as a \textit{one-sided}—or one-\textit{tailed}—P-value. On the other hand, if we ask for the probability of obtaining a value \textit{more extreme, regardless of the direction, than the one we observed}, \(\mathbb{P}(|t| \ge |t_{\text{obs}}|)\), that would require the integral of both tails, and it is referred to as a \textit{two-sided}—or two-\textit{tailed}—P-value. Given the symmetry of the \(t\)-distribution, this reduces to double the size of the one-sided P-value. 
\[
    P_{\text{one-sided}} = \mathbb{P}(t \ge t_{\text{obs}}) = \int_{t_{\text{obs}}}^{\infty} f(t; \nu) \; dt \; ,
\]

\[
    P_{\text{two-sided}} = \mathbb{P}(|t| \ge |t_{\text{obs}}|) = 2 \int_{|t_{\text{obs}}|}^{\infty} f(t; \nu) \; dt \; .
\]

For a review of cumulative probabilities and integrating probability distributions, go back to Chapter \ref{chapter2}, and for a review on integral calculus and some warm-up examples, see Appendix \ref{appendix3}. If this feels a bit heavy, this is where computer softwares and libraries become particularly useful, as they not only implement the calculation of the statistic but the numerical integration of such distribution, yealding to the P-value without the need for manual integral calculus. Examples of these will be \texttt{scipy} library of \texttt{Python}, and the \texttt{stats} package of \texttt{R}, among many others [...].

\medskip

\textbf{Example.} For a sample of size \(n = 10\), observed average \(\bar{x} = 5.2\), variance \(s = 1.0\), and \(\mu_0 = 5\), then
\[
    t_{\text{obs}} = \frac{5.2-5}{1/\sqrt{10}} \approx 0.63,
\]

Given this observed value, and the degrees of freedom \(\nu=9\), the two-sided p-value would be \(p\approx0.54\).

\begin{figure}[ht]

    \centering

    % Subfigure 1 .............................................................
    \begin{subfigure}{0.8\textwidth}
        \centering
        \includegraphics[width=0.8\textwidth]{5_figures/chapter4/t_distribution.png}
        \caption{The Student's t distribution of the t-statistic, given different values of the degrees of freedom $\nu$.}
        \label{fig:t_distribution_dof}
    \end{subfigure}

    \vspace{1.2em}

    % Subfigure 2 .............................................................
    \begin{subfigure}{0.8\textwidth}
        \centering
        \includegraphics[width=0.8\textwidth]{5_figures/chapter4/t_test_1_sample_p_one_tailed.png}
        \caption{Representation of the 1-sided P-value, computed as the integral of the right tail of the t distribution.}
        \label{fig:t_distribution_1_sided_p}
    \end{subfigure}

    \vspace{1.2em}

    % Subfigure 3 .............................................................
    \begin{subfigure}{0.8\textwidth}
        \centering
        \includegraphics[width=0.8\textwidth]{5_figures/chapter4/t_test_1_sample_p_two_tailed.png}
        \caption{Representation of the 2-sided P-value. Given the symmetry of the t-distribution, it can be obtained as double in size of the 1-sided integral.}
        \label{fig:t_distribution_2_sided_p}
    \end{subfigure}

    \label{fig:t_test_comparison}

\end{figure}

% Subsection ..........................................................................................................
\subsection{Two sample \(t\)-test: Compare sample means of two independent groups}

The two-sample \(t\)-test is an extension of the one-sample case, hence the general approach will be almost identical as the one discussed in previous section. It is used to test two independent samples, \(chi_1\) and \(\chi_2\), of lengths \(n_1\) and \(n_2\). The null hypothesis is still formulated about the true population means, as \textit{both observations come—are sampled from—the same distribution, with an expected true mean \(\mu\)}. Remember again that such quantity, unaccessible in theory, can be either fitted from previous data, or predicted through the computation of an expected value, as we discussed in Chapter \ref{chapter2}.

\medskip

Now take a series of measurements for both samples, and compute the sample means \(\bar{x}_1\), \(\bar{x}_2\), as estimators of the true population mea \(\mu\), and the sample variances \(s_1^2\), \(s_2^2\), as estimators of the true population variance \(\sigma^2\).

\medskip

In the same way we proceeded for the one-sample case, we combine the expectation given by our \(H_0\) and the estimators computed out of our data, into the \textit{two-sample \(t\)-statistic}, or \textit{two-sample \(t\)-statistic test}, defined as
\begin{equation}
    t = \dfrac{\bar{x}_1 - \bar{x}_2}{\sqrt{\dfrac{s_1^2}{n_1} + \dfrac{s_2^2}{n_2 }}} \; .
    \label{t_statistic_two_sample}
\end{equation}
The denominator is normally called \textit{pooled standard deviation}, and denoted by \(s_p\). We can see that this quantity behaves in the same way as the one-sample case, tending to zero, as the sample means of both groups tend to each other, \(t \rightarrow 0\) as \(\bar{x}_1 \rightarrow \bar{x}_2\),

\medskip

Again, \(t\) is a real-valued random variable, and it follows the same Student's \(t\)-distribution of eq. \eqref{t_distribution}. The only difference is that the degrees of freedom \(\nu\) are now obtained by combining the lengths of both samples \textit{\(\nu = n_1 + n_2 - 2\)}.

\medskip

The computation of the P-value, is identical to the one-sample case, as we just need to integrate the \(t\)-distribution. Again, given the symmetry of the \(t\)-distribution, the two-sided P-value is just double the size of the one-sided case. For a review of cumulative probabilities and integrating probability distributions, go back to Chapter \ref{chapter2}, and for a review on integral calculus and some warm-up examples, see Appendix \ref{appendix3}.

\medskip

\textbf{Example.} For sample of size \(n_1 = 10\), observed average \(\bar{x} = 5.2\), variance \(s = 1.0\), and \(\mu_0 = 5\), then
\[
    t_{\text{obs}} = \frac{5.2-5}{1/\sqrt{10}} \approx 0.63,
\]

Given this observed value, and the degrees of freedom \(\nu=9\), the two-sided p-value would be \(p\approx0.54\).

% Subsection ..........................................................................................................
\subsection{Fisher's \(F\) test: Compare variation of two independent groups}

The Fisher's variance-ratio test, or \(F\)-test for short, was introduced by Fisher in the 1920s and formally developed in his works \textit{Statistical methods for research workers} and \textit{The design of experiments}, in 1925 and 1935. The impact of Fisher's work, not only in statistics but also in evolutionary biology, experimental design, hypothesis tesing and mathematical modelling is credited still nowadays as one of the greatest among the twentieth century, by far [...].

\medskip

The \(F\) test begins by formulating some null hypothesis about the true population variance, \textit{prior to any sampling or data collection}. Normally, the null hypothesis is simply written as \textit{both samples under study come from the same distribution, with true population variance \(\sigma^2\)}. As we did in previous cases, remember that such quantity is hypothesized value about the true population, and it can be either fitted from data, or predicted through the computation of an expected value, as we discussed in Chapter \ref{chapter2}.

\medskip

Now take a series of measurements for two independent samples \(\chi_1\), \(chi_2\) of sizes \(n_1\) and \(n_2\), compute the sample variances \(s_1^2\), \(s_2^2\), as estimators of the true population variances \(\sigma_1^2\) and \(\sigma_2^2\). Then the Fisher \(F\)-\textit{statistic}, or \(F\)-\textit{statistic test}, is defined just as the ratio
\begin{equation}
    F = \dfrac{s_1^2}{s_2^2} \; .
    \label{f_statistic_exact}
\end{equation}
Similarly to what happend in previous cases, we can notice that as the sample variances \(s_1^2\), \(s_2^2\) approach each other, the \(F\)-variable tends to one. It was designed for this precise purpose, to quantify how different—or similar—two independent groups are from each other, and in the ideal case \(s_1^2 \rightarrow s_2^2\), then \(F \rightarrow 1\). Recall the definition of the \(t\)-statistic, as here we can start noticing that, in general, statistic tests are normally defined such that, in the case of \(H_0\) being true, they reduce to a small, simple value.

\medskip

If we pay close attention, we can notice that unlike the \(t\)-test, where the null hypothesis was stated directly in terms of a \(\mu\) parameter, appearing explicitly in the definition of the \(t\)-statistic, there is no explicit trace of \(H_0\) in the definition of our \(F\), which is computed just as the ratio of two sample variances. There is a historical reason for this, that we will revisit further in this chapter, related to how the very idea of hypotheis, parameter and statistic test were used in Fisher's time, different from modern usage. As we will see, the classical \(F\)-test encodes the null hypothesis through the \textit{condition under which the ratio of \(F\) of sample variances follow a specific distribution}. In practice, this reflects the fact that the \(t\)-test is formulated around an explicit parameter hypothesis, whereas the \(F\)-test arose from Fisher's analysis of sampling distributions.

\medskip

Same as in the two previous examples, the \(F\)-statistic is obtained out of a set of random observations. If I repeat the same measurements in a different sample, or a different day, or under different conditions, they may lead to different values \(s_1\) and \(s_2\), hence producing a different \(F\). This means that \textit{\(F\) is a real-valued random variable itself}, and it will follow \textit{some} distribution. The mathematical definition of such distribution—or density—of the \(F\)-variable, as introduced in Fisher's [...], is called the Fisher's \textit{\(F\)-distribution},
\begin{equation}
    f(F; \nu_1, \nu_2) = \dfrac{1}{B \bigg( \dfrac{\nu_1}{2}, \dfrac{\nu_2}{2} \bigg)} \; \bigg( \dfrac{\nu_1}{\nu_2} \bigg)^{\frac{\nu_1}{2}}\; F^{\frac{\nu_1}{2} - 1} \;  \bigg( 1 + \dfrac{\nu_1}{\nu_2} F \bigg)^{-\frac{(\nu_1 + \nu_2)}{2}} \; ,
    \label{f_distribution}
\end{equation}
where the parameters \(\nu_1\), \(\nu_2\) represent the \textit{degrees of freedom}, and they are related to the length of the samples \(\nu_1 = n_1 - 1\), \(\nu_2 = n_2 - 1\). You may see that some textbooks and literature sources write it as \(F_{\nu_1, \nu_2}\), which is perfectly fine and common standard, but we prefere to use here the letter \(F\) just for the statistic, and \(f(F; \nu_1, \nu_2)\) for the distribution of \(F\) given \(\nu_1\) and \(\nu_2\) degrees of freedom, rather than referring to both elements with the same symbol. It can be demonstrated that as \(\nu_1,\nu_2 \to \infty\), \(f(t; \nu_1, \nu_2)\) concentrates at \(1\) and \(\log f(F; \nu_1, \nu_2)\) becomes approximately Gaussian. The explicit demonstration is out of the scope of this course, but it can be found at [...].

As we mentioned alread, it is under the null hypothesis \(H_0: \sigma_1^2 = \sigma_2^2\) that the \(F\)-statistic follows the Fisher distribution. Modern Wald and likelihood-ratio tests provide a unified parameter-based framework.

\medskip

Back to the computation of the P-value, given Fisher's definition, \textit{the probability of obtaining a value at least as extreme as the one observed for our statistic, asuming the expected value—or values—given by our null hypothesis \(H_0\) and our random data}, we just need to compute the cumulative probability—that is, the integral—of the \(F\) distribution. 

\medskip

If we ask for the probability of obtaining a value \textit{strictly greater or strictly smaller than the one we observed for our statistic}, \(\mathbb{P}(F \ge F_{\text{obs}})\), we just need to compute the integral of one of the distribution tales, and it give the one-sided P-value. But the two-sided case, the probability of obtaining a value \textit{more extreme, regardless of the direction} would require the integral of both tails, and given the asymmetry of the \(F\)-distribution, becomes a non-trivial task. The common formulation is written as follows
\[
    P_{\text{one-sided}} = \mathbb{P}(F \ge F_{\text{obs}}) = \int_{F_{\text{obs}}}^{\infty} f(F; \nu_1, \nu_2) \; df \; ,
\]

\[
    P_{\text{two-sided}} = 2 \min \!\left\{ \int_{0}^{F_{\text{obs}}} f(F; \nu_1, \nu_2) \; df, \; \int_{F_{\text{obs}}}^{\infty} f(F; \nu_1, \nu_2) \; df \right\}.
\]

For a review of cumulative probabilities and integrating probability distributions, go back to Chapter \ref{chapter2}, and for a review on integral calculus and some warm-up examples, see Appendix \ref{appendix3}. If this feels a bit heavy, this is where computer softwares and libraries become particularly useful, as they not only implement the calculation of the statistic but the numerical integration of such distribution, yealding to the P-value without the need for manual integral calculus. Examples of these will be \texttt{scipy} library of \texttt{Python}, and the \texttt{stats} package of \texttt{R}, among many others [...].

\medskip

\textbf{Example.}
If \(n_1=n_2=10\), \(S_1^2=4\), \(S_2^2=2\), then
\[
f_{\text{obs}}=2,\qquad \nu_1=\nu_2=9,
\]
yielding a one-sided p-value \(p\approx0.12\).

\begin{figure}[ht]

    \centering

    % Subfigure 1 .............................................................
    \begin{subfigure}{0.8\textwidth}
        \centering
        \includegraphics[width=0.8\textwidth]{5_figures/chapter4/f_distribution.png}
        \caption{The Fisher's \(F\) distribution of the \(F\)-statistic, given different values of the degrees of freedom $\nu$.}
        \label{fig:f_distribution_dof}
    \end{subfigure}

    \vspace{1.2em}

    % Subfigure 2 .............................................................
    \begin{subfigure}{0.8\textwidth}
        \centering
        \includegraphics[width=0.8\textwidth]{5_figures/chapter4/f_test_one_tailed.png}
        \caption{Representation of the 1-sided P-value, computed as the integral of the right tail of the t distribution.}
        \label{fig:f_distribution_1_sided_p}
    \end{subfigure}

    \vspace{1.2em}

    % Subfigure 3 .............................................................
    \begin{subfigure}{0.8\textwidth}
        \centering
        \includegraphics[width=0.8\textwidth]{5_figures/chapter4/f_test_two_tailed.png}
        \caption{Representation of the 2-sided P-value. Given the symmetry of the t-distribution, it can be obtained as double in size of the 1-sided integral.}
        \label{fig:f_distribution_2_sided_p}
    \end{subfigure}

    \label{fig:f_test_comparison}

\end{figure}

% Subsection ..........................................................................................................
\subsection{Fisher's ANOVA: Compare variation of multiple groups}

In the same case the two-sample \(t\)-test was an extension of the one-sample case, both wollowing Gosset's formulation of the statistic and corresponding distribution, Fisher's Analysis of Variance, or ANOVA test, is a specific case of the general \(F\) test we just discussed. It was developed as part of his investigation of the sampling distributions of quadratic forms under normality, aiming to check whether two sources of variability could plausibly be attributed to the same underlying variance, without an explicit parameter-first formulation of hypotheses. Rather than comparing two variances in isolation, Fisher decomposed total variability into components attributable to multiple factors and experimental design.

\medskip

In short, ANOVA tests whether the variability between group means is large relative to the variability within groups.

\medskip

Now take a series of measurements for two independent samples \(\chi_1\), \(chi_2\) of sizes \(n_1\) and \(n_2\), compute the sample variances \(s_1^2\), \(s_2^2\), as estimators of the true population variances \(\sigma_1^2\) and \(\sigma_2^2\). Then the Fisher \(F\)-\textit{statistic}, or \(F\)-\textit{statistic test}, is defined just as the ratio
\begin{equation}
    F = \dfrac{s_{\text{between}}^2}{s_{\text{withinn}}^2} \; .
    \label{f_statistic_anova}
\end{equation}
Similarly to what happend in previous cases, we can notice that as the sample variances \(s_1^2\), \(s_2^2\) approach each other, the \(F\)-variable tends to one. It was designed for this precise purpose, to quantify how different—or similar—two independent groups are from each other, and in the ideal case \(s_1^2 \rightarrow s_2^2\), then \(F \rightarrow 1\). Recall the definition of the \(t\)-statistic, as here we can start noticing that, in general, statistic tests are normally defined such that, in the case of \(H_0\) being true, they reduce to a small, simple value.

\medskip

% Subsection ..........................................................................................................
\subsection{Pearson's $\chi^{2}$ test: Compare distributions and testing for normality}

The chi-square test was introduced by Karl Pearson in 1900 as part of his work on goodness-of-fit and contingency tables. Pearson's original formulation was oriented towards evaluating distriutions and quantify similarly, rather than formal hypothesis testing: Given a list of observations \{\(O_1, O_2, \dots, O_n\)\} and a series of expectations \{\(E_1, E_2, \dots, E_n\)\}, the \(\chi^2\) statistic
\begin{equation}
    \chi^2 = \sum_{i} \frac{(O_i - E_i)^2}{E_i}
    \label{chi2_statistic}
\end{equation}
was conceived as a numerical measure of discrepancy between observed and expected frequencies under a proposed model. Large values of \(\chi^2\) indicated poor agreement, while a \(\chi^2\) close to zero indicates point-wise similarly. \footnote{Pearson’s original work does not distinguish null and alternative hypotheses in the modern sense, nor does it invoke Type~I and Type~II errors; these concepts were introduced later by Neyman and Pearson in the late 1920s and early 1930s.}

\medskip

Same as in the two previous examples, the \(\chi^2\)-statistic is obtained out of a set of random observations. If I repeat the same measurements in a different sample, or a different day, or under different conditions, they may lead to different values \(O_i\), hence producing a different \(\chi^2\). This means that \textit{\(\chi^2\) is a real-valued random variable itself}, and it will follow \textit{some} distribution. The mathematical definition of such distribution—or density—of the \(\chi^2\)-variable, as introduced by Pearson in his work [...], is called the Pearson's \textit{\(\chi^2\)-distribution},
\begin{equation}
    f(\chi^2; \nu) = \dfrac{(\chi^2)^{\nu/2 - 1} \; e^{-\chi^2/2}}{2^{\nu/2} \; \Gamma \bigg( \dfrac{\nu}{2} \bigg)} \; ,
    \label{chi2_distribution}
\end{equation}

where \(\chi^2\) is a strictly positive quantity, and the degrees of freedom \(\nu\) depend on the number of constraints imposed by the data structure and on the number of parameters estimated under the null hypothesis, unlike the \(t\)-test where the degrees of freedom are always \(n-1\) due to the estimation of a single mean, now they are defined as. 
\begin{itemize}
    \item When used to evaluate the goodness-of-fit test (multinomial, with \(k\) categories) \(\nu = n - 1 - p,\) where \(p\) is the number of parameters estimated from the data.  In particular, \(\nu = n - 1\) if no parameters are estimated, and \(\nu = n - 2\) if one parameter is estimated (e.g. a mean). 
    \item When used to test of independence in a contingency table (\(r \times c\)). \(\nu = (r-1)(c-1)\). In all cases, the degrees of freedom reflect the effective number of independent components remaining after accounting for normalization constraints and parameter estimation. 
\end{itemize}

\medskip

In the modern framework, the chi-square test is formulated with an explicit null hypothesis and a P-value. One specifies
\[
    H_0:\ \text{the observed frequencies follow the model } F_0
\]
(or independence in a contingency table), derives the asymptotic distribution

\[
    \chi^2 \sim \chi^2_{\nu} \quad\text{under } H_0,
\]
and evaluates significance via the tail probability of the observed statistic. This reformulation transformed Pearson’s discrepancy measure into a decision-theoretic test with controlled error rates, fully aligned with modern parametric inference.

\medskip

If we ask for the probability of obtaining a value \textit{strictly greater or strictly smaller than the one we observed for our statistic}, \(\mathbb{P}(F \ge F_{\text{obs}})\), we just need to compute the integral of one of the distribution tales, and it give the one-sided P-value. But the two-sided case, the probability of obtaining a value \textit{more extreme, regardless of the direction} would require the integral of both tails, and given the asymmetry of the \(F\)-distribution, becomes a non-trivial task. The common formulation is written as follows
\[
    P_{\text{one-sided}} = \mathbb{P} (\chi^2 \ge \chi^2_{\text{obs}}) = \int_{\chi^2_{\text{obs}}}^{\infty} f(\chi^2; \nu) \; d\chi^2,
\]

\[
    P_{\text{two-sided}} = 2 \min \!\left\{ \int_{0}^{\chi^2_{\text{obs}}} f(\chi^2; \nu) \; d\chi^2, \; \int_{x_{\text{obs}}}^{\infty} f(\chi^2; \nu) \; d\chi^2 \right\}.
\]

For a review of cumulative probabilities and integrating probability distributions, go back to Chapter \ref{chapter2}, and for a review on integral calculus and some warm-up examples, see Appendix \ref{appendix3}. If this feels a bit heavy, this is where computer softwares and libraries become particularly useful, as they not only implement the calculation of the statistic but the numerical integration of such distribution, yealding to the P-value without the need for manual integral calculus. Examples of these will be \texttt{scipy} library of \texttt{Python}, and the \texttt{stats} package of \texttt{R}, among many others [...].

\medskip

A deeper theoretical connection appears through likelihood-based testing. In many settings, Pearson’s chi-square statistic is asymptotically equivalent to the likelihood-ratio statistic
\[
    -2 \log \Lambda,
\]
a result formalized by Wilks in the 1930s.  Both statistics converge in distribution to a chi-square law under the null hypothesis, reflecting the quadratic approximation of the log-likelihood near the true parameter. From this perspective, the classical chi-square test can be viewed as an early large-sample approximation to likelihood-based inference, with likelihood-ratio, Wald, and score tests providing a unified parametric framework that generalizes Pearson’s original idea.

\begin{figure}[ht]

    \centering

    % Subfigure 1 .............................................................
    \begin{subfigure}{0.8\textwidth}
        \centering
        \includegraphics[width=0.8\textwidth]{5_figures/chapter4/chi2_distribution.png}
        \caption{The Pearson's \(\chi^2\) distribution of the \(\chi^2\)-statistic, given different values of the degrees of freedom $\nu$.}
        \label{fig:chi2_distribution_dof}
    \end{subfigure}

    \vspace{1.2em}

    % Subfigure 2 .............................................................
    \begin{subfigure}{0.8\textwidth}
        \centering
        \includegraphics[width=0.8\textwidth]{5_figures/chapter4/chi2_test_one_tailed.png}
        \caption{Representation of the 1-sided P-value, computed as the integral of the right tail of the t distribution.}
        \label{fig:chi2_distribution_1_sided_p}
    \end{subfigure}

    \vspace{1.2em}

    % Subfigure 3 .............................................................
    \begin{subfigure}{0.8\textwidth}
        \centering
        \includegraphics[width=0.8\textwidth]{5_figures/chapter4/chi2_test_two_tailed.png}
        \caption{Representation of the 2-sided P-value. Given the symmetry of the t-distribution, it can be obtained as double in size of the 1-sided integral.}
        \label{fig:chi2_distribution_2_sided_p}
    \end{subfigure}

    \label{fig:chi2_test_comparison}

\end{figure}

% Subsection ..........................................................................................................
\subsection{The Wald test: asymptotic behavior}

The Wald test was developed in the 1930s by Abraham Wald in the context of general parametric inference and decision theory. Its purpose is to test hypotheses about finite-dimensional parameters in statistical models using
large-sample approximations. Unlike classical tests tailored to specific settings, the Wald test provides a general framework applicable to linear models, generalized linear models, and maximum likelihood estimation.

\medskip

Conceptually, the Wald test generalizes classical parametric tests such as the \(t\)- and \(F\)-tests by formulating hypotheses directly in terms of model parameters. It thus unifies earlier procedures within a common asymptotic theory.

\medskip

Let \(\hat\theta\) be an estimator of a parameter \(\theta\). The Wald statistic
is
\[
    W = (\hat\theta - \theta_0)^\top \widehat{\mathrm{Var}}(\hat\theta)^{-1} (\hat\theta - \theta_0).
\]
Under the null hypothesis and suitable regularity conditions,
\[
    W \xrightarrow{d} \chi^2_p,
\]
where \(p\) is the number of tested constraints.  P-values are computed from the upper tail of the chi-square distribution.

% Parametric and non-parametric tests .................................................................................
\section{Parametric and non-parametric tests}

Statistical tests are often described as \emph{parametric} or \emph{non-parametric}, a distinction that reflects both historical development and underlying philosophical views about statistical modeling. Parametric tests were developed first, at the beginning of the twentieth century, in a context where probability models were seen as idealized descriptions of data. These tests assume that observations come from a distribution belonging to a family described by a small number of parameters, such as the mean and variance. Statistical inference then focuses directly on these parameters.

\medskip

In practice, parametric tests are frequently associated with the Gaussian (normal) distribution. This historical association arises because many of the foundational procedures of classical statistics—such as the \(t\)-test, the \(F\)-test, and analysis of variance—are exact under normality. As a result, the parametric versus non-parametric distinction is often informally described as “Gaussian versus non-Gaussian.”  This simplification is useful pedagogically, but it should be remembered that parametric models also include many non-Gaussian distributions, such as the binomial or Poisson.

\medskip

Non-parametric tests emerged later, largely in the 1940s and 1950s, motivated by the recognition that real data often deviate from idealized models. Rather than assuming a specific distributional form, these methods aim to remain valid under broad and unspecified distributions.  They typically rely on ranks, signs, or empirical distributions, and therefore make fewer assumptions about the shape of the data.

\medskip

From a historical perspective, the development of statistical testing can be roughly organized as a timeline. Early work by Pearson and Fisher between 1900 and 1930 established the foundations of parametric inference, including the chi-square test, the \(t\)-test, and analysis of variance. In the mid-twentieth century, researchers such as Wilcoxon, Mann, and Whitney introduced distribution-free methods to address practical limitations of these classical procedures. Later developments, including asymptotic theory and robust statistics, provided a unifying framework that connects parametric and non-parametric approaches.

\medskip

To organize the tests presented in this section, it is helpful to focus on their \emph{goal} rather than on their label. Some tests are designed to compare central tendencies between samples, others to assess variability, and others to evaluate the overall agreement between data and a theoretical model. Viewed this way, non-parametric tests are not simply substitutes for parametric ones, but complementary tools that reflect different assumptions, historical traditions, and inferential aims.

% Subsection ..........................................................................................................
\subsection{Wilcoxon signed-rank test}

The Wilcoxon signed-rank test was introduced by Frank Wilcoxon in 1945 as a distribution-free alternative to the one-sample and paired-sample \(t\)-tests. It was motivated by situations in which normality could not be assumed but a test of central location was still desired. The parametric analogue is the one-sample or paired \(t\)-test, which tests a hypothesis about a mean. By contrast, the Wilcoxon signed-rank test targets symmetry of the distribution about a specified location.

\medskip

The test statistic is based on the ranks of the absolute deviations \(|X_i - \theta_0|\), with signs retained. Under the null hypothesis of symmetry, the statistic has a known finite-sample distribution; for moderate to large samples, it is commonly approximated by a normal distribution, from which p-values are obtained.

% Subsection ..........................................................................................................
\subsection{Mann--Whitney \(U\) test}

The Mann--Whitney \(U\) test, independently introduced by Mann and Whitney in 1947, provides a non-parametric alternative to the two-sample \(t\)-test for independent samples.  It was designed to compare two populations without assuming normality or equal variances. The parametric analogue is the two-sample \(t\)-test, which compares population means. The Mann--Whitney test instead assesses whether one distribution tends to produce larger observations than the other.

\medskip

The test statistic \(U\) is constructed from the ranks of the pooled samples. Under the null hypothesis that the two distributions are identical, \(U\) has a known exact distribution and, asymptotically, a normal distribution. P-values are computed either exactly or via the normal approximation.

% Subsection ..........................................................................................................
\subsection{Levene median-based test}

Levene’s test was proposed by Howard Levene in 1960 as a robust alternative to Fisher’s variance-ratio test.  The median-based version, later emphasized by Brown and Forsythe, improves robustness against non-normality. The parametric analogue is the classical \(F\)-test for equality of variances, which is highly sensitive to departures from normality.  Levene’s test replaces variances by absolute deviations from group centers.

\medskip

The statistic is computed by applying a one-way ANOVA to the transformed data \(|X_{ij} - \tilde X_i|\), where \(\tilde X_i\) is the group median. Under the null hypothesis of equal spreads, the test statistic follows approximately an \(F\) distribution, from which p-values are obtained.

% Subsection ..........................................................................................................
\subsection{Kruskal--Wallis test}

The Kruskal--Wallis test was introduced in 1952 by Kruskal and Wallis as a non-parametric extension of one-way ANOVA.  It was motivated by the need to compare more than two groups without assuming normality. The parametric analogue is Fisher’s one-way ANOVA, which tests equality of group
means. The Kruskal--Wallis test instead evaluates whether the group distributions are identical.

\medskip

The test statistic is based on the ranks of all observations:
\[
    H = \frac{12}{N(N+1)} \sum_{i=1}^k n_i (\bar R_i - \bar R)^2 \; .
\]
Under the null hypothesis, \(H\) converges in distribution to \(\chi^2_{k-1}\). P-values are computed from the chi-square distribution.

% Subsection ..........................................................................................................
\subsection{The Kolmogorov--Smirnov test}

The Kolmogorov--Smirnov test was developed in the 1930s by Kolmogorov and later extended by Smirnov as a general goodness-of-fit procedure.  It compares an empirical distribution to a fully specified theoretical distribution. The parametric analogue is the chi-square goodness-of-fit test, which relies on binning and asymptotic approximations.  The Kolmogorov--Smirnov test instead measures the maximum discrepancy between distribution functions.

The test statistic is
\[
    D = \sup_x |F_n(x) - F_0(x)| \; .
\]
Under the null hypothesis, \(D\) has a known distribution independent of \(F_0\). P-values are computed from this distribution or its asymptotic form.

% Subsection ..........................................................................................................
\subsection{The Shapiro--Wilk test}

The Shapiro--Wilk test was introduced by Shapiro and Wilk in 1965 as a powerful goodness-of-fit test specifically designed to assess normality. It was motivated by the low power of general-purpose tests when applied to normal models. The parametric analogue is not a test of means or variances, but rather the assumption of normality underlying \(t\)-tests, \(F\)-tests, and ANOVA. The Shapiro--Wilk test directly targets this assumption.

The test statistic is
\[
    W = \frac{\left(\sum_{i=1}^n a_i x_{(i)}\right)^2}{\sum_{i=1}^n (x_i - \bar x)^2} \; ,
\]
where the coefficients \(a_i\) depend on normal order statistics.  The distribution of \(W\) under the null hypothesis is obtained via approximation or simulation, and p-values are computed accordingly.

% Error types in hypothesis testing ...................................................................................
\section{Error types in hypothesis testing}

Modern hypothesis testing emerged in the early twentieth century as an attempt to formalize uncertainty, error, and decision making in empirical science.  Three major approaches—Fisherian significance testing, Neyman–Pearson hypothesis testing, and Bayesian inference—address these issues in fundamentally different ways, while later philosophical analyses by Reichenbach and Popper clarified their distinct aims.  Subsequent commentators such as Cox, Mayo, and Lehmann have emphasized both the strengths of each framework and the conceptual tensions created by their later amalgamation in textbook practice.

\medskip

Ronald A.\ Fisher introduced significance tests in the 1920s as tools for assessing the \emph{strength of evidence} against a null hypothesis \cite{fisher_1925_statistical,fisher_1935_design}.  In Fisher’s view, a null hypothesis \(H_0\) is a reference model, and the p-value is defined as the probability, under \(H_0\), of observing data at least as extreme as those obtained.  Small p-values indicate discordance between data and model, but Fisher rejected fixed decision thresholds and did not formalize Type~II errors or power.  Type~I error appears implicitly as the tail probability under \(H_0\), not as a long-run operating characteristic.  Hypothesis testing, for Fisher, is evidential rather than decisional: it informs scientific judgment but does not prescribe action.

\medskip

Jerzy Neyman and Egon Pearson developed a sharply different framework in the 1930s, motivated by repeated decision making \cite{neyman_pearson_1933_efficient}.  Here, hypotheses \(H_0\) and \(H_1\) are competing models, and tests are designed to control error rates in the long run.  Type~I error (\(\alpha\)) and Type~II error (\(\beta\)) are central primitives, and optimal tests maximize power subject to a fixed \(\alpha\).  P-values play no essential role; instead, decisions are based on pre-specified critical regions.  This approach interprets hypothesis testing as a rule for action under uncertainty rather than as a measure of evidential support.

\medskip

Bayesian inference, originating in Bayes’s posthumous essay \cite{bayes_1763_doctrine} and developed by Laplace and later subjectivists such as de~Finetti \cite{definetti_1974_probability}, rejects Type~I and Type~II errors as fundamental concepts.  Probability is interpreted as rational degree of belief, and hypotheses themselves are assigned probabilities.  Inference proceeds by updating prior beliefs via Bayes’ theorem to obtain posterior probabilities or Bayes factors.  Hypothesis testing becomes model comparison, and decisions—if required—are made by minimizing expected loss.  The Bayesian framework thus dissolves the classical error dichotomy by reframing uncertainty epistemically rather than behaviorally.

\medskip

Hans Reichenbach provided the clearest philosophical articulation of the frequentist stance underlying Neyman–Pearson theory \cite{reichenbach_1938_prediction}. He distinguished \emph{prediction}—statements about long-run frequencies—from \emph{inference}—claims about truth or belief.  Statistical tests, on this view, justify actions and predictions through their error properties, not through probabilistic assertions about hypotheses.  This position sharply contrasts with Bayesian epistemology and clarifies why frequentist testing can function without assigning probabilities to hypotheses.

\medskip

Karl Popper rejected probabilistic confirmation altogether, arguing that science advances through bold conjectures and severe attempts at falsification \cite{popper_1934_logic}.  Statistical tests, in his view, contribute by formulating risky predictions whose failure can refute theories, not by accumulating evidence or controlling long-run errors.  Popper’s philosophy is incompatible with Bayesian confirmation and only partially aligned with frequentist testing, insofar as both emphasize error and refutation rather than belief.

\medskip

Erich Lehmann, in his definitive treatment of hypothesis testing \cite{lehmann_1959_testing}, emphasized the formal coherence and optimality of Neyman–Pearson theory while explicitly distinguishing it from Fisher’s evidential approach.  D.\,R.~Cox later argued that the routine combination of p-values with fixed significance thresholds conflates logically distinct inferential goals \cite{cox_2006_principles}.  Deborah Mayo further developed an error-statistical philosophy in which evidential interpretation is grounded in the severity with which hypotheses are tested \cite{mayo_1996_error,mayo_2018_severe}.  Together, these authors converge on a common diagnosis: the modern textbook procedure of hypothesis testing is a pragmatic but conceptually hybrid construct, blending incompatible foundations.

\medskip

The coexistence of Fisherian evidence, Neyman–Pearson decision rules, Bayesian belief updating, Popperian falsification, and Reichenbach’s predictive frequentism reflects not confusion but plurality.  Each framework answers a different question—about evidence, action, belief, or prediction—and Type~I and Type~II errors acquire meaning only within the Neyman–Pearson decision-theoretic context.  Understanding these distinctions is essential for the principled use and interpretation of hypothesis tests in modern statistics.
