% Chapter4. Introduction to hypothesis testing ........................................................................
\chapter{Introduction to hypothesis testing}
\label{chapter4}

\epigraph{\textit{The object of statistical science is the reduction of data to relevant information.}}{— Ronald A. Fisher}

The term \textit{hypothesis testing} lies on top of the two pillars we have mentioned in previous chapters. On the one hand, we will use the basic statistical analysis tools we described in Chapter \ref{chapter1}, such as sampling, estimators and general of data visualization. At the same time, we will rely on probability theory to predict expected values about the true popuplation parameters, assuming certain distributions, etc, following what we discussed in Chapter \ref{chapter2}. Most of our examples will assume that our data is simple, smooth, and Gaussian distributed—what people normality refer to as \textit{parametric}—given the Law of Large Numbers and the Central Limit Theorem, and building confidence intervals and critical regions to ensure our estimators—sample mean and variance—reliably represent the population under study, quantifying their central tendency and variation. All these have been discussed in Chapter \ref{chapter3}. 

\medskip

With all these properly covered, we can now start formulating and testing hypotheses. With a predictive mathematical theory, such as probability and combinatorics, we can compute expected values for the true population mean or variance of a given population. With statistical analysis we can build estimators that quantify central tendency and variation, and visualize distribution and outlier behaviors. Finally, we can rely on the results given by the LLN and CLT, and confidence intervals to ensure that bout our expectations and data is smooth and simple to address. With all these, we will be able to to define a new type of \textit{informative quantity} normally referred to as \textit{statistic}, or \textit{statistic test}, that quantifies how much our observed data approaches—or differs from–the expected or hypothesized value. Finally, following the so-called modern, or Pearson-Neyman approach to hypotheis testing, we will learn how to quantify significance through the computaiton of the Pearson value—or P-value, for short.

\medskip 

Before rushing to mathematical or technical definitions, we would like to pencil a brief historical note, that we hope will shed some light on this topic, sometimes rather obscure. The very idea of hypothesis, and verification against experimental observations, is indeed old, and can be traced back to [...]. But quantities such as the arithmetic mean or other estimators we discussed in previous chapters were not properly defined or become a standard in scientific research until quite later [...]. Similarly, the very foundations of probability theory, including the idea of random variable, unitarity, or distributions—Uniform, Binomial, Gaussian—, were not properly until the work of Kolmogorov in 1933 [...]. The problem of mathematically quantify how much an expectation matches or approaches empirical data, is indeed even younger. It is only since the early 1920s with the works of Karl Pearson and Ronald Fisher, among others, that the very idea of statistic tests were developed, and indeed quite different that the modern approach we are used to nowadays. It was Fisher in the (1922, 1925) connected least squares, likelihood, and sampling distributions, establishing the foundations of modern inference. The original development of either the $\chi^2$, the $t$-test, or the Fisher $F$-test, \textit{was not linked to the acceptance / rejectance of hypotheis based on P-values}. The whole philosophy of relying on P-values to accept / reject null hypotheses, and the very formulation of null / alternative hypotheis on the first place, traces back to Egon Pearson and Neyman (1937), as part of their work on formalizing the idea of confidence intervals as frequentist procedures. The meaning of all these quantifies and their interpretation remains nowadays an open philosophical debate, hence we will define them carefully and walk throug each of them with clear examples. We will revisit the interpretation and historical discussion further in this chapter, and hope this brief disclamer will help to start dismantling now preconceived notions, and approach the topic carefully.

% Prediction vs inference revisited ...................................................................................
\section{Prediction vs inference revisted}

When formulating hypothesis about natural phenomena, we shall remember once again the difference betwen population and smapling. On the one hand, we have an idealized, unaccessible population with unaccessible true mean, variance, etc. The way I can compute a mathematical prediction for a random variable, whatever it represents, is through the calculation of an expected value—also referred to \textit{momentum}—given a random variable and its distribution.

\medskip

Given a random variable, we can compute expected values, or momenta of the distriutions:

\medskip

Population mean for a discrete random variable \(x_i\)
\begin{equation}
    \mu = \mathbb{E}[x] = \frac{1}{N}\sum_{i = 1}^{N} x_i \; \mathbb{P}(x_i)
    \label{population_mean_discrete}
\end{equation}

Population variance for a discrete random variable \(x_i\)
\begin{equation}
    \sigma^2 = \mathbb{E}[x - \mu] = \frac{1}{N}\sum_{i = 1}^{N} (x_i - \mu)^2 \; \mathbb{P}(x_i)
    \label{population_var_discrete}
\end{equation}

Population mean for a continous random variable \(x\)
\begin{equation}
    \mu = \mathbb{E}[x] = \int_{i = -\infty}^{\infty} x_i * \; f(x) \; dx
    \label{population_mean_continous}
\end{equation}

Population variance for a continous random variable \(x\)
\begin{equation}
    \mu = \mathbb{E}[x - \mu] = \int_{i = -\infty}^{\infty} (x_i - \mu) * \; f(x) \; dx
    \label{population_var_continous}
\end{equation}

if \(x\) is a continous random variable.

\medskip

Hence hypotheses will \textit{always be formulated in terms of a mathematical predictions about the population parameters}. If I believe in Newtonian mechanics, a hypothesis could be to write down Newont's second law and use it to predict where and when a stone would fall when dropped from a certain height—its position and time. If my hypothesis is that a gene has a cerain impact in a known disease, or in response to stress, —its expression level, or counts. Or, if I am studying the relation between smokers in the UK and their probability to develop lung cancer [...]. In any of these cases, upon hypothesis. I would need samples, or groups, of measurements, normally referred to simply as \textit{data}.

\medskip

Observed sample mean for a sample \(\chi = \{x_1, x_2, ..., x_n\}\)
\begin{equation}
    \bar{x} = \frac{1}{n}\sum_{i = 1}^{\infty} x_i
    \label{sample_mean}
\end{equation}

Observed sample variance for a sample \(\chi = \{x_1, x_2, ..., x_n\}\)
\begin{equation}
    s^2 = \frac{1}{n - 1}\sum_{i = 1}^{\infty} (x_i - \bar{x})^2
    \label{sample_variance}
\end{equation}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\textwidth]{5_figures/chapter1/prediction_vs_inference.png}
    \caption{Representation of the predictive (from theory, or model, to experimental verification) and inferential (from data, measurement, observation to underlying truth) approaches to natural phenomena. As an example of the predictive branch of mathematics dealing with uncertainty we would find the theory of probability, while the descriptive way of addressing the same problem is normally regarded as statistical inference.}
    \label{fig:prediction_vs_inference2}
\end{figure}

% General approach to hypothesis testing ..............................................................................
\section{General approach to hypothesis testing}

When dealing with hypothesis, predictions, experiments and data, there is plenty of approaches and formulations, as many as instruments, scales and fields of study. These do change form one field to another, and they do change in time. Our very idea of hypothsis, prediction, measurement, and law [...]. Nowadays, when people refer to \textit{hypothesis testing} they mean very specific approach, almost and algorithmc-wise set of rules, that is applied in general to inference and data science problems. We will define such approach as the "modern", or "general" approach to hypothesis testing, that asumes some basic notions of probability theory, distributions and randomness, with some of statistics, estimators and sample description [...]. The whole idea of statistic test, P-value and significance, that we will discuss now, ranges indeed from quite recent times, back to Pearson, Fisher, and Neyman in the early 1900s.

\begin{itemize}
    \item Formulate hypothesis. Normally referred to as \textit{null} hypothesis \(H_0\), as the expectation that our prediction or expectation will follow, and \textit{alternative} hypothesis \(H_1\), representing the case of finding a surpsing observation, that deviates from \(H_0\). These hypothesis will \textit{always} be made about the \textit{true population parameters}, and commonly formulated as the computation of an expected value, that we discussed in Chapter \ref{chapter2}.
    \item Experiment, measurement, observation. Any process, regardless of instrumentation and object of study, that involves a measurement, an observation, or data collection of any kind from one or more samples.
    \item Compute statistic, or statistic test. Out of our random data we can caompute any \textit{informative quantity}, which can be an estimator like the sample mean, the variance, etc, or a more abstract quantity that represents how close are the these mean and variance from their expected values, given \(H_0\) 
    \item Copute P-value: the probability that, given a certain assumtion for our true population parameters and our random data, we obtained a value at least as extreme as the one we got for our statistic.
    \item interpretation of the result, normally accept / reject the null hypothesis based on the P-value and some significance threshold.
\end{itemize}

A couple of notes about this general roadmap. A statistic can be just an estimator, like the sample mean [...]. Fisher's definition of P-value as extream [...]. The approach is a mixed of Fisher and Pearson-Neyman [...].

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{5_figures/chapter1/estimators.png}
    \caption{Representation of the \textit{true} population mean $\mu$, in black, and the observed \textit{sample} mean $\bar{x}$. The true mean is and ideal and unaccessible quantity, while the sample mean can be computed as an estimator of the finite sample.}
    \label{fig:estimators2}
\end{figure}

% Statistic tests: some examples ......................................................................................
\section{Statistic tests: common examples}

% Subsection ..........................................................................................................
\subsection{One sample \(t\)-test: Compare sample mean with hypothesized value}

The \(t\)-test is arguably the simplest example of statistic test we will discuss. It was developed in 1908 by William S. Gosset, a statistician working at the Guinness factory in Dublin, trying to accurately estimate the error of the mean when the population variance is unknown, as part of the brewing process. Due to his affiliation to the Guinness company he has not allowed to publicly share his work and hence he submitted it to the \textit{Biometrika} statistics journal under the pseudonym \textit{Student}. This is why it remains nowadays known as the \textit{Student's \(t\)-test}.

\medskip

The test begins by formulating some null hypothesis about the true population mean, \textit{prior to any sampling or data collection}. Normally, the null hypothesis is simply written as \textit{the true population mean is expected to take the value \(\mu\)}. It is important here to stop and think carefully about what is the physical quantity that we are actually going to measure. Remember that such quantity, unaccessible in theory, can be either fitted from data, or predicted through the computation of an expected value, as we discussed in Chapter \ref{chapter2}.

\medskip

Now take a series of observations, or measurements, and group them in a given sample \(\chi = \{x_1, x_2, \dots, x_n\}\). Out of them, we can compute the sample mean \(\bar{x}\), as an estimator of the true population mea \(\mu\), and the sample standard deviation \(s\), as an estimator of the true population standard deviation \(\sigma\).

\medskip

Given these three elements (the expected value \(\mu\), given by our null hypothesis \(H_0\), the sample mean \(\bar{x}\) and the sample standard deviation \(s\), as our estimators) we can compute the \(t\)-\textit{statistic}, or \(t\)-\textit{statistic test}, defined as
\begin{equation}
    t = \dfrac{\bar{x} - \mu}{s/\sqrt{n}} \; .
    \label{t_statistic_one_sample}
\end{equation}
Let's look at this quantity for a second. We will notice that as the sample mean \(\bar{x}\) approaches the expected value \(\mu\), the \(t\)-variable tends to zero. It was designed for this precise purpose, to quantify how different—or similar—our data is from the hipothesized value, and in the ideal case \(\bar{x} \rightarrow \mu\), then \(t \rightarrow 0\). 

\medskip

It is important to note here that \(t\) is obtained out of a set of random observations. If I repeat the same measurements in a different sample, or a different day, or under different conditions, they may lead to different values of \(\bar{x}\) and \(s\), hence producing a different \(t\). This means that \textit{\(t\) is a real-valued random variable itself}, and it will follow \textit{some} distribution. The mathematical definition of such distribution—or density—of the \(t\)-variable, as introduced in Gosset's work [...], is called the \textit{Student's \(t\)-distribution},
\begin{equation}
    f(t; \nu) = \dfrac{\Gamma \bigg( \dfrac{\nu + 1}{2} \bigg)}{\sqrt{\pi\nu} \; \Gamma \big(\dfrac{\nu}{2} \big)} \; \bigg( 1 + \dfrac{t^2}{\nu} \bigg)^{(\nu + 1) / 2} \; ,
    \label{t_distribution}
\end{equation}
where the parameter \(\nu\) is referred to as the \textit{degrees of freedom}, and it is simply related to the length of the sample \textit{\(\nu = n - 1\)}. You may see that some textbooks and literature sources write it as \(t_\nu\), which is perfectly fine and common standard, but we prefere to use here the letter \(t\) just for the statistic, and \(f(t; \nu)\) for the distribution of \(t\) given \(\nu\) degrees of freedom, rather than referring to both elements with the same symbol. It can be demonstrated that as the degrees of freedom increase, the \(t\)-distribution tends asymptotically to a Gaussian distribution, normally written as \(f(t; \nu) \to \mathcal N(0,1)\) for \(\nu\to\infty\). The explicit demonstration is out of the scope of this course, but it can be found at [...].

\medskip

We arrive now to the final step; the computation of the P-value. Back to the well known Fisher's definition of P-value, \textit{the probability of obtaining a value at least as extreme as the one observed for our statistic, asuming the expected value—or values—given by our null hypothesis \(H_0\) and our random data}, we see that once known the distribution of the \(t\)-statistic, we just need to compute the cumulative probability—that is, the integral—of such distribution. 

\medskip

If we ask for the probability of obtaining a value \textit{strictly greater or strictly smaller than the one we observed for our statistic}, \(\mathbb{P}(t \ge t_{\text{obs}})\), we just need to compute the integral of one of the distribution tales, and it is referred to as a \textit{one-sided}—or one-\textit{tailed}—P-value. On the other hand, if we ask for the probability of obtaining a value \textit{more extreme, regardless of the direction, than the one we observed}, \(\mathbb{P}(|t| \ge |t_{\text{obs}}|)\), that would require the integral of both tails, and it is referred to as a \textit{two-sided}—or two-\textit{tailed}—P-value. Given the symmetry of the \(t\)-distribution, this reduces to double the size of the one-sided P-value. 
\[
    P_{\text{one-sided}} = \mathbb{P}(t \ge t_{\text{obs}}) = \int_{t_{\text{obs}}}^{\infty} f(t; \nu) \; dt \; ,
\]

\[
    P_{\text{two-sided}} = \mathbb{P}(|t| \ge |t_{\text{obs}}|) = 2 \int_{|t_{\text{obs}}|}^{\infty} f(t; \nu) \; dt \; .
\]

For a review of cumulative probabilities and integrating probability distributions, go back to Chapter \ref{chapter2}, and for a review on integral calculus and some warm-up examples, see Appendix \ref{appendix3}. If this feels a bit heavy, this is where computer softwares and libraries become particularly useful, as they not only implement the calculation of the statistic but the numerical integration of such distribution, yealding to the P-value without the need for manual integral calculus. Examples of these will be \texttt{scipy} library of \texttt{Python}, and the \texttt{stats} package of \texttt{R}, among many others [...].

\medskip

\textbf{Example.} For a sample of size \(n = 10\), observed average \(\bar{x} = 5.2\), variance \(s = 1.0\), and \(\mu_0 = 5\), then
\[
    t_{\text{obs}} = \frac{5.2-5}{1/\sqrt{10}} \approx 0.63,
\]

Given this observed value, and the degrees of freedom \(\nu=9\), the two-sided p-value would be \(p\approx0.54\).

\begin{figure}[ht]

    \centering

    % Subfigure 1 .............................................................
    \begin{subfigure}{0.8\textwidth}
        \centering
        \includegraphics[width=0.8\textwidth]{5_figures/chapter4/t_distribution.png}
        \caption{The Student's t distribution of the t-statistic, given different values of the degrees of freedom $\nu$.}
        \label{fig:t_distribution_dof}
    \end{subfigure}

    \vspace{1.2em}

    % Subfigure 2 .............................................................
    \begin{subfigure}{0.8\textwidth}
        \centering
        \includegraphics[width=0.8\textwidth]{5_figures/chapter4/t_test_1_sample_p_one_tailed.png}
        \caption{Representation of the 1-sided P-value, computed as the integral of the right tail of the t distribution.}
        \label{fig:t_distribution_1_sided_p}
    \end{subfigure}

    \vspace{1.2em}

    % Subfigure 3 .............................................................
    \begin{subfigure}{0.8\textwidth}
        \centering
        \includegraphics[width=0.8\textwidth]{5_figures/chapter4/t_test_1_sample_p_two_tailed.png}
        \caption{Representation of the 2-sided P-value. Given the symmetry of the t-distribution, it can be obtained as double in size of the 1-sided integral.}
        \label{fig:t_distribution_2_sided_p}
    \end{subfigure}

    \label{fig:t_test_comparison}

\end{figure}

% Subsection ..........................................................................................................
\subsection{Two sample \(t\)-test: Compare sample means of two independent groups}

The two-sample \(t\)-test is an extension of the one-sample case, hence the general approach will be almost identical as the one discussed in previous section. It is used to test two independent samples, \(chi_1\) and \(\chi_2\), of lengths \(n_1\) and \(n_2\). The null hypothesis is still formulated about the true population means, as \textit{both observations come—are sampled from—the same distribution, with an expected true mean \(\mu\)}. Remember again that such quantity, unaccessible in theory, can be either fitted from previous data, or predicted through the computation of an expected value, as we discussed in Chapter \ref{chapter2}.

\medskip

Now take a series of measurements for both samples, and compute the sample means \(\bar{x}_1\), \(\bar{x}_2\), as estimators of the true population mea \(\mu\), and the sample variances \(s_1^2\), \(s_2^2\), as estimators of the true population variance \(\sigma^2\).

\medskip

In the same way we proceeded for the one-sample case, we combine the expectation given by our \(H_0\) and the estimators computed out of our data, into the \textit{two-sample \(t\)-statistic}, or \textit{two-sample \(t\)-statistic test}, defined as
\begin{equation}
    t = \dfrac{\bar{x}_1 - \bar{x}_2}{\sqrt{\dfrac{s_1^2}{n_1} + \dfrac{s_2^2}{n_2 }}} \; .
    \label{t_statistic_two_sample}
\end{equation}
The denominator is normally called \textit{pooled standard deviation}, and denoted by \(s_p\). We can see that this quantity behaves in the same way as the one-sample case, tending to zero, as the sample means of both groups tend to each other, \(t \rightarrow 0\) as \(\bar{x}_1 \rightarrow \bar{x}_2\),

\medskip

Again, \(t\) is a real-valued random variable, and it follows the same Student's \(t\)-distribution of eq. \eqref{t_distribution}. The only difference is that the degrees of freedom \(\nu\) are now obtained by combining the lengths of both samples \textit{\(\nu = n_1 + n_2 - 2\)}.

\medskip

The computation of the P-value, is identical to the one-sample case, as we just need to integrate the \(t\)-distribution. Again, given the symmetry of the \(t\)-distribution, the two-sided P-value is just double the size of the one-sided case. For a review of cumulative probabilities and integrating probability distributions, go back to Chapter \ref{chapter2}, and for a review on integral calculus and some warm-up examples, see Appendix \ref{appendix3}.

\medskip

\textbf{Example.} For sample of size \(n_1 = 10\), observed average \(\bar{x} = 5.2\), variance \(s = 1.0\), and \(\mu_0 = 5\), then
\[
    t_{\text{obs}} = \frac{5.2-5}{1/\sqrt{10}} \approx 0.63,
\]

Given this observed value, and the degrees of freedom \(\nu=9\), the two-sided p-value would be \(p\approx0.54\).

% Subsection ..........................................................................................................
\subsection{Fisher's \(F\) test: Compare variation of two independent groups}

The Fisher's variance-ratio test, or \(F\)-test for short, was introduced by Fisher in the 1920s and formally developed in his works \textit{Statistical methods for research workers} and \textit{The design of experiments}, in 1925 and 1935. The impact of Fisher's work, not only in statistics but also in evolutionary biology, experimental design, hypothesis tesing and mathematical modelling is credited still nowadays as one of the greatest among the twentieth century, by far [...].

\medskip

The \(F\) test begins by formulating some null hypothesis about the true population variance, \textit{prior to any sampling or data collection}. Normally, the null hypothesis is simply written as \textit{both samples under study come from the same distribution, with true population variance \(\sigma^2\)}. As we did in previous cases, remember that such quantity is hypothesized value about the true population, and it can be either fitted from data, or predicted through the computation of an expected value, as we discussed in Chapter \ref{chapter2}.

\medskip

Now take a series of measurements for two independent samples \(\chi_1\), \(chi_2\) of sizes \(n_1\) and \(n_2\), compute the sample variances \(s_1^2\), \(s_2^2\), as estimators of the true population variances \(\sigma_1^2\) and \(\sigma_2^2\). Then the Fisher \(F\)-\textit{statistic}, or \(F\)-\textit{statistic test}, is defined just as the ratio
\begin{equation}
    F = \dfrac{s_1^2}{s_2^2} \; .
    \label{f_statistic_exact}
\end{equation}
Similarly to what happend in previous cases, we can notice that as the sample variances \(s_1^2\), \(s_2^2\) approach each other, the \(F\)-variable tends to one. It was designed for this precise purpose, to quantify how different—or similar—two independent groups are from each other, and in the ideal case \(s_1^2 \rightarrow s_2^2\), then \(F \rightarrow 1\). Recall the definition of the \(t\)-statistic, as here we can start noticing that, in general, statistic tests are normally defined such that, in the case of \(H_0\) being true, they reduce to a small, simple value.

\medskip

If we pay close attention, we can notice that unlike the \(t\)-test, where the null hypothesis was stated directly in terms of a \(\mu\) parameter, appearing explicitly in the definition of the \(t\)-statistic, there is no explicit trace of \(H_0\) in the definition of our \(F\), which is computed just as the ratio of two sample variances. There is a historical reason for this, that we will revisit further in this chapter, related to how the very idea of hypotheis, parameter and statistic test were used in Fisher's time, different from modern usage. As we will see, the classical \(F\)-test encodes the null hypothesis through the \textit{condition under which the ratio of \(F\) of sample variances follow a specific distribution}. In practice, this reflects the fact that the \(t\)-test is formulated around an explicit parameter hypothesis, whereas the \(F\)-test arose from Fisher's analysis of sampling distributions.

\medskip

Same as in the two previous examples, the \(F\)-statistic is obtained out of a set of random observations. If I repeat the same measurements in a different sample, or a different day, or under different conditions, they may lead to different values \(s_1\) and \(s_2\), hence producing a different \(F\). This means that \textit{\(F\) is a real-valued random variable itself}, and it will follow \textit{some} distribution. The mathematical definition of such distribution—or density—of the \(F\)-variable, as introduced in Fisher's [...], is called the Fisher's \textit{\(F\)-distribution},
\begin{equation}
    f(F; \nu_1, \nu_2) = \dfrac{1}{B \bigg( \dfrac{\nu_1}{2}, \dfrac{\nu_2}{2} \bigg)} \; \bigg( \dfrac{\nu_1}{\nu_2} \bigg)^{\frac{\nu_1}{2}}\; F^{\frac{\nu_1}{2} - 1} \;  \bigg( 1 + \dfrac{\nu_1}{\nu_2} F \bigg)^{-\frac{(\nu_1 + \nu_2)}{2}} \; ,
    \label{f_distribution}
\end{equation}
where the parameters \(\nu_1\), \(\nu_2\) represent the \textit{degrees of freedom}, and they are related to the length of the samples \(\nu_1 = n_1 - 1\), \(\nu_2 = n_2 - 1\). You may see that some textbooks and literature sources write it as \(F_{\nu_1, \nu_2}\), which is perfectly fine and common standard, but we prefere to use here the letter \(F\) just for the statistic, and \(f(F; \nu_1, \nu_2)\) for the distribution of \(F\) given \(\nu_1\) and \(\nu_2\) degrees of freedom, rather than referring to both elements with the same symbol. It can be demonstrated that as \(\nu_1,\nu_2 \to \infty\), \(f(t; \nu_1, \nu_2)\) concentrates at \(1\) and \(\log f(F; \nu_1, \nu_2)\) becomes approximately Gaussian. The explicit demonstration is out of the scope of this course, but it can be found at [...].

As we mentioned alread, it is under the null hypothesis \(H_0: \sigma_1^2 = \sigma_2^2\) that the \(F\)-statistic follows the Fisher distribution. Modern Wald and likelihood-ratio tests provide a unified parameter-based framework.

\medskip

Back to the computation of the P-value, given Fisher's definition, \textit{the probability of obtaining a value at least as extreme as the one observed for our statistic, asuming the expected value—or values—given by our null hypothesis \(H_0\) and our random data}, we just need to compute the cumulative probability—that is, the integral—of the \(F\) distribution. 

\medskip

If we ask for the probability of obtaining a value \textit{strictly greater or strictly smaller than the one we observed for our statistic}, \(\mathbb{P}(F \ge F_{\text{obs}})\), we just need to compute the integral of one of the distribution tales, and it give the one-sided P-value. But the two-sided case, the probability of obtaining a value \textit{more extreme, regardless of the direction} would require the integral of both tails, and given the asymmetry of the \(F\)-distribution, becomes a non-trivial task. The common formulation is written as follows
\[
    P_{\text{one-sided}} = \mathbb{P}(F \ge F_{\text{obs}}) = \int_{F_{\text{obs}}}^{\infty} f(F; \nu_1, \nu_2) \; df \; ,
\]

\[
    P_{\text{two-sided}} = 2 \min \!\left\{ \int_{0}^{F_{\text{obs}}} f(F; \nu_1, \nu_2) \; df, \; \int_{F_{\text{obs}}}^{\infty} f(F; \nu_1, \nu_2) \; df \right\}.
\]

For a review of cumulative probabilities and integrating probability distributions, go back to Chapter \ref{chapter2}, and for a review on integral calculus and some warm-up examples, see Appendix \ref{appendix3}. If this feels a bit heavy, this is where computer softwares and libraries become particularly useful, as they not only implement the calculation of the statistic but the numerical integration of such distribution, yealding to the P-value without the need for manual integral calculus. Examples of these will be \texttt{scipy} library of \texttt{Python}, and the \texttt{stats} package of \texttt{R}, among many others [...].

\medskip

\textbf{Example.}
If \(n_1=n_2=10\), \(S_1^2=4\), \(S_2^2=2\), then
\[
f_{\text{obs}}=2,\qquad \nu_1=\nu_2=9,
\]
yielding a one-sided p-value \(p\approx0.12\).

\begin{figure}[ht]

    \centering

    % Subfigure 1 .............................................................
    \begin{subfigure}{0.8\textwidth}
        \centering
        \includegraphics[width=0.8\textwidth]{5_figures/chapter4/f_distribution.png}
        \caption{The Fisher's \(F\) distribution of the \(F\)-statistic, given different values of the degrees of freedom $\nu$.}
        \label{fig:f_distribution_dof}
    \end{subfigure}

    \vspace{1.2em}

    % Subfigure 2 .............................................................
    \begin{subfigure}{0.8\textwidth}
        \centering
        \includegraphics[width=0.8\textwidth]{5_figures/chapter4/f_test_one_tailed.png}
        \caption{Representation of the 1-sided P-value, computed as the integral of the right tail of the t distribution.}
        \label{fig:f_distribution_1_sided_p}
    \end{subfigure}

    \vspace{1.2em}

    % Subfigure 3 .............................................................
    \begin{subfigure}{0.8\textwidth}
        \centering
        \includegraphics[width=0.8\textwidth]{5_figures/chapter4/f_test_two_tailed.png}
        \caption{Representation of the 2-sided P-value. Given the symmetry of the t-distribution, it can be obtained as double in size of the 1-sided integral.}
        \label{fig:f_distribution_2_sided_p}
    \end{subfigure}

    \label{fig:f_test_comparison}

\end{figure}

% Subsection ..........................................................................................................
\subsection{Fisher's ANOVA: Compare variation of multiple groups}

In the same case the two-sample \(t\)-test was an extension of the one-sample case, both wollowing Gosset's formulation of the statistic and corresponding distribution, Fisher's Analysis of Variance, or ANOVA test, is a specific case of the general \(F\) test we just discussed. It was developed as part of his investigation of the sampling distributions of quadratic forms under normality, aiming to check whether two sources of variability could plausibly be attributed to the same underlying variance, without an explicit parameter-first formulation of hypotheses. Rather than comparing two variances in isolation, Fisher decomposed total variability into components attributable to multiple factors and experimental design.

\medskip

In short, ANOVA tests whether the variability between group means is large relative to the variability within groups.

\medskip

Now take a series of measurements for two independent samples \(\chi_1\), \(chi_2\) of sizes \(n_1\) and \(n_2\), compute the sample variances \(s_1^2\), \(s_2^2\), as estimators of the true population variances \(\sigma_1^2\) and \(\sigma_2^2\). Then the Fisher \(F\)-\textit{statistic}, or \(F\)-\textit{statistic test}, is defined just as the ratio
\begin{equation}
    F = \dfrac{s_{\text{between}}^2}{s_{\text{withinn}}^2} \; .
    \label{f_statistic_anova}
\end{equation}
Similarly to what happend in previous cases, we can notice that as the sample variances \(s_1^2\), \(s_2^2\) approach each other, the \(F\)-variable tends to one. It was designed for this precise purpose, to quantify how different—or similar—two independent groups are from each other, and in the ideal case \(s_1^2 \rightarrow s_2^2\), then \(F \rightarrow 1\). Recall the definition of the \(t\)-statistic, as here we can start noticing that, in general, statistic tests are normally defined such that, in the case of \(H_0\) being true, they reduce to a small, simple value.

\medskip

If we pay close attention, we can notice that unlike the \(t\)-test, where the null hypothesis was stated directly in terms of a \(\mu\) parameter, appearing explicitly in the definition of the \(t\)-statistic, there is no explicit trace of \(H_0\) in the definition of our \(F\), which is computed just as the ratio of two sample variances. There is a historical reason for this, that we will revisit further in this chapter, related to how the very idea of hypotheis, parameter and statistic test were used in Fisher's time, different from modern usage. As we will see, the classical \(F\)-test encodes the null hypothesis through the \textit{condition under which the ratio of \(F\) of sample variances follow a specific distribution}. In practice, this reflects the fact that the \(t\)-test is formulated around an explicit parameter hypothesis, whereas the \(F\)-test arose from Fisher's analysis of sampling distributions.

\medskip

Same as in the two previous examples, the \(F\)-statistic is obtained out of a set of random observations. If I repeat the same measurements in a different sample, or a different day, or under different conditions, they may lead to different values \(s_1\) and \(s_2\), hence producing a different \(F\). This means that \textit{\(F\) is a real-valued random variable itself}, and it will follow \textit{some} distribution. The mathematical definition of such distribution—or density—of the \(F\)-variable, as introduced in Fisher's [...], is called the Fisher's \textit{\(F\)-distribution},
[...]
where the parameters \(\nu_1\), \(\nu_2\) represent the \textit{degrees of freedom}, and they are related to the length of the samples \(\nu_1 = n_1 - 1\), \(\nu_2 = n_2 - 1\). You may see that some textbooks and literature sources write it as \(F_{\nu_1, \nu_2}\), which is perfectly fine and common standard, but we prefere to use here the letter \(F\) just for the statistic, and \(f(F; \nu_1, \nu_2)\) for the distribution of \(F\) given \(\nu_1\) and \(\nu_2\) degrees of freedom, rather than referring to both elements with the same symbol. It can be demonstrated that as \(\nu_1,\nu_2 \to \infty\), \(f(t; \nu_1, \nu_2)\) concentrates at \(1\) and \(\log f(F; \nu_1, \nu_2)\) becomes approximately Gaussian. The explicit demonstration is out of the scope of this course, but it can be found at [...].

As we mentioned alread, it is under the null hypothesis \(H_0: \sigma_1^2 = \sigma_2^2\) that the \(F\)-statistic follows the Fisher distribution. Modern Wald and likelihood-ratio tests provide a unified parameter-based framework.

\medskip

Back to the computation of the P-value, given Fisher's definition, \textit{the probability of obtaining a value at least as extreme as the one observed for our statistic, asuming the expected value—or values—given by our null hypothesis \(H_0\) and our random data}, we just need to compute the cumulative probability—that is, the integral—of the \(F\) distribution. 

\medskip

If we ask for the probability of obtaining a value \textit{strictly greater or strictly smaller than the one we observed for our statistic}, \(\mathbb{P}(F \ge F_{\text{obs}})\), we just need to compute the integral of one of the distribution tales, and it give the one-sided P-value. But the two-sided case, the probability of obtaining a value \textit{more extreme, regardless of the direction} would require the integral of both tails, and given the asymmetry of the \(F\)-distribution, becomes a non-trivial task. The common formulation is written as follows
\[
    P_{\text{one-sided}} = \mathbb{P}(F \ge F_{\text{obs}}) = \int_{F_{\text{obs}}}^{\infty} f(F; \nu_1, \nu_2) \; df \; ,
\]

\[
    P_{\text{two-sided}} = 2 \min \!\left\{ \int_{0}^{F_{\text{obs}}} f(F; \nu_1, \nu_2) \; df, \; \int_{F_{\text{obs}}}^{\infty} f(F; \nu_1, \nu_2) \; df \right\}.
\]

For a review of cumulative probabilities and integrating probability distributions, go back to Chapter \ref{chapter2}, and for a review on integral calculus and some warm-up examples, see Appendix \ref{appendix3}. If this feels a bit heavy, this is where computer softwares and libraries become particularly useful, as they not only implement the calculation of the statistic but the numerical integration of such distribution, yealding to the P-value without the need for manual integral calculus. Examples of these will be \texttt{scipy} library of \texttt{Python}, and the \texttt{stats} package of \texttt{R}, among many others [...].

\medskip

\textbf{Example.}
If \(n_1=n_2=10\), \(S_1^2=4\), \(S_2^2=2\), then
\[
f_{\text{obs}}=2,\qquad \nu_1=\nu_2=9,
\]
yielding a one-sided p-value \(p\approx0.12\).

% Subsection ..........................................................................................................
\subsection{Pearson's $\chi^{2}$ test: Compare distributions and testing for normality}

\paragraph{Pearson’s \(\chi^2\) test and the chi-square distribution.}
Pearson’s \(\chi^2\) test (1900) assesses agreement between observed and expected frequencies.  
With observed counts \(O_i\) and expected counts \(E_i\),
\[
    X^2 = \sum_{i=1}^{k}\frac{(O_i-E_i)^2}{E_i}, \qquad X^2\sim\chi^2_\nu,
\]
where \(\nu=k-1-p\) and \(p\) is the number of estimated parameters.

\emph{p-values.}
\[
    \text{One-sided (standard): }\; p=\mathbb P(X\ge x_{\text{obs}}) = \int_{x_{\text{obs}}}^{\infty} f_{\chi^2_\nu}(x)\,dx,
\]
\[
    \text{Two-sided: }\; p = 2\min\!\left\{ \int_{0}^{x_{\text{obs}}} f_{\chi^2_\nu}(x)\,dx, \; \int_{x_{\text{obs}}}^{\infty} f_{\chi^2_\nu}(x)\,dx \right\}.
\]

\paragraph{The chi-square test: historical and modern perspectives.}
The chi-square test was introduced by Karl Pearson in 1900 as part of his work on goodness-of-fit and contingency tables.  Pearson’s original formulation was rooted in biometric practice rather than formal hypothesis testing: the statistic
\[
    \chi^2 = \sum_{i} \frac{(O_i - E_i)^2}{E_i}
\]
was conceived as a numerical measure of discrepancy between observed and expected frequencies under a proposed model.  Large values of \(\chi^2\) indicated poor agreement, but early usage emphasized comparison with tabulated
critical values and qualitative judgment, reflecting a period that predates the formal Neyman--Pearson framework.\footnote{Pearson’s original work does not
distinguish null and alternative hypotheses in the modern sense, nor does it invoke Type~I and Type~II errors; these concepts were introduced later by Neyman and Pearson in the late 1920s and early 1930s.}

\medskip

In the modern framework, the chi-square test is formulated with an explicit null hypothesis and a p-value.  One specifies
\[
    H_0:\ \text{the observed frequencies follow the model } F_0
\]
(or independence in a contingency table), derives the asymptotic distribution
\[
    \chi^2 \sim \chi^2_{\nu} \quad\text{under } H_0,
\]
and evaluates significance via the tail probability of the observed statistic. This reformulation transformed Pearson’s discrepancy measure into a decision-theoretic test with controlled error rates, fully aligned with modern parametric inference.

A deeper theoretical connection appears through likelihood-based testing. In many settings, Pearson’s chi-square statistic is asymptotically equivalent to the likelihood-ratio statistic
\[
    -2 \log \Lambda,
\]
a result formalized by Wilks in the 1930s.  Both statistics converge in distribution to a chi-square law under the null hypothesis, reflecting the
quadratic approximation of the log-likelihood near the true parameter. From this perspective, the classical chi-square test can be viewed as an early
large-sample approximation to likelihood-based inference, with likelihood-ratio, Wald, and score tests providing a unified parametric framework that generalizes
Pearson’s original idea.

\emph{Example.}
For \(k=4\) categories with \(\nu=3\) and observed statistic \(x_{\text{obs}}=6\),
the right-tailed p-value is \(p\approx0.11\).
As \(\nu\to\infty\),
\[
\frac{X^2-\nu}{\sqrt{2\nu}}\xrightarrow[]{d}\mathcal N(0,1),
\]
so the chi-square becomes approximately Gaussian after centering and scaling.

\begin{figure}[ht]

    \centering

    % Subfigure 1 .............................................................
    \begin{subfigure}{0.8\textwidth}
        \centering
        \includegraphics[width=0.8\textwidth]{5_figures/chapter4/chi2_distribution.png}
        \caption{The Pearson's \(\chi^2\) distribution of the \(\chi^2\)-statistic, given different values of the degrees of freedom $\nu$.}
        \label{fig:chi2_distribution_dof}
    \end{subfigure}

    \vspace{1.2em}

    % Subfigure 2 .............................................................
    \begin{subfigure}{0.8\textwidth}
        \centering
        \includegraphics[width=0.8\textwidth]{5_figures/chapter4/chi2_test_one_tailed.png}
        \caption{Representation of the 1-sided P-value, computed as the integral of the right tail of the t distribution.}
        \label{fig:chi2_distribution_1_sided_p}
    \end{subfigure}

    \vspace{1.2em}

    % Subfigure 3 .............................................................
    \begin{subfigure}{0.8\textwidth}
        \centering
        \includegraphics[width=0.8\textwidth]{5_figures/chapter4/chi2_test_two_tailed.png}
        \caption{Representation of the 2-sided P-value. Given the symmetry of the t-distribution, it can be obtained as double in size of the 1-sided integral.}
        \label{fig:chi2_distribution_2_sided_p}
    \end{subfigure}

    \label{fig:chi2_test_comparison}

\end{figure}

% Subsection ..........................................................................................................
\subsection{The Wald test: assymptotic behavior}

The Wald test is parametric because it tests hypotheses about finite-dimensional model parameters using an estimator whose—asymptotic—distribution is derived from an assumed parametric model.

% Parametric and non-parametric tests .................................................................................
\section{Parametric and non-parametric tests}

% Subsection ..........................................................................................................
\subsection{Wilcoxon signed-ranked test}

% Subsection ..........................................................................................................
\subsection{Mann–Whitney \(U\) test}

% Subsection ..........................................................................................................
\subsection{Levene median-based test}

% Subsection ..........................................................................................................
\subsection{Kruskal-Wallis test}

% Subsection ..........................................................................................................
\subsection{The Kolmogorov-Smirnov test}

% Subsection ..........................................................................................................
\subsection{The Saphiro-Wilk test}

The Shapiro–Wilk test addresses the same goodness-of-fit problem as the \(chi_2\) and Kolmogorov–Smirnov tests, but differs fundamentally in construction: rather than measuring global discrepancies between empirical and theoretical distributions, it exploits the structure of normal order statistics, yielding substantially higher power for detecting departures from normality [...].

% Error types in hypothesis testing ...................................................................................
\section{Error types in hypothesis testing}

Modern hypothesis testing emerged in the early twentieth century as an attempt to formalize uncertainty, error, and decision making in empirical science.  Three major approaches—Fisherian significance testing, Neyman–Pearson hypothesis testing, and Bayesian inference—address these issues in fundamentally different ways, while later philosophical analyses by Reichenbach and Popper clarified their distinct aims.  Subsequent commentators such as Cox, Mayo, and Lehmann have emphasized both the strengths of each framework and the conceptual tensions created by their later amalgamation in textbook practice.

\medskip

Ronald A.\ Fisher introduced significance tests in the 1920s as tools for assessing the \emph{strength of evidence} against a null hypothesis \cite{fisher_1925_statistical,fisher_1935_design}.  In Fisher’s view, a null hypothesis \(H_0\) is a reference model, and the p-value is defined as the probability, under \(H_0\), of observing data at least as extreme as those obtained.  Small p-values indicate discordance between data and model, but Fisher rejected fixed decision thresholds and did not formalize Type~II errors or power.  Type~I error appears implicitly as the tail probability under \(H_0\), not as a long-run operating characteristic.  Hypothesis testing, for Fisher, is evidential rather than decisional: it informs scientific judgment but does not prescribe action.

\medskip

Jerzy Neyman and Egon Pearson developed a sharply different framework in the 1930s, motivated by repeated decision making \cite{neyman_pearson_1933_efficient}.  Here, hypotheses \(H_0\) and \(H_1\) are competing models, and tests are designed to control error rates in the long run.  Type~I error (\(\alpha\)) and Type~II error (\(\beta\)) are central primitives, and optimal tests maximize power subject to a fixed \(\alpha\).  P-values play no essential role; instead, decisions are based on pre-specified critical regions.  This approach interprets hypothesis testing as a rule for action under uncertainty rather than as a measure of evidential support.

\medskip

Bayesian inference, originating in Bayes’s posthumous essay \cite{bayes_1763_doctrine} and developed by Laplace and later subjectivists such as de~Finetti \cite{definetti_1974_probability}, rejects Type~I and Type~II errors as fundamental concepts.  Probability is interpreted as rational degree of belief, and hypotheses themselves are assigned probabilities.  Inference proceeds by updating prior beliefs via Bayes’ theorem to obtain posterior probabilities or Bayes factors.  Hypothesis testing becomes model comparison, and decisions—if required—are made by minimizing expected loss.  The Bayesian framework thus dissolves the classical error dichotomy by reframing uncertainty epistemically rather than behaviorally.

\medskip

Hans Reichenbach provided the clearest philosophical articulation of the frequentist stance underlying Neyman–Pearson theory \cite{reichenbach_1938_prediction}. He distinguished \emph{prediction}—statements about long-run frequencies—from \emph{inference}—claims about truth or belief.  Statistical tests, on this view, justify actions and predictions through their error properties, not through probabilistic assertions about hypotheses.  This position sharply contrasts with Bayesian epistemology and clarifies why frequentist testing can function without assigning probabilities to hypotheses.

\medskip

Karl Popper rejected probabilistic confirmation altogether, arguing that science advances through bold conjectures and severe attempts at falsification \cite{popper_1934_logic}.  Statistical tests, in his view, contribute by formulating risky predictions whose failure can refute theories, not by accumulating evidence or controlling long-run errors.  Popper’s philosophy is incompatible with Bayesian confirmation and only partially aligned with frequentist testing, insofar as both emphasize error and refutation rather than belief.

\medskip

Erich Lehmann, in his definitive treatment of hypothesis testing \cite{lehmann_1959_testing}, emphasized the formal coherence and optimality of Neyman–Pearson theory while explicitly distinguishing it from Fisher’s evidential approach.  D.\,R.~Cox later argued that the routine combination of p-values with fixed significance thresholds conflates logically distinct inferential goals \cite{cox_2006_principles}.  Deborah Mayo further developed an error-statistical philosophy in which evidential interpretation is grounded in the severity with which hypotheses are tested \cite{mayo_1996_error,mayo_2018_severe}.  Together, these authors converge on a common diagnosis: the modern textbook procedure of hypothesis testing is a pragmatic but conceptually hybrid construct, blending incompatible foundations.

\medskip

The coexistence of Fisherian evidence, Neyman–Pearson decision rules, Bayesian belief updating, Popperian falsification, and Reichenbach’s predictive frequentism reflects not confusion but plurality.  Each framework answers a different question—about evidence, action, belief, or prediction—and Type~I and Type~II errors acquire meaning only within the Neyman–Pearson decision-theoretic context.  Understanding these distinctions is essential for the principled use and interpretation of hypothesis tests in modern statistics.
