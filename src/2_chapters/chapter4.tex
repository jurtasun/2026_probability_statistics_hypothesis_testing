% Chapter4. Introduction to hypothesis testing ........................................................................
\chapter{Introduction to hypothesis testing}
\label{chapter4}

\epigraph{\textit{The object of statistical science is the reduction of data to relevant information.}}{— Ronald A. Fisher}

The term \textit{hypothesis testing} lies on top of the two pillars we have mentioned in previous chapters. On the one hand, we will use the basic statistical analysis tools we described in Chapter \ref{chapter1}, such as sampling, estimators and general of data visualization. At the same time, we will rely on probability theory to predict expected values about the true popuplation parameters, assuming certain distributions, etc, following what we discussed in Chapter \ref{chapter2}. Most of our examples will assume that our data is simple, smooth, and Gaussian distributed—what people normality refer to as \textit{parametric}—given the Law of Large Numbers and the Central Limit Theorem, and building confidence intervals and critical regions to ensure our estimators—sample mean and variance—reliably represent the population under study, quantifying their central tendency and variation. All these have been discussed in Chapter \ref{chapter3}. 

\medskip

With all these properly covered, we can now start formulating and testing hypotheses. With a predictive mathematical theory, such as probability and combinatorics, we can compute expected values for the true population mean or variance of a given population. With statistical analysis we can build estimators that quantify central tendency and variation, and visualize distribution and outlier behaviors. Finally, we can rely on the results given by the LLN and CLT, and confidence intervals to ensure that bout our expectations and data is smooth and simple to address. With all these, we will be able to to define a new type of \textit{informative quantity} normally referred to as \textit{statistic}, or \textit{statistic test}, that quantifies how much our observed data approaches—or differs from–the expected or hypothesized value. Finally, following the so-called modern, or Pearson-Neyman approach to hypotheis testing, we will learn how to quantify significance through the computaiton of the Pearson value—or P-value, for short.

\medskip 

Before rushing to mathematical or technical definitions, we would like to pencil a brief historical note, that we hope will shed some light on this topic, sometimes rather obscure. The very idea of hypothesis, and verification against experimental observations, is indeed old, and can be traced back to [...]. But quantities such as the arithmetic mean or other estimators we discussed in previous chapters were not properly defined or become a standard in scientific research until quite later [...]. Similarly, the very foundations of probability theory, including the idea of random variable, unitarity, or distributions—Uniform, Binomial, Gaussian—, were not properly until the work of Kolmogorov in 1933 [...]. The problem of mathematically quantify how much an expectation matches or approaches empirical data, is indeed even younger. It is only since the early 1920s with the works of Karl Pearson and Ronald Fisher, among others, that the very idea of statistic tests were developed, and indeed quite different that the modern approach we are used to nowadays. It was Fisher in the (1922, 1925) connected least squares, likelihood, and sampling distributions, establishing the foundations of modern inference. The original development of either the $\chi^2$, the $t$-test, or the Fisher $F$-test, \textit{was not linked to the acceptance / rejectance of hypotheis based on P-values}. The whole philosophy of relying on P-values to accept / reject null hypotheses, and the very formulation of null / alternative hypotheis on the first place, traces back to Egon Pearson and Neyman (1937), as part of their work on formalizing the idea of confidence intervals as frequentist procedures. The meaning of all these quantifies and their interpretation remains nowadays an open philosophical debate, hence we will define them carefully and walk throug each of them with clear examples. We will revisit the interpretation and historical discussion further in this chapter, and hope this brief disclamer will help to start dismantling now preconceived notions, and approach the topic carefully.

% Prediction vs inference revisited ...................................................................................
\section{Prediction vs inference revisted}

When formulating hypothesis about natural phenomena, we shall remember once again the difference betwen population and smapling. On the one hand, we have an idealized, unaccessible population with unaccessible true mean, variance, etc. The way I can compute a mathematical prediction for a random variable, whatever it represents, is through the calculation of an expected value—also referred to \textit{momentum}—given a random variable and its distribution.

\medskip

Given a random variable, we can compute expected values, or momenta of the distriutions:

\medskip

Population mean for a discrete random variable \(x_i\)
\begin{equation}
    \mu = \mathbb{E}[x] = \frac{1}{N}\sum_{i = 1}^{N} x_i \; \mathbb{P}(x_i)
    \label{population_mean_discrete}
\end{equation}

Population variance for a discrete random variable \(x_i\)
\begin{equation}
    \sigma^2 = \mathbb{E}[x - \mu] = \frac{1}{N}\sum_{i = 1}^{N} (x_i - \mu)^2 \; \mathbb{P}(x_i)
    \label{population_var_discrete}
\end{equation}

Population mean for a continous random variable \(x\)
\begin{equation}
    \mu = \mathbb{E}[x] = \int_{i = -\infty}^{\infty} x_i * \; f(x) \; dx
    \label{population_mean_continous}
\end{equation}

Population variance for a continous random variable \(x\)
\begin{equation}
    \mu = \mathbb{E}[x - \mu] = \int_{i = -\infty}^{\infty} (x_i - \mu) * \; f(x) \; dx
    \label{population_var_continous}
\end{equation}

if \(x\) is a continous random variable.

\medskip

Hence hypotheses will \textit{always be formulated in terms of a mathematical predictions about the population parameters}. If I believe in Newtonian mechanics, a hypothesis could be to write down Newont's second law and use it to predict where and when a stone would fall when dropped from a certain height—its position and time. If my hypothesis is that a gene has a cerain impact in a known disease, or in response to stress, —its expression level, or counts. Or, if I am studying the relation between smokers in the UK and their probability to develop lung cancer [...]. In any of these cases, upon hypothesis. I would need samples, or groups, of measurements, normally referred to simply as \textit{data}.

\medskip

Observed sample mean for a sample \(\chi = \{x_1, x_2, ..., x_n\}\)
\begin{equation}
    \bar{x} = \frac{1}{n}\sum_{i = 1}^{\infty} x_i
    \label{sample_mean}
\end{equation}

Observed sample variance for a sample \(\chi = \{x_1, x_2, ..., x_n\}\)
\begin{equation}
    s^2 = \frac{1}{n - 1}\sum_{i = 1}^{\infty} (x_i - \bar{x})^2
    \label{sample_variance}
\end{equation}


\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\textwidth]{5_figures/chapter1/prediction_vs_inference.png}
    \caption{Representation of the predictive (from theory, or model, to experimental verification) and inferential (from data, measurement, observation to underlying truth) approaches to natural phenomena. As an example of the predictive branch of mathematics dealing with uncertainty we would find the theory of probability, while the descriptive way of addressing the same problem is normally regarded as statistical inference.}
    \label{fig:prediction_vs_inference2}
\end{figure}

\medskip

\section{General approach to hypothesis testing}

When dealing with hypothesis, predictions, experiments and data, there is plenty of approaches and formulations, as many as instruments, scales and fields of study. These do change form one field to another, and they do change in time. Our very idea of hypothsis, prediction, measurement, and law [...]. Nowadays, when people refer to \textit{hypothesis testing} they mean very specific approach, almost and algorithmc-wise set of rules, that is applied in general to inference and data science problems. We will define such approach as the "modern", or "general" approach to hypothesis testing, that asumes some basic notions of probability theory, distributions and randomness, with some of statistics, estimators and sample description [...]. The whole idea of statistic test, P-value and significance, that we will discuss now, ranges indeed from quite recent times, back to Pearson, Fisher, and Neyman in the early 1900s.

\begin{itemize}
    \item Formulate hypothesis. Normally referred to as \textit{null} hypothesis \(H_0\), as the expectation that our prediction or expectation will follow, and \textit{alternative} hypothesis \(H_1\), representing the case of finding a surpsing observation, that deviates from \(H_0\). These hypothesis will \textit{always} be made about the \textit{true population parameters}, and commonly formulated as the computation of an expected value, that we discussed in Chapter \ref{chapter2}.
    \item Experiment, measurement, observation. Any process, regardless of instrumentation and object of study, that involves a measurement, an observation, or data collection of any kind from one or more samples.
    \item Compute statistic, or statistic test. Out of our random data we can caompute any \textit{informative quantity}, which can be an estimator like the sample mean, the variance, etc, or a more abstract quantity that represents how close are the these mean and variance from their expected values, given \(H_0\) 
    \item Copute P-value: the probability that, given a certain assumtion for our true population parameters and our random data, we obtained a value at least as extreme as the one we got for our statistic.
    \item interpretation of the result, normally accept / reject the null hypothesis based on the P-value and some significance threshold.
\end{itemize}

A couple of notes about this general roadmap. A statistic can be just an estimator, like the sample mean [...]. Fisher's definition of P-value as extream [...]. The approach is a mixed of Fisher and Pearson-Neyman [...].

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{5_figures/chapter1/estimators.png}
    \caption{Representation of the \textit{true} population mean $\mu$, in black, and the observed \textit{sample} mean $\bar{x}$. The true mean is and ideal and unaccessible quantity, while the sample mean can be computed as an estimator of the finite sample.}
    \label{fig:estimators2}
\end{figure}

\section{Statistical tests: some examples}

\subsection{Compare sample mean with hypothesized value - One sample t-test}

The \(t\)-test is arguably the simplest example of statistic test we will discuss. It was developed in 1908 by William S. Gosset, a statistician working at the Guiness factory in Dublin, trying to accurately describe the error of the mean when the population variance is unknown, as part of the brewing process. Due to his affiliation to the Guiness company we has not allowed to share his work and hence he submitted it to the \textit{Biometrika} statistics journal under the pseudonym \textit{Student}. This is why it remains nowadays known as the Student's \(t\)-test.

\medskip

We start by formulating some hypotheis about the true population mean and variance, \textit{prior to any sampling or data collection}. It is important here to stop and think carefully about \textit{what is the physical quantity that we are eventually goint to measure}. Normally, the null hypotheis is simply written as "the true population mean is expected to take the value \(\mu\)". Remember that such quantity can be computed as an expected value of a random variable.

\medskip

Now take a series of observations, or measurements, and groupd them in a given sample \(\chi = \{x_1, x_2, \dots, x_n\}\). Out of them, we can compute the sample mean \(\bar{x}\), as an estimator of the true population mean, and the sample standard deviation \(s\), as an estimator of the true population standard deviation.

\medskip

Now, given these three elements, we can compute the \(t\)-\textit{statistic}, or \(t\)-\textit{statistic test}, as
\[
    t = \frac{\bar{x} - \mu}{s/\sqrt{n}},
\]

Let's look at this quantitty for a second. We will notice that as the sample mean \(\bar{x}\) approaches the expected value \(\mu\), the \(t\)-variable tends to zero. It was designed for this precise purpose, to quantify how different our data is from the expected value, and in the ideal case \(\bar{x} = \mu\), then \(t = 0\). 

\medskip

Finally, towards the computation of the P-value, it is important to note that \(t\) is computed out of a set of random observations. If I repeat the same measurements in a different sample, or a different day, they may lead to a different sample mean and variance, hence arriving to a different \(t\). And Hence, textit{\(t\) is a random variable} itself. This is important because, back to the well known Fisher's definition of P-value, \textit{the probability of obtaining a value at least as extreme as the one we observed for our statistic, asuming the expected value given by \(H_0\) and our random data}, we see that we just need to know what is the \textit{distribution} of the \(t\)-statistic, and then compute the cumulative probability—the integral—of such distribution. For a review of cumulative probabilities and integrating probability distributions, go back to Chapter \ref{chapter2}, and for a review on integral calculus and some examples, see Appendix \ref{appendix3}.

\medskip

It is known to follow a Student's \(t\)-distribution with $\nu$ degrees of freedom. Normally written as \(t \sim t_\nu,\ \nu = n-1 \). As \(\nu\to\infty\), \(t_\nu\to\mathcal N(0,1)\).

\[
    \text{One-sided: }\; p=\mathbb P(T\ge t_{\text{obs}}) = \int_{t_{\text{obs}}}^{\infty} f_{t_\nu}(t) \; dt \; ,
\]

\[
    \text{Two-sided: }\; p=\mathbb P(|T|\ge |t_{\text{obs}}|) = 2\int_{|t_{\text{obs}}|}^{\infty} f_{t_\nu}(t) \; dt \; .
\]

\emph{Example.} For a sample of size \(n = 10\), observed average \(\bar{x} = 5.2\), variance \(s = 1.0\), and \(\mu_0 = 5\), then
\[
    t_{\text{obs}} = \frac{5.2-5}{1/\sqrt{10}} \approx 0.63,
\]
Given this observed value, and the degrees of freedom \(\nu=9\), the two-sided p-value would be \(p\approx0.54\).

\begin{figure}[ht]

    \centering

    % Subfigure 1 .............................................................
    \begin{subfigure}{0.8\textwidth}
        \centering
        \includegraphics[width=0.8\textwidth]{5_figures/chapter4/t_distribution.png}
        \caption{The Student's t distribution of the t-statistic, given different values of the degrees of freedom $\nu$.}
        \label{fig:t_distribution_dof}
    \end{subfigure}

    \vspace{1.2em}

    % Subfigure 2 .............................................................
    \begin{subfigure}{0.8\textwidth}
        \centering
        \includegraphics[width=0.8\textwidth]{5_figures/chapter4/t_test_1_sample_p_one_tailed.png}
        \caption{Representation of the 1-sided P-value, computed as the integral of the right tail of the t distribution.}
        \label{fig:t_distribution_1_sided_p}
    \end{subfigure}

    \vspace{1.2em}

    % Subfigure 3 .............................................................
    \begin{subfigure}{0.8\textwidth}
        \centering
        \includegraphics[width=0.8\textwidth]{5_figures/chapter4/t_test_1_sample_p_two_tailed.png}
        \caption{Representation of the 2-sided P-value. Given the symmetry of the t-distribution, it can be obtained as double in size of the 1-sided integral.}
        \label{fig:t_distribution_2_sided_p}
    \end{subfigure}

    \label{fig:t_test_comparison}

\end{figure}

\subsection{Compare sample means of two independent groups - Two sample t-test}

\subsection{Compare variation on two groups - Fisher's exact test}

\paragraph{Fisher’s variance-ratio test and the \(F\) distribution.}
Fisher introduced the variance-ratio test in the 1920s in the context of ANOVA.  
For two independent normal samples,
\[
F=\frac{S_1^2}{S_2^2},
\qquad
F\sim F_{\nu_1,\nu_2},
\quad
\nu_1=n_1-1,\ \nu_2=n_2-1,
\]
under \(H_0:\sigma_1^2=\sigma_2^2\).

\emph{p-values.}
\[
\text{One-sided: }\;
p=\mathbb P(F\ge f_{\text{obs}})
=
\int_{f_{\text{obs}}}^{\infty} f_{F_{\nu_1,\nu_2}}(f)\,df,
\]
\[
\text{Two-sided: }\;
p
=
2\min\!\left\{
\int_{0}^{f_{\text{obs}}} f_{F_{\nu_1,\nu_2}}(f)\,df,\;
\int_{f_{\text{obs}}}^{\infty} f_{F_{\nu_1,\nu_2}}(f)\,df
\right\}.
\]

\emph{Example.}
If \(n_1=n_2=10\), \(S_1^2=4\), \(S_2^2=2\), then
\[
f_{\text{obs}}=2,\qquad \nu_1=\nu_2=9,
\]
yielding a one-sided p-value \(p\approx0.12\).
As \(\nu_1,\nu_2\to\infty\), \(F_{\nu_1,\nu_2}\) concentrates at \(1\) and \(\log F\) becomes approximately Gaussian.

\begin{figure}[ht]

    \centering

    % Subfigure 1 .............................................................
    \begin{subfigure}{0.8\textwidth}
        \centering
        \includegraphics[width=0.8\textwidth]{5_figures/chapter4/f_distribution.png}
        \caption{The Student's t distribution of the t-statistic, given different values of the degrees of freedom $\nu$.}
        \label{fig:f_distribution_dof}
    \end{subfigure}

    \vspace{1.2em}

    % Subfigure 2 .............................................................
    \begin{subfigure}{0.8\textwidth}
        \centering
        \includegraphics[width=0.8\textwidth]{5_figures/chapter4/f_test_one_tailed.png}
        \caption{Representation of the 1-sided P-value, computed as the integral of the right tail of the t distribution.}
        \label{fig:f_distribution_1_sided_p}
    \end{subfigure}

    \vspace{1.2em}

    % Subfigure 3 .............................................................
    \begin{subfigure}{0.8\textwidth}
        \centering
        \includegraphics[width=0.8\textwidth]{5_figures/chapter4/f_test_two_tailed.png}
        \caption{Representation of the 2-sided P-value. Given the symmetry of the t-distribution, it can be obtained as double in size of the 1-sided integral.}
        \label{fig:f_distribution_2_sided_p}
    \end{subfigure}

    \label{fig:f_test_comparison}

\end{figure}

\subsection{Compare variation o multiple groups - Fisher's ANOVA}

\subsection{Compare distributions and testing for normality - $\chi^{2}$ test}


\paragraph{Pearson’s \(\chi^2\) test and the chi-square distribution.}
Pearson’s \(\chi^2\) test (1900) assesses agreement between observed and expected frequencies.  
With observed counts \(O_i\) and expected counts \(E_i\),
\[
X^2
=
\sum_{i=1}^{k}\frac{(O_i-E_i)^2}{E_i},
\qquad
X^2\sim\chi^2_\nu,
\]
where \(\nu=k-1-p\) and \(p\) is the number of estimated parameters.

\emph{p-values.}
\[
\text{One-sided (standard): }\;
p=\mathbb P(X\ge x_{\text{obs}})
=
\int_{x_{\text{obs}}}^{\infty} f_{\chi^2_\nu}(x)\,dx,
\]
\[
\text{Two-sided: }\;
p
=
2\min\!\left\{
\int_{0}^{x_{\text{obs}}} f_{\chi^2_\nu}(x)\,dx,\;
\int_{x_{\text{obs}}}^{\infty} f_{\chi^2_\nu}(x)\,dx
\right\}.
\]

\emph{Example.}
For \(k=4\) categories with \(\nu=3\) and observed statistic \(x_{\text{obs}}=6\),
the right-tailed p-value is \(p\approx0.11\).
As \(\nu\to\infty\),
\[
\frac{X^2-\nu}{\sqrt{2\nu}}\xrightarrow[]{d}\mathcal N(0,1),
\]
so the chi-square becomes approximately Gaussian after centering and scaling.

\section{Parametric and non-parametric tests}

\section{Error types in hypothesis testing}

Modern hypothesis testing emerged in the early twentieth century as an attempt to formalize uncertainty, error, and decision making in empirical science.  Three major approaches—Fisherian significance testing, Neyman–Pearson hypothesis testing, and Bayesian inference—address these issues in fundamentally different ways, while later philosophical analyses by Reichenbach and Popper clarified their distinct aims.  Subsequent commentators such as Cox, Mayo, and Lehmann have emphasized both the strengths of each framework and the conceptual tensions created by their later amalgamation in textbook practice.

\medskip

Ronald A.\ Fisher introduced significance tests in the 1920s as tools for assessing the \emph{strength of evidence} against a null hypothesis \cite{fisher_1925_statistical,fisher_1935_design}.  In Fisher’s view, a null hypothesis \(H_0\) is a reference model, and the p-value is defined as the probability, under \(H_0\), of observing data at least as extreme as those obtained.  Small p-values indicate discordance between data and model, but Fisher rejected fixed decision thresholds and did not formalize Type~II errors or power.  Type~I error appears implicitly as the tail probability under \(H_0\), not as a long-run operating characteristic.  Hypothesis testing, for Fisher, is evidential rather than decisional: it informs scientific judgment but does not prescribe action.

\medskip

Jerzy Neyman and Egon Pearson developed a sharply different framework in the 1930s, motivated by repeated decision making \cite{neyman_pearson_1933_efficient}.  Here, hypotheses \(H_0\) and \(H_1\) are competing models, and tests are designed to control error rates in the long run.  Type~I error (\(\alpha\)) and Type~II error (\(\beta\)) are central primitives, and optimal tests maximize power subject to a fixed \(\alpha\).  P-values play no essential role; instead, decisions are based on pre-specified critical regions.  This approach interprets hypothesis testing as a rule for action under uncertainty rather than as a measure of evidential support.

\medskip

Bayesian inference, originating in Bayes’s posthumous essay \cite{bayes_1763_doctrine} and developed by Laplace and later subjectivists such as de~Finetti \cite{definetti_1974_probability}, rejects Type~I and Type~II errors as fundamental concepts.  Probability is interpreted as rational degree of belief, and hypotheses themselves are assigned probabilities.  Inference proceeds by updating prior beliefs via Bayes’ theorem to obtain posterior probabilities or Bayes factors.  Hypothesis testing becomes model comparison, and decisions—if required—are made by minimizing expected loss.  The Bayesian framework thus dissolves the classical error dichotomy by reframing uncertainty epistemically rather than behaviorally.

\medskip

Hans Reichenbach provided the clearest philosophical articulation of the frequentist stance underlying Neyman–Pearson theory \cite{reichenbach_1938_prediction}. He distinguished \emph{prediction}—statements about long-run frequencies—from \emph{inference}—claims about truth or belief.  Statistical tests, on this view, justify actions and predictions through their error properties, not through probabilistic assertions about hypotheses.  This position sharply contrasts with Bayesian epistemology and clarifies why frequentist testing can function without assigning probabilities to hypotheses.

\medskip

Karl Popper rejected probabilistic confirmation altogether, arguing that science advances through bold conjectures and severe attempts at falsification \cite{popper_1934_logic}.  Statistical tests, in his view, contribute by formulating risky predictions whose failure can refute theories, not by accumulating evidence or controlling long-run errors.  Popper’s philosophy is incompatible with Bayesian confirmation and only partially aligned with frequentist testing, insofar as both emphasize error and refutation rather than belief.

\medskip

Erich Lehmann, in his definitive treatment of hypothesis testing \cite{lehmann_1959_testing}, emphasized the formal coherence and optimality of Neyman–Pearson theory while explicitly distinguishing it from Fisher’s evidential approach.  D.\,R.~Cox later argued that the routine combination of p-values with fixed significance thresholds conflates logically distinct inferential goals \cite{cox_2006_principles}.  Deborah Mayo further developed an error-statistical philosophy in which evidential interpretation is grounded in the severity with which hypotheses are tested \cite{mayo_1996_error,mayo_2018_severe}.  Together, these authors converge on a common diagnosis: the modern textbook procedure of hypothesis testing is a pragmatic but conceptually hybrid construct, blending incompatible foundations.

\medskip

The coexistence of Fisherian evidence, Neyman–Pearson decision rules, Bayesian belief updating, Popperian falsification, and Reichenbach’s predictive frequentism reflects not confusion but plurality.  Each framework answers a different question—about evidence, action, belief, or prediction—and Type~I and Type~II errors acquire meaning only within the Neyman–Pearson decision-theoretic context.  Understanding these distinctions is essential for the principled use and interpretation of hypothesis tests in modern statistics.

\newpage

\subsection*{Exercises}

\textbf{1.} Exercise [...].\\

\textbf{2.} Exercise [...].\\

\textbf{3.} Exercise [...].\\

\newpage

\subsection*{Solutions}

\textbf{1.} Solution [...].\\

\textbf{2.} Solution [...].\\

\textbf{3.} Solution [...].\\
