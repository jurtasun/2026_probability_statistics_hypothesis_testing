% Chapter 1. Descriptive statistics ....................................................................................
\chapter{Descriptive statistics}
\label{chapter1}

\epigraph{\textit{Statistics is the grammar of science.}}{— Karl Pearson}

As a first approach to probability and statistics, we should properly define both topics and their main fields of study. Although deeply related, and both historically rooted in \textit{combinatorics}—the mathematical study of counting discrete structures, and how they change—they constitute well-differentiated fields of mathematical analysis. A clear distinction often made is that probability is a \textit{predictive} branch of mathematics, dealing with random events, and aiming to compute expected values for unknown outcomes. On the other hand, statistics can be viewed as a \textit{descriptive} approach to uncertainty, based on sampling finite sets of observations from a given population and constructing informative quantities, called \textit{estimators}, to explore central tendency and variation. Such distinctions have been extensively debated by mathematicians, experimental scientists, and philosophers of science \cite{salmon_1984_scientific}.

\medskip

As a rule of thumb, probability provides a formal language for modelling uncertainty, whereas statistics concerns the epistemic problem of learning from data. In this chapter, we introduce basic ideas of statistical inference such as population, sampling, and estimators of central tendency and variation, together with notions of representation and visualization. The foundations of probability theory, rooted in the works of Bernoulli, Laplace, and Gauss, among others, will be covered in Chapter \ref{chapter2}.

A philosophical position often adopted is that statistics is essentially the study of uncertainty, and that the statistician’s role is to assist other fields that encounter uncertainty in their work. In practice, statistics is ordinarily associated with data, and it is the link between the variability in the data and the uncertainty inherent in the phenomenon under study that has occupied statisticians \cite{glymour_1980_theory}.

\medskip

As a final note, let us emphasize how these two approaches can and do coexist in science. It is often said that science proceeds by formulating hypotheses and making predictions, which are then compared and benchmarked against experimental results. While this description applies well to many areas, it is a simplification and not universally accurate. Some sciences—such as Newtonian mechanics, much of physics, and chemistry—rely on building precise models that generate quantitative predictions later tested by experiment. A clear example is the use of Newtonian mechanics to predict where and when a stone will fall when thrown, followed by experimental measurement.

By contrast, a paradigmatic example of an inference-driven scientific theory is Darwinian evolution, which does not aim primarily to predict individual outcomes but rather to reconstruct and explain observed patterns from available evidence. This distinction is worth emphasizing, as definitions of science that focus exclusively on predictive power can be misleading. Different sciences may differ substantially in methods, instrumentation, and conceptual tools, yet they are all equally legitimate in their aims and explanatory roles \cite{stigler_1986_history, hacking_1975_emergence}.

% Population and sampling .............................................................................................
\section{Population and sampling}

A large part of the history of science can be seen as a continuous effort to translate observations of reality into precise mathematical terms. Describing natural phenomena in numerical language requires tools that relate one or more relevant quantities—often called \textit{variables}—and explain how they change with respect to one another. The goal of modelling may be, for example, to determine the distance from the Earth to the Sun, estimate the number of stars in the observable universe, or relate the incidence of lung cancer to environmental or behavioral factors, such as smoking.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.7\textwidth]{5_figures/chapter1/prediction_vs_inference.png}
    \caption{Representation of two complementary approaches to randomness and variability: a predictive, theory-driven path from model to experiment, and a descriptive, inference-based path from data to explanation. Probability and statistics are normally framed as the branches of mathematics addressing both ways.}
    \label{fig:prediction_vs_inference}
\end{figure}

\medskip

In the same way mathematics is often summarized as the tasks related to \textit{count}, \textit{measure}, and \textit{sort}, statistical problems can be grouped into three broad categories. \textit{Sampling} concerns the selection of a finite set of observations from a larger, typically unknown population. \textit{Estimation} involves constructing numerical summaries that describe how the data are distributed, while \textit{visualization} addresses how observations are represented and how such representations influence interpretation. All three are fundamentally concerned with uncertainty and variability.

\medskip

Hence, all statistical inquiries begin with observations and measurements, which we normally refer to as \textit{data}. And data begins with the act of selection, or \textit{sampling}. The natural world offers a vast abundance of phenomena, with endless opportunities for observation, but only a finite subset can ever be recorded. This distinction gives rise to two central notions: the \textit{population}, which we denote by $\mathcal{P}$, represents the complete set of all possible observations under study. We will write it as
\begin{equation}
	\mathcal{P} = \{x_1, x_2, \dots, x_N\} \; .
\end{equation}

The \textit{sample} $\mathcal{S}$, on the other hand, is the finite subset actually collected. For a series of $N$ observations $x_1$, $x_2$, ..., $x_N$, a sample of just $n$ elements—less than the total, which is denoted by the upper case $N$—is defined as
\begin{equation}
	\mathcal{S} = \{x_1, x_2, \dots, x_n\}, \quad n < N \; ,
\end{equation}
The population represents the ideal object of inference, while the sample is the concrete, finite evidence available to us.  As an example, if I want to study some disease and its relation smokers in a given country, the behavior of a specific population, or the active genes in the genome, I will never have access to the \textit{complete population}, but only to the amount of them that I am able to question, measure, or survey. This distinction is far from trivial. A poorly chosen sample often misrepresents the population and may induce bias, whereas a carefully constructed one mirrors its essential features, and can be used to describe the underlying nature.

\medskip

Equally important is the recognition that not all data is equal, neither behaves in the same way. A common distinction is to consider \textit{categorical} and \textit{numerical} data. Categorical—or \textit{qualitative}—data describes qualities or labels such as the eye colour of students in a classroom (blue, brown, green), the brand of a purchased smartphone, etc. Sometimes they are further divided into \textit{nominal} categories, with no natural order, like the eye colour or the smartphone brand, and \textit{ordinal} categories with a meaningful order. Examples of these would be the finishing places in a race (first, second, third), survey responses ranging from \textit{strongly disagree} to \textit{strongly agree}, etc.

\medskip

The other big family is normally referred to as numerical—or \textit{quantitative}—data. It represents numerical quantities and is often subdivided into \textit{discrete}, countable numbers, such as the number of books on a shelf (4, 5, 6) or the number of goals scored in a match, and \textit{continuous} values that can take any number within a range, such as the time a sprinter takes to run 100 meters, or the height of a person measured with some arbitrary precision.

\medskip

Distinguishing between these types is no mere slang; different types of observations require different mathematical tools, and will be described in different ways. For example, it would not make sense to compute a mean out of smartphone brands, but to compute the mean of their prices is informative. Similarly, the distribution of finishing places after a race might be summarized by a median position, whereas heights of athletes could be studied with averages and measures of spread. A correct classification of data is thus a safeguard against misuse and a guide toward insight.

\medskip

As a summary, sampling and proper description of data establish the ground upon which statistics is built. Before calculating, summarizing, or diving into inference, one must ensure that the information collected is both \textit{representative and properly understood}. Without these foundations, descriptive measures risk floating unmoored, detached from the reality they claim to represent. Accurate sampling and rigorous description will lead to a faithful representation of the phenomena under study and their relationships, detecting anomalies, and even building accurate predictions.

\medskip

Let's end this section with a historical note. As we have mentioned, uncertainty has long been associated with games of chance and gambling, but it was not addressed as a statistical problem until much later. The Royal Statistical Society, founded in 1834, together with many other early statistical organizations, was originally established to gather and publish data, with the aim of informing social and economic questions through systematic measurement. It did not take long before statisticians began to ask how such data might best be analyzed and interpreted, and modern \textit{statistical inference} gradually emerged. Among the Society’s influential early figures were Adolphe Quetelet, whose work helped establish statistics as a tool for studying social phenomena, and Charles Babbage, who advocated quantitative approaches to scientific and administrative problems.

Among its most famous members was Florence Nightingale, admitted in 1858 as the Society’s first female member, whose work exemplified the practical use of statistical reasoning in policy and public health. In the early $20^{\text{th}}$ century, foundational questions concerning probability and inference were further clarified through the work of thinkers such as Frank P. Ramsey, who connected probability with rational belief and decision-making \cite{ramsey_1931_foundations}. Other notable presidents of the Royal Statistical Society have included William Beveridge and Ronald Fisher, whose contributions will be discussed in Chapter~\ref{chapter4}.

\medskip

Andrew Lang's famous quote \textit{"most people use statistics as a drunken man uses lamp-posts—for support rather than illumination"}, highlights the tendency to use statistics as a crutch, relying on them for validation rather than seeking genuine understanding. Lang's observation serves as a cautionary reminder to approach statistical data with critical thinking and not merely as a tool to bolster preconceived notions.

% Central tendency and variation ......................................................................................
\section{Central tendency and variation}

Once we have drawn a clear distinction between the population under study and the selected sample, we immediately face a fundamental problem. Neither the population mean—often referred to as the \textit{true} mean and usually denoted by $\mu$—nor the population variance, known as the \textit{true} variance and written as $\sigma^2$, are directly available to us. As we have seen, the only information at our disposal is the finite set of observations contained in our sample. From this limited information, we therefore seek to construct quantities that provide insight into key features of the population, such as its central value or its variability. These quantities are known as \textit{statistical estimators}. Common examples include the \textit{sample mean}, the \textit{median}, the \textit{variance} and the \textit{standard deviation}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{5_figures/chapter1/estimators.png}
    \caption{Representation of the \textit{true} population mean $\mu$, in black, and the observed \textit{sample} mean $\bar{x}$. The true mean is and ideal and unaccessible quantity, while the sample mean can be computed as an estimator of the finite sample.}
    \label{fig:estimators}
\end{figure}

\medskip

Once observations have been collected, a natural question arises: what is the \textit{center}, or \textit{typical}, value of this data set? Measures of central tendency address this question by summarizing the data with a single representative number, offering an immediate sense of where the observations are located within the distribution.

\medskip

\textbf{The sample mean:}

\medskip

The \textit{sample mean}, or \textit{average} is perhaps the most familiar measure of central tendency. Imagine we are doing an experiment where we measure some variable, and let's call it $x$ for simplicity. $x$ can be anything we could measure, like position at a given time, energy of some system, concentration of a specific substance, etc. Let's imagine we repeat the measurement $n$ times, and we obtain the values $x_1, x_2, \dots, x_n$. That will be our set of observations—our \textit{sample}—$\mathcal{S}$. We could simply write it as a list—or a \textit{vector}—in the following way:
\begin{equation}
	\mathcal{S} = \{x_1, x_2, \dots, x_n\} \; . \nonumber  
\end{equation}

Keep in mind that from the mathematics perspective the word \textit{vector} has a slightly different meaning, with subtleties related to algebraic operations and relations they should satisfy, but for the purpose of this course, where we prioritize above all simplicity, a vector and a list of numbers will be essentially the same thing.

\medskip

We can define the sample mean of an arbitrary large sample of $n$ observations, as the sum of all elements divided by the total. We will write it as $\bar{x}$, and define it as follows:
\begin{equation}
	\bar{x} = \frac{1}{n} (x_{1} + x_{2} + ... + x_{n}) \; .
	\label{eq:sample_mean1}
\end{equation}

We can write this in a slightly more compact way as a \textit{summation}, as follows:
\begin{equation}
	\bar{x} = \frac{1}{n} \sum_{i = 1}^{n} x_{i} \; .
	\label{eq:sample_mean2}
\end{equation}

Here we denote the sum of all elements $x_{i}$ with the greek letter $\sum$, starting with the first one ($x_1$, for $i = 1$) and until the last one ($x_n$, for $i = n$). The expressions \eqref{eq:sample_mean1} and \eqref{eq:sample_mean2} mean \textit{exactly} the same thing, just written in different ways.
\medskip

Let's illustrate with an example. Suppose we repeat a measurement three times, obtaining the results $x_1 = 1$, $x_2 = 2$, and $x_3 = 3$. Our sample is then $\mathcal{S} = \{1, 2, 3\}$, and the sample mean is
\begin{equation}
	\bar{x} = \frac{1}{3} \sum_{i = 1}^{3} x_{i} = \frac{1}{3} (1 + 2 + 3) = 2 \; . \nonumber
\end{equation}

As a warm-up exercise, try computing the same mean value for a second sample, let's say $\mathcal{S} = \{4, 5, 6\}$. Substituting into the general expression \eqref{eq:sample_mean2} gives
\begin{equation}
	\bar{x} = \frac{1}{3} \sum_{i = 1}^{3} x_{i} = \frac{1}{3} (4 + 5 + 6) = 5 \; . \nonumber
\end{equation}

As we see, the sample mean captures information about the "central" value, where most events cluster. Although useful, it is sensitive to extreme values—often called \textit{outliers}—which motivates the definition of additional, more robust measures of central tendency.

\medskip

\textbf{The median:}

\medskip

The \textit{median} represents similar information, as the value that splits the ordered data set in half. For an ordered sample $x_{(1)} \le x_{(2)} \le \dots \le x_{(n)}$, the median $M$ is defined as
\begin{equation}
	M = \begin{cases} 
		x_{(k+1)} \; , & \text{if } n = 2k+1 \text{ (odd)} \; , \\[2mm]
		\dfrac{x_{(k)} + x_{(k+1)}}{2} \; , & \text{if } n = 2k \text{ (even)} \; .
	\end{cases}
	\label{eq:median}
\end{equation}

Note that here $k$ is just an integer that helps locate the middle position of an ordered data set of size $n$. If the sample size $n$ is even, we write $n = 2k$, while for $n$ odd, we write $n = 2k + 1$. In the case of an odd-sized sample, the median is just the middle-point, while for an even size, it is computed as the average of the two middle points.

\medskip 

The mathematical definition \eqref{eq:median} may seem a bit unnatural at first, so let's navigate it with a couple of examples. Consider the sample $\mathcal{S} = \{1, 2, 3, 5, 3, 2, 7\}$. First, we order the data: 
\begin{equation}
	\mathcal{S}_{\text{ordered}} = \{1, 2, 2, 3, 3, 5, 7\} \; . \nonumber
\end{equation}

Since the sample has an odd number of elements ($n = 7$), the median is just the middle value: 
\begin{equation}
	M = x_{(4)} = 3 \; . \nonumber
\end{equation}

Now consider an even-sized sample $\mathcal{S} = \{1, 2, 3, 5, 4, 3, 2, 7\}$. Ordering the data gives 
\begin{equation}
	\mathcal{S}_{\text{ordered}} = \{1, 2, 2, 3, 3, 4, 5, 7\}. \nonumber
\end{equation}

Which has now an even number of elements ($n = 8$). Hence, applying such case in \eqref{eq:median}, the median is the average of the two middle values
\begin{equation}
	M = \frac{x_{(4)} + x_{(5)}}{2} = \frac{3 + 3}{2} = 3 \; . \nonumber
\end{equation}

Unlike the mean, the median is robust to outliers and skewed data, capturing the central position of the dataset even with repeated values. To illustrate that, let's have a look at the following sample $\mathcal{S} = \{1, 2, 3, 3, 4, 4, 200\}$, which contains the value $200$ as a huge outlier. The sample mean would be
\begin{equation}
	\bar{x} = \frac{1}{7}(1 + 2 + 3 + 3 + 4 + 4 + 200) = \frac{217}{7} = 31 \; . \nonumber
\end{equation}

While the median, given a size $n = 7$ would just be the middle (4th) value
\begin{equation}
    M = 3 \; . \nonumber
\end{equation}

For instance, the data represented in LHS of Figure \ref{fig:distribution_visual_comparison} will be accurately described by computing the mean, given its symmetric behaviour, while the one in the RHS will be better addressed with a median, accounting for the skewness and the presence of outliers.

\medskip

\textbf{The sample variance:}

\medskip

Beyond central location, it is important to understand the \textit{spread} of the data. We define the \textit{sample variance} $s^2$ of a data set as a quantity that captures how far the elements are from the mean value:
\begin{equation}
	s^2 = \frac{1}{n - 1} \sum_{i = 1}^{n} (x_{i} - \bar{x})^{2} \; .
	\label{eq:sample_variance}
\end{equation}

The factor $n - 1$ in the denominator of \eqref{eq:sample_variance} is called the \textit{Bessel correction factor}. A technical explanation is that this correction ensures that $s^2$ is an \textit{unbiased estimator} of the population variance, a concept we will discuss in Chapter \ref{chapter3}. In particular, it reflects the fact that the sample mean $\bar{x}$ is itself estimated from the data.

\medskip

Note that the variance is obtained by summing the squared differences between each observation and the mean. The differences are squared so that positive and negative deviations do not cancel and the result is always non-negative. If all elements in the sample are very close to the mean, the variance $s^2$ will be small; if the elements are widely spread, the variance will be larger.

\medskip

Let us illustrate this definition with an example. For the sample $\mathcal{S} = \{1, 2, 3\}$, which has $n = 3$ observations and sample mean $\bar{x} = 2$, the variance is
\begin{equation}
	s^2 = \frac{1}{3 - 1} \sum_{i = 1}^{3} (x_{i} - \bar{x})^{2} = \frac{1}{2} \big((1 - 2)^{2} + (2 - 2)^{2} + (3 - 2)^{2}\big) = \frac{1}{2} (1 + 0 + 1) = 1 \; , \nonumber
\end{equation}

which we could interpret as, on average, the elements of the list being \textit{one unit} away from the mean. 

\medskip

As a warm-up exercise, try to compute the variance for a second sample, let's say $\mathcal{S} = \{4, 5, 6\}$. By substituting in the general expression \eqref{eq:sample_variance} you should get the result
\begin{equation}
	s^2 = \frac{1}{3 - 1} \sum_{i = 1}^{3} (x_{i} - \bar{x})^{2} = \frac{1}{2} \big((4 - 5)^{2} + (5 - 5)^{2} + (6 - 5)^{2}\big) = \frac{1}{2} (1 + 0 + 1) = 1 \; . \nonumber
\end{equation}

We obtain again a variance $s^2 = 1$, indicating as in the previous example, that the elements of this sample $\mathcal{S}$ are also \textit{one unit} away from the mean.

\medskip

\noindent\textbf{The standard deviation:}

\medskip

Another useful quantity used to characterize variability is the so-called \textit{standard deviation}, which is simply the square root of the variance,
\begin{equation}
	s = \sqrt{\frac{1}{n - 1} \sum_{i = 1}^{n} (x_{i} - \bar{x})^{2}} \; ,
	\label{eq:sample_std}
\end{equation}
and therefore has the same units as the original data. While the variance measures spread in squared units, the standard deviation provides a more interpretable measure of how far, on average, observations lie from the mean.

At a glance, variance and standard deviation both quantify how much the elements of a data set deviate from the mean, capturing the notion of \textit{spread}.

\medskip

\noindent\textbf{Quantiles:}

\medskip

Finally, \textit{quantiles} provide another way to describe the distribution of data by dividing an ordered data set into equal proportions. The $p$-th quantile $Q_p$ is defined as the value below which a fraction $p$ of the data lies. Common special cases include the \textit{first quartile} ($Q_1$, corresponding to the 25th percentile), the \textit{median} ($Q_2$, the 50th percentile), and the \textit{third quartile} ($Q_3$, the 75th percentile). Informally, they answer questions of the form: \emph{“What value separates the lowest $p\%$ of observations from the rest?”} They are particularly useful for describing skewed data and for comparing distributions without relying solely on the mean. To illustrate this idea with a simple example, consider the ordered sample
\[
	\mathcal{S} = \{1, 2, 3, 4, 5, 6, 7, 8\} \; .
\]
Since there are $n = 8$ observations, the median $Q_2$ lies halfway between the fourth and fifth elements and is therefore
\[
	Q_2 = \frac{4 + 5}{2} = 4.5 \; .
\]
The first quartile $Q_1$ separates the lowest $25\%$ of the data and lies between the second and third elements, while the third quartile $Q_3$ lies between the sixth and seventh elements. In this way, quartiles summarize the distribution by marking its lower tail, central region, and upper tail.

\medskip

A rigorous definition of quantiles requires the notion of a distribution function and cumulative probability, which we will introduce in the next chapter. For a continuous cumulative distribution function (CDF) $F$, the $p$-th quantile satisfies
\begin{equation}
	Q_p = \inf \{ x : F(x) \ge p \} \; .
	\label{eq:quantiles}
\end{equation}

\medskip

In summary, the mean, median, mode, variance, standard deviation, and quantiles provide a rich and complementary view of a data set’s central tendency and variability. Together, they allow for both numerical and graphical summaries that capture essential features of the data.

\medskip

Variation is not merely a technical detail; it is the essence of uncertainty. Without spread, probability would be trivial, as every outcome would be identical. It is precisely in the differences among observations that statistical inquiry finds its substance. Central tendency and variation thus form complementary lenses through which data become intelligible: they allow us to assess whether groups are similar or different, whether an observation is ordinary or surprising, and whether observed variation can plausibly be attributed to chance. In this sense, descriptive statistics foreshadows the inferential methods to come, hinting at deeper regularities beneath the numbers.

% Data visualization ..................................................................................................
\section{Data visualization}

While numerical summaries are useful, the human mind often grasps patterns much more quickly through vision than through calculation. By \textit{data visualization} we mean a family of techniques used to transform numbers and sequences into shapes, colours, and spatial structures that are easier to interpret and can often be understood at a glance. Visualization turns abstraction into perception and frequently reveals regularities that remain hidden when data are examined only through formulas or numerical summaries. Today, a broad range of disciplines grouped under the name of data visualization—or data \textit{representation}—have become central pillars of scientific practice and data-driven inquiry.

\begin{figure}[ht]

    \centering

    % ---------------- Row 1: Small bins ----------------
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{5_figures/chapter1/histogram_1_bins_small.png}
        \caption{Histogram (clean data).}
        \label{fig:histogram_1_small}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{5_figures/chapter1/histogram_2_bins_small.png}
        \caption{Histogram (with outliers).}
        \label{fig:histogram_2_small}
    \end{subfigure}

    \medskip

    % ---------------- Row 2: Large bins ----------------
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{5_figures/chapter1/histogram_1_bins_large.png}
        \caption{Histogram (clean data).}
        \label{fig:histogram_1_large}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{5_figures/chapter1/histogram_2_bins_large.png}
        \caption{Histogram (with outliers).}
        \label{fig:histogram_2_large}
    \end{subfigure}

    \medskip

    \caption{Comparison of graphical summaries for $n=100$ observations drawn from a Gaussian distribution. Left column: uncontaminated data, with both small and large binning size. Right column: data affected by outliers, with both small and large binning size. Figure shows how a small bin size may give lead to resoulution, but also introduce noise.}
    \label{fig:histogram_comparison}
    
\end{figure}

\medskip

Among the oldest and most fundamental visualization tools is the \textit{histogram}. The idea of dividing data into intervals in order to visualize frequency dates back to the late $19^{\text{th}}$ century, when Karl Pearson formalized the histogram as a graphical representation closely connected to probability distributions \cite{pearson_1892_grammar, pearson_1895_contributions}. A histogram divides the range of a data set into consecutive intervals, or \textit{bins}, and represents the number—or relative \textit{frequency}—of observations falling within each bin by the height of a bar. This simple yet powerful plot provides an immediate visual impression of the distribution, allowing one to identify symmetry, skewness, concentration of values, and potential gaps. For instance, a symmetric histogram suggests a roughly balanced distribution around the mean, whereas a right-skewed histogram indicates that large values are less frequent but may still exert a strong influence on measures such as the mean. Histograms are therefore widely used in the physical sciences and in contexts where data are compared to theoretical or mathematical predictions.

\begin{figure}[ht]
    \centering

    % ---------------- Row 1: Histograms ----------------
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{5_figures/chapter1/histogram_1.png}
        \caption{Histogram (clean data).}
        \label{fig:histogram_1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{5_figures/chapter1/histogram_2.png}
        \caption{Histogram (with outliers).}
        \label{fig:histogram_2}
    \end{subfigure}

    \medskip

    % ---------------- Row 2: Box plots ----------------
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{5_figures/chapter1/box_1.png}
        \caption{Box plot (clean data).}
        \label{fig:box_1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{5_figures/chapter1/box_2.png}
        \caption{Box plot (with outliers).}
        \label{fig:box_2}
    \end{subfigure}

    \medskip

    % ---------------- Row 3: Violin plots ----------------
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{5_figures/chapter1/violin_1.png}
        \caption{Violin plot (clean data).}
        \label{fig:violin_1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{5_figures/chapter1/violin_2.png}
        \caption{Violin plot (with outliers).}
        \label{fig:violin_2}
    \end{subfigure}

    \caption{Comparison of graphical summaries for $n=100$ observations drawn from a Gaussian distribution.  
    Left column: uncontaminated data. Right column: data affected by outliers.  
    The rows show, respectively, histograms, box plots, and violin plots, illustrating how different visualization techniques respond to skewness and extreme observations.}
    \label{fig:distribution_visual_comparison}

\end{figure}

\medskip

Constructing an informative histogram requires some care. As a general rule, bins should be of equal width and together cover the entire range of the data, while reflecting natural groupings when possible. The choice of bin width plays a crucial role: using many narrow bins increases resolution but can introduce spurious fluctuations, especially in the presence of outliers, whereas using a small number of wide bins produces a smoother and more robust picture at the cost of detail. Because extreme values can strongly affect the appearance of a histogram, they must be handled thoughtfully. For highly skewed distributions, numerical summaries such as the median and the interquartile range (IQR) often provide a more stable description than the mean.

\begin{figure}[ht]
    
    \centering

    % ----------- Subfigure 1 -----------
    \begin{subfigure}{0.8\textwidth}
        \centering
        \includegraphics[width=0.6\textwidth]{5_figures/chapter1/mean_std_hist.png}
        \caption{Three sets of observations $\chi_1, \chi_2, \chi_3$ of sample size $n = 100$ drawn from a Gaussian distribution, with the mean value and standard deviation represented as a histogram.}
        \label{fig:mean_std_hist}
    \end{subfigure}

    \vspace{1.2em}

    % ----------- Subfigure 2 -----------
    \begin{subfigure}{0.8\textwidth}
        \centering
        \includegraphics[width=0.6\textwidth]{5_figures/chapter1/mean_std_box.png}
        \caption{Three sets of observations $\chi_1, \chi_2, \chi_3$ of sample size $n = 100$ drawn from a Gaussian distribution, with the mean value and standard deviation represented as a box plot.}
        \label{fig:mean_std_box}
    \end{subfigure}

    \vspace{1.2em}

    % ----------- Subfigure 3 -----------
    \begin{subfigure}{0.8\textwidth}
        \centering
        \includegraphics[width=0.6\textwidth]{5_figures/chapter1/mean_std_violin.png}
        \caption{Three sets of observations $\chi_1, \chi_2, \chi_3$ of sample size $n = 100$ drawn from a Gaussian distribution, with the mean value and standard deviation represented as a violin plot.}
        \label{fig:mean_std_violin}
    \end{subfigure}

    % ----------- Global caption -----------
    \caption{Comparison of three visualization methods—histogram, box plot, and violin plot—showing the mean and variability of three samples of size $n = 100$ drawn from a Gaussian distribution.}
    \label{fig:mean_std_all}

\end{figure}

\medskip

Another widely used visualization is the \textit{box plot}, also known as the \textit{box-and-whisker} plot, introduced by John Tukey in 1970 as part of his work on exploratory data analysis \cite{tukey_1977_eda}. The box plot offers a compact summary of a data set’s central tendency, spread, and potential outliers. It is constructed from five key statistics: the minimum, first quartile ($Q_1$), median ($Q_2$), third quartile ($Q_3$), and maximum. The box itself represents the interquartile range ($\mathrm{IQR} = Q_3 - Q_1$), while observations lying more than 1.5 times the IQR from the quartiles are typically flagged as outliers. Box plots allow for rapid comparisons across multiple groups and are particularly effective at revealing asymmetry and variability without being overly influenced by extreme values. For this reason, they are especially common in biological, medical, and clinical sciences, where experiments are often repeated many times with relatively small sample sizes.

\medskip

More recently, the \textit{violin plot} has emerged as a refinement of the box plot, combining summary statistics with a smooth representation of the distribution’s shape. Although its precise origin is less sharply documented, the violin plot gained prominence in the late $20^{\text{th}}$ century with the development of statistical software environments such as \textsf{R} and \textsf{Python}, particularly following the work of Hintze and Nelson in the 1990s \cite{hintze_1997_violin}. A violin plot augments the box plot by incorporating a kernel density estimate of the data, producing a mirrored shape that reflects the underlying distribution. In addition to displaying the median and quartiles, violin plots reveal features such as skewness and multimodality that may be obscured in simpler summaries, making them especially useful for comparing several groups side by side.

\medskip

The importance of visualization extends beyond convenience. Early work by Spearman on association \cite{spearman_1904_association} and later demonstrations such as Anscombe’s quartet \cite{anscombe_1973_graphs} highlighted a crucial lesson: data sets with nearly identical numerical summaries can exhibit dramatically different graphical structures. Visualization is therefore not merely a tool for presentation, but a fundamental instrument of statistical reasoning. By complementing numerical summaries, graphical representations help guard against misinterpretation and deepen our understanding of variability, structure, and uncertainty in data.

\medskip

As a note, let us emphasize that the roots of data visualization can be traced back well before modern statistics, to the early development of analytical geometry in the $17^{\text{th}}$ century. A decisive step was taken by René Descartes, whose introduction of coordinate systems in \textit{"La Géométrie"} (1637) provided a systematic way to represent numerical relationships geometrically \cite{descartes_1637_geometry}. By associating quantities with positions in space, Descartes established the conceptual framework underlying graphs, curves, and plots as representations of functional relationships. Although his work was not statistical in intent, it laid the mathematical foundation upon which later visual representations of empirical data would be built. Over the following centuries, this geometric perspective was gradually adapted to observational and social data, culminating in the $19^{\text{th}}$-century development of statistical graphics—such as frequency plots and histograms—as tools for exploring variability, association, and uncertainty.

\medskip

Following Descartes’ geometric framework, graphical representations of data evolved through the work of figures such as William Playfair, who in the late $18^{\text{th}}$ century introduced line and bar charts to depict economic time series, and Adolphe Quetelet, who applied statistical regularities to social phenomena. In the $19^{\text{th}}$ century, visualization became both an analytical and persuasive tool, exemplified by Florence Nightingale’s use of statistical graphics in public health and Charles Joseph Minard’s multivariate maps. These developments established visualization as a central instrument for exploring, communicating, and reasoning about data, long before the formalization of modern statistical theory \cite{minard_1862_graphique, playfair_1786_atlas}.
