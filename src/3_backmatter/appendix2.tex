\chapter{A review of functions and derivatives}
\label{appendix2}

Calculus begins with the study of how quantities depend on one another and how they change. At its foundation lies the concept of a \emph{function}, which formalizes the idea that one quantity is determined by another. Functions provide the language through which variation, motion, and growth are described in mathematics, physics, and the natural sciences.

\medskip

Historically, the notion of a function evolved gradually. Early uses appear implicitly in the work of Rene Descartes, who introduced coordinate geometry and expressed curves through algebraic equations \cite{descartes_1637_geometry}. The explicit concept of a function as a mapping between quantities was later clarified in the $18^{\text{th}}$ century by Leonhard Euler, whose writings established much of the notation and terminology still in use today \cite{euler_1748_introductio}.

\medskip

\noindent\textbf{Functions:}

Informally, a function assigns to each input value exactly one output value. Formally, a function \(f\) from a set \(A\) to a set \(B\) is written as
\[
f : A \to B,
\qquad x \mapsto f(x).
\]
This abstract definition allows the same concept to describe physical laws, statistical models, and mathematical relationships.

\medskip

A central idea in calculus is that of a \emph{limit}. Limits capture the behavior of a function as its input approaches a given value, even if the function is not defined or not well behaved at that point. Informally, limits allow us to reason about processes that involve approaching, rather than reaching, a value. This concept is essential for making precise sense of continuity, instantaneous change, and accumulation.

\medskip

\noindent\textbf{Limits:}

Given a function \(f\), we say that the limit of \(f(x)\) as \(x\) approaches \(a\) is \(L\) if
\[
\lim_{x \to a} f(x) = L.
\]
This notation expresses that the values of \(f(x)\) can be made arbitrarily close to \(L\) by choosing \(x\) sufficiently close to \(a\), regardless of the value of \(f(a)\) itself.

\medskip

Historically, the idea underlying limits emerged from attempts to reason about motion, change, and infinite divisibility. Ancient Greek philosophers, most famously in Zenoâ€™s paradoxes, highlighted conceptual difficulties surrounding infinite processes and the passage to a limit. Early mathematicians addressed these issues using geometric intuition rather than formal definitions, as seen in the method of exhaustion developed by Archimedes.

\medskip

In the seventeenth century, Isaac Newton and Gottfried Wilhelm Leibniz independently introduced differential calculus using intuitive notions of infinitesimal quantities and vanishing increments \cite{newton_1687_principia,leibniz_1684_nova}. While enormously successful in applications, these approaches relied on informal reasoning that left the logical foundations of calculus unclear.

\medskip

A decisive conceptual shift occurred in the nineteenth century, particularly through the work of Augustin-Louis Cauchy. Cauchy replaced infinitesimal arguments with definitions based on limiting processes, making explicit the dependence of approximation on arbitrarily small errors \cite{cauchy_1821_analyse}. This transition laid the groundwork for the modern, rigorous treatment of limits and continuity, later refined through precise \(\varepsilon\)--\(\delta\) definitions.

\medskip

From a philosophical perspective, the concept of a limit provides a way to reason about idealized processes without requiring that they ever be completed. It allows mathematics to describe continuous change using finite statements, reconciling infinite processes with logical precision. Limits thus form the conceptual bridge between discrete approximation and continuous behavior, underpinning the modern formulation of calculus \cite{courant_1965_calculus}.

\medskip

The derivative arises from the study of limits and provides a precise definition of instantaneous rate of change. Geometrically, the derivative of a function at a point corresponds to the slope of the tangent line at that point. Physically, it describes quantities such as velocity or growth rate.

\medskip

\noindent\textbf{Derivatives:}

The derivative of a function \(f\) at a point \(x\) is defined as the limit
\[
f'(x) = \lim_{h \to 0} \frac{f(x+h) - f(x)}{h},
\]
provided this limit exists. This definition links algebraic computation with geometric intuition.

\medskip

The development of differential calculus is closely associated with Isaac Newton and Gottfried Wilhelm Leibniz, who independently formulated its fundamental principles in the late $17^{\text{th}}$ century \cite{newton_1687_principia,leibniz_1684_nova}. Newton emphasized motion and physical interpretation, while Leibniz introduced a symbolic notation that proved especially flexible and influential.

\medskip

A decisive step toward modern calculus was taken in the $19^{\text{th}}$ century by Augustin-Louis Cauchy, who placed limits and derivatives on a rigorous analytical foundation. Cauchy replaced intuitive appeals to infinitesimals with precise definitions based on limiting processes, thereby clarifying continuity, convergence, and differentiation \cite{cauchy_1821_analyse}. This shift marked the transition from classical calculus to modern mathematical analysis.

\medskip

Earlier traditions also contributed important ideas related to change and approximation. In the Indian mathematical tradition, members of the Kerala school (fourteenth to sixteenth centuries) developed infinite series expansions for trigonometric functions and sophisticated approximation techniques motivated by astronomical computation \cite{pingree_1978_kerala}. In the medieval Arabic world, mathematicians refined algebraic methods, studied polynomial behavior, and developed geometric approaches to tangents and maxima, providing conceptual tools later absorbed into European mathematics through translation and transmission \cite{russell_1994_islamic}.

\medskip

From a philosophical standpoint, calculus represents an effort to make sense of continuous change using finite reasoning. The introduction of limits resolved long-standing paradoxes about infinity and infinitesimals by replacing informal arguments with precise definitions. Modern calculus, as taught today, builds on this foundation by emphasizing clarity, rigor, and conceptual understanding rather than purely mechanical computation \cite{courant_1965_calculus}.
