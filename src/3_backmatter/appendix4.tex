\chapter{Some special functions}
\label{appendix4}

In mathematics and statistics, certain functions appear so frequently and play such a foundational role that they are regarded as \emph{special functions}. Rather than being introduced for their own sake, these functions typically emerge from attempts to solve concrete problems involving error, accumulation, normalization, or symmetry. In probability and statistical inference, they often serve as canonical models or normalizing constants.

\medskip

\textbf{The Gaussian distribution:}

The Gaussian (or normal) distribution originated in practical problems of measurement and observation. When astronomers and physicists attempted to combine repeated measurements of the same quantity, they observed that small errors were common while large deviations were rare, and that positive and negative errors tended to balance symmetrically. In the early nineteenth century, Carl Friedrich Gauss showed that assuming such errors followed a specific bell-shaped curve led to mathematically optimal estimation procedures under reasonable symmetry assumptions \cite{gauss_1809_theoria}. Closely related ideas were also developed independently by Pierre-Simon Laplace in his analytic theory of probability \cite{laplace_1812_theorie}.

\medskip

In modern probability theory, the Gaussian distribution occupies a central position because of its stability under aggregation. The Central Limit Theorem explains that when many small, independent effects are added together, their combined outcome tends to follow a Gaussian distribution, regardless of the details of the individual components \cite{kolmogorov_1933_grundbegriffe}. This explains why Gaussian models arise so widely in natural and social phenomena.

\medskip

A real-valued random variable \(X\) is said to follow a Gaussian distribution with mean \(\mu\) and variance \(\sigma^2>0\) if its probability density function is
\[
    f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp \!\left(-\frac{(x-\mu)^2}{2\sigma^2}\right).
\]

\medskip

\textbf{The exponential distribution:}

The exponential distribution arises from the study of waiting times and the occurrence of events over time. Historically, it is closely connected to early work on stochastic processes and the modeling of random events, particularly in contexts where events occur continuously and independently, such as radioactive decay or arrivals to a queue. In the nineteenth century, Siméon-Denis Poisson’s work on rare events laid the groundwork for such models, while later developments by Andrey Markov clarified the role of memoryless processes \cite{poisson_1837_judgements,markov_1906_extension}.

\medskip

Intuitively, the exponential distribution describes situations in which the probability of an event occurring in the next instant does not depend on how long one has already waited. This \emph{memoryless} property distinguishes it from most other continuous distributions and makes it a natural model for lifetimes, waiting times, and durations until the next random event.

\medskip

A nonnegative random variable \(X\) is said to follow an exponential distribution with rate parameter \(\lambda>0\) if its probability density function is
\[
    f(x) = \lambda e^{-\lambda x}, \qquad x \ge 0.
\]
In probability and statistics, the exponential distribution plays a foundational role as the continuous analogue of the geometric distribution and as a building block for more complex stochastic models, including Poisson processes and reliability theory \cite{degroot_2012_probability}.

\medskip

\textbf{The Euler Gamma function:}

The Euler Gamma function arose from a simple but far-reaching question: how can the factorial function be extended beyond whole numbers? While the factorial \(n!\) counts arrangements for positive integers, many problems in analysis and probability require a smooth function that behaves like a factorial even when its argument is not an integer. Leonhard Euler addressed this problem in the eighteenth century by introducing what is now called the Gamma function, providing a continuous extension of the factorial concept.

\medskip

It arises in the evaluation of improper integrals, solutions of differential equations, and complex analysis through its analytic continuation and functional equation. In probability theory, the Gamma function appears systematically as a normalization factor ensuring that probability densities integrate to one

\medskip

For \(x>0\), the Gamma function is defined by the improper integral
\[
    \Gamma(x) = \int_0^\infty t^{x-1} e^{-t}\,dt.
\]
For positive integers \(n\), it satisfies \(\Gamma(n) = (n-1)!\). In probability theory, the Gamma function appears naturally in the normalization of continuous distributions and in the study of waiting times and scale parameters \cite{degroot_2012_probability}.

\medskip

\textbf{The Euler Beta function:}

The Euler Beta function is closely related to the Gamma function and arose historically from the study of definite integrals involving powers of variables constrained to a finite interval. Euler showed that many such integrals could be expressed compactly using a special two-parameter function, now known as the Beta function.

\medskip

The Beta function arises naturally when integrating products of powers over bounded intervals, making it a fundamental object in integral calculus and special-function theory. Through its relationship with the Gamma function, it encodes how probability mass is distributed between competing components constrained to sum to one. In probability theory, it serves as the normalizing constant for distributions on the unit interval and simplex, playing a key role in Bayesian modeling of proportions and probabilities.

\medskip

For \(x>0\) and \(y>0\), the Beta function is defined by
\[
    B(x,y) = \int_0^1 t^{x-1}(1-t)^{y-1}\,dt.
\]
A key identity links the Beta and Gamma functions:
\[
    B(x,y) = \frac{\Gamma(x)\Gamma(y)}{\Gamma(x+y)}.
\]

In probability and statistics, the Beta function appears as the normalizing constant of the Beta distribution, which is widely used to model proportions, probabilities, and prior beliefs in Bayesian inference \cite{jaynes_2003_logic}. Together, the Gamma and Beta functions illustrate how integral calculus underlies the mathematical structure of many statistical models.
