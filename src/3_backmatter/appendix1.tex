\chapter{A review of linear algebra}
\label{appendix1}

Linear algebra is one of the central languages of modern mathematics and science. It provides the formal framework for describing linear relations, symmetry, and structure, and it underlies vast areas of analysis, probability, statistics, physics, computer science, and data science. Historically, linear algebra did not arise as a single theory, but rather as a collection of methods developed to solve systems of linear equations, study geometry, and understand transformations of space. Only in the late nineteenth and early twentieth centuries was it unified into the abstract theory now taught under the name \emph{linear algebra} \cite{strang_2016_linear,axler_2015_linear}.

\medskip

From a philosophical perspective, linear algebra marks a transition from concrete computation to structural thinking. Instead of focusing on individual equations or numerical solutions, the subject emphasizes vector spaces, mappings between them, and invariant properties under changes of coordinates. This abstraction allows the same mathematical ideas to apply equally to geometry, differential equations, probability models, and statistical inference.

\medskip

A solid background in linear algebra typically includes the following core topics, which are either required or strongly recommended for further study in probability, statistics, and applied mathematics:

\medskip

Historically, the development of linear algebra is associated with several key figures and traditions spanning many centuries. Gauss laid the foundations for the systematic solution of linear systems through elimination methods \cite{gauss_1809_theoria}, while Hermann Grassmann introduced abstract vector spaces in his \emph{Ausdehnungslehre} (1844), a work far ahead of its time \cite{grassmann_1844_ausdehnungslehre}. Arthur Cayley and James Joseph Sylvester developed matrix theory in the $19^{\text{th}}$ century \cite{cayley_1858_matrices}, and David Hilbert and Emmy Noether later contributed decisively to the structural and axiomatic understanding of linear spaces and linear operators.

An often underemphasized contribution comes from the medieval Arabic mathematical tradition, which played a crucial role in the development of symbolic and algorithmic methods for solving equations. In particular, Muhammad ibn Musa al-Khwarizmi, working in $9^{\text{th}}$-century Baghdad, presented systematic procedures for solving linear and quadratic equations in his treatise \emph{Al-Kitab al-Mukhtasar fi Hisab al-Jabr wa-l-Muqabala} \cite{alkhwarizmi_820_aljabr}. Although formulated rhetorically rather than using modern symbolic notation, this work introduced rule-based manipulation of equations and classification of solution types. These methods strongly influenced later European mathematics through Latin translations \cite{rosen_1831_algebra} and helped establish algebra as a distinct discipline. The term \emph{algebra} itself derives from the word \emph{al-jabr} in the title of al-Khwarizmiâ€™s treatise, reflecting the historical continuity between equation solving, algorithmic reasoning, and the later emergence of linear algebra as a formal theory.

\medskip

\textbf{Vectors and vector spaces:}

Vectors are elements of a vector space over a field (typically \(\mathbb{R}\) or \(\mathbb{C}\)), equipped with addition and scalar multiplication. Linear combinations, span, linear independence, bases, and dimension formalize the idea of degrees of freedom and coordinate representations. Formally, a vector space \(V\) satisfies
\[
\alpha \mathbf{v} + \beta \mathbf{w} \in V
\quad \text{for all } \mathbf{v}, \mathbf{w} \in V \text{ and } \alpha, \beta \in \mathbb{F}.
\]

\medskip

\textbf{Matrices and systems of linear equations:}

Matrices arise naturally as representations of linear maps between finite-dimensional vector spaces. Central topics include matrix operations, rank, inversion, row reduction, and the solution of linear systems. A system of linear equations can be written compactly as
\[
A\mathbf{x} = \mathbf{b},
\]
where \(A\) is a matrix, \(\mathbf{x}\) the vector of unknowns, and \(\mathbf{b}\) the observation or constraint vector.

\medskip

\textbf{Linear transformations:}

Linear transformations are functions between vector spaces that preserve addition and scalar multiplication. For a linear map \(T : V \to W\),
\[
T(\alpha \mathbf{v} + \beta \mathbf{w}) = \alpha T(\mathbf{v}) + \beta T(\mathbf{w}).
\]
Key concepts include kernel and image, injectivity and surjectivity, change of basis, and composition of linear maps. This viewpoint emphasizes structure over computation and clarifies the geometric meaning of matrices.

\medskip

\textbf{Inner products and geometry:}

Inner product spaces introduce notions of length, angle, and orthogonality. An inner product \(\langle \cdot, \cdot \rangle\) induces a norm via
\[
\|\mathbf{v}\| = \sqrt{\langle \mathbf{v}, \mathbf{v} \rangle}.
\]
Important concepts include orthogonal projections, orthonormal bases, and the Gram--Schmidt process. These ideas are foundational for least squares methods, regression, and statistical estimation.

\medskip

\textbf{Eigenvalues and eigenvectors:}

Eigenvalues describe intrinsic directions and scaling properties of linear transformations. A scalar \(\lambda\) is an eigenvalue of a matrix \(A\) if there exists a nonzero vector \(\mathbf{v}\) such that
\[
A\mathbf{v} = \lambda \mathbf{v}.
\]
Diagonalization and spectral decomposition, especially for symmetric matrices, play a central role in applications ranging from differential equations to principal component analysis.

\medskip

\noindent\textbf{Determinants and volume:}

Although less central in modern abstract treatments, determinants provide geometric insight into volume, orientation, and invertibility. For a square matrix \(A\),
\[
\det(A) \neq 0 \quad \Longleftrightarrow \quad A \text{ is invertible}.
\]
Determinants also appear in change-of-variables formulas and multivariate analysis.

\medskip

In modern mathematics, linear algebra serves both as a computational toolkit and as a conceptual foundation. Mastery of its basic structures is essential not only for solving concrete problems, but also for understanding more advanced theories where linear spaces provide local or approximate descriptions of complex phenomena. As such, linear algebra is best learned not merely as a collection of techniques, but as a coherent framework for reasoning about structure, symmetry, and linearity.
