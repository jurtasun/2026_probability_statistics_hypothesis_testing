% Introduction ........................................................................................................
\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}

\epigraph{\textit{Even fire obeys the laws of numbers.}}{— J.B. Joseph Fourier}

\section*{A bit of history}
\addcontentsline{toc}{section}{A bit of history}

As one might expect, the origins of probability and related concepts can be traced back to very ancient times. Civilizations such as the Babylonians, Egyptians, and Greeks already encountered uncertainty in various aspects of life, including commerce, games of chance, and divination. Consequently, notions of randomness and stochasticity have deep historical roots. For instance, archaeological findings suggest that the earliest known dice date back over 5,000 years, reflecting humanity’s early fascination with chance and unpredictability \cite{finkel_2007_dice}. Although these cultures had not yet developed a formal mathematical theory of probability, they recognized recurring patterns in random events and attempted to anticipate outcomes through either empirical observation or superstition. For a detailed historical overview, see Florence Nightingale's 1962 manuscript \textit{"Games, Gods and Gambling"} \cite{david_1962_games}.

\medskip

While classical Greek and Roman philosophers frequently discussed the nature of chance, necessity, and determinism, their inquiries remained primarily philosophical rather than mathematical. From Aristotle to Cicero, many have distinguished between events occurring by chance and those determined by fate, foreshadowing later developments in probability theory \cite{cicero_45bce_divination}. These early ideas, though lacking quantitative formalism, provided the intellectual foundation for later scientific inquiry into randomness and causality.

A significant shift occurred during the late medieval and early Renaissance periods, when more rigorous mathematical ideas began to shape. Italian mathematician and gambler Gerolamo Cardano (1501–1576) is normally credited as making the first substantial contributions to the mathematical analysis of chance. His work \textit{"Liber de Ludo Aleae (Book on Games of Chance)"} \cite{cardano_1663_ludo}, posthumously published in 1663, is one of the earliest known texts to explore probability through the analysis of gambling problems. However, Cardano’s reasoning, while insightful, lacked the symbolic clarity and mathematical rigour of modern probability theory. Readers consulting the original manuscript will notice an ambiguous and sometimes inconsistent symbolic system, quite unlike the formal structures we use nowadays.

\medskip

The formalization of probability as a mathematical discipline did not occur until the $17^{\text{th}}$ century, most notably through the seminal correspondence between Blaise Pascal and Pierre de Fermat. Their work, motivated by problems such as finding a fair division of stakes in interrupted games of chance, introduced foundational concepts such as combinatorics, expected values, and variance \cite{devlin_2008_unfinished}. These developments paved the way for later contributions by Christiaan Huygens, who in 1657 wrote the first published textbook on probability \textit{"De Ratiociniis in Ludo Aleae (On Reasoning in Games of Chance)"} \cite{huygens_1657_ratiociniis}, and Jacob Bernoulli, whose 1713 \textit{"Ars Conjectandi (The Art of Conjecturing)"} remains among the most influential early texts in the field. Their works, along with many others, collectively laid the groundwork for the probabilistic and statistical methods that foreshadow modern scientific reasoning \cite{bernoulli_1713_ars, hald_1990_history}.

\medskip

From the $19^{\text{th}}$ century onwards, probability theory became increasingly intertwined with statistics and inference, giving rise to the mathematical and empirical frameworks that led to modern scientific analysis of uncertainty. Building on the analytical foundations laid by Laplace—who unified probability with inference through inverse probability—and Gauss’s theory of errors, Adolphe Quetelet—Belgian astronomer and mathematician—played a decisive role in extending probabilistic reasoning to social and biological phenomena. His notion of the statistical individual, or \textit{homme moyen}, framed variation not as noise to be eliminated, but as a fundamental object of study, helping to establish statistics as a distinct discipline concerned with populations rather than isolated events \cite{gauss_1809_theoria, laplace_1812_theorie, quetelet_1835_homme}.

Within this emerging landscape, Florence Nightingale stands as a central figure in the practical and institutional adoption of statistical reasoning. Through her innovative use of graphical representations and her advocacy for quantitative evidence in public health policy, she demonstrated the power of statistical methods as tools for decision-making under uncertainty \cite{nightingale_1858_notes}. In parallel, Joseph Fourier’s work on heat conduction introduced series expansions and integral transforms that, while originally developed in the context of mathematical physics, would later become indispensable in the study of random processes, signal analysis, noise, and diffusion. Although Nightingale and Fourier approached uncertainty from fundamentally different directions—one through empirical data and social reform, the other through mathematical analysis of physical systems—their contributions expanded the scope of probabilistic thinking and helped prepare the intellectual ground for the later development of stochastic processes and statistical physics.

\medskip

A further conceptual leap occurred in the early $20^{\text{th}}$ century with the work of Russian mathematician Andrey Markov. At a time when probability theory was still largely built on the assumption of independent events, he tried to understanding whether the familiar regular patterns of probability—such as averages stabilizing over time—could still arise when events influenced one another. To explore this question, he studied sequences in which the outcome of each step depends only on the immediately preceding one, rather than on the entire past.

This simple idea, later formalized as what are now called Markov chains, showed that randomness and order need not rely on complete independence. Even when successive events are linked, long-term statistical patterns can still emerge under suitable conditions. Markov’s work, developed within the strong Russian tradition of rigorous analysis, opened the door to the modern study of systems that evolve over time under uncertainty, from physical processes to language, biology, and many contemporary data-driven models \cite{markov_1906_extension}.

Markov’s investigations inaugurated the systematic study of dependence in stochastic processes and laid the groundwork for much of modern probability theory. The conceptual framework he introduced has since become central to a wide range of disciplines, including statistical mechanics, linguistics, finance, and more recently, machine learning and data science, where Markovian models serve as fundamental tools for modeling sequential and temporal data \cite{doob_1953_stochastic}.

\medskip

As a final note, the modern axiomatic formulation of probability was introduced in the early $20^{\text{th}}$ century by the Russian mathematician Andrey Kolmogorov. In his 1933 monograph \textit{"Grundbegriffe der Wahrscheinlichkeitsrechnung (Foundations of the Theory of Probability)"} \cite{kolmogorov_1933_grundbegriffe}, Kolmogorov synthesized classical and frequentist ideas into a rigorous mathematical framework based on measure theory. His axioms remain the standard foundation for probability theory to this day. It may seem surprising that a concept with such ancient origins was not formally axiomatized until relatively recent times, and we will return to Kolmogorov’s formulation and its implications in greater detail in further chapters. Nevertheless, philosophical discussions about the interpretation of probability and its relation to the physical sciences—especially in the context of determinism, epistemology, and modern topics such as quantum mechanics—predate Kolmogorov's formulation and continue to evolve to this day.
