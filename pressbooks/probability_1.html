<!-- Chapter 1. Descriptive statistics -->
<p class="p1" style="text-align: right"><em>Statistics is the grammar of science</em></p>
<p class="p1" style="text-align: right">— Karl Pearson</p>
&nbsp;

<p class="indent" style="text-align: justify; text-indent: 2em">A large part of history of science could be summarized as a continuous effort to translate observations of reality into precise, mathematical terms. To define mathematically the phenomena we find in the natural world, it is necessary to develop tools that express the one or more relevant quantities—sometimes called <em>variables—</em>and how they relate or change depending on one another. The purpose of such modelling might be, for instance, to determine the distance from the earth to the sun, to estimate the number of stars in the observable universe, or relating the number of lung cancer patients to pollution levels around smoking areas.</p>
<p class="indent" style="text-align: justify; text-indent: 2em">It has been said that mathematics could be summarized as all tasks related to <em>count</em>, <em>measure</em>, and <em>sort</em>. In a similar way, we could summarize all statistical issues as concern with <em>uncertainty</em>, or <em>variation</em> among observations. The word <em data-start="9" data-end="21">statistics</em> entered the English language through the work British politician and military officer John Sinclair, who in 1791 began publishing his monumental <em data-start="138" data-end="171">Statistical Account of Scotland, a </em>series of documentary publications covering life in Scotland in the 18th, 19th and 20th centuries. Adapting the German <em data-start="193" data-end="204">Statistik</em>—then denoting the descriptive study of states and their conditions—Sinclair expanded the term’s meaning to encompass “<em>an inquiry into the state of a country with respect to the happiness of its inhabitants</em>.” In doing so, he bridged the administrative traditional sense of the term with an empirical spirit that foreshadowed modern social science, into a discipline devoted to the systematic understanding of society itself. Through this chapter we will introduce terms such as description of populations, sampling, and chance. In further chapters we will develop and revisit ideas such as <em>randomness</em>, <em>relationship</em>, <em>correlation</em>, <em>confidence</em> and <em>reproducibility</em>, among others [...].</p>
<p class="indent" style="text-align: justify; text-indent: 2em">Historically, uncertainty has been associated with games of chance and gambling. The Royal Statistical Society, together with many other statistical groups, was originally set up to just <em>gather and publish data</em>, as an attempt to reduce its uncertainty. It remains an essential part of statistical activity today, and most Governments have statistical offices whose function is the plain acquisition and presentation of statistics. It did not take long before statisticians wondered how the data might best be used, and modern <em>statistical inference</em> was born.</p>
<p class="indent" style="text-align: justify; text-indent: 2em">The mathematical formalization of decision-making is actually quite a recent development. It is usually attributed to British mathematician Frank P. Ramsey (1903–1930), who in his 1926 paper <em>"Truth and Probability"</em> [...] introduced a formal, subjective interpretation of probability, laying the groundwork for what later became expected utility theory in decision-making under uncertainty. In short, Ramsey formalized how rational agents should assign probabilities and make decisions based on personal beliefs and preferences. All starting from the apparently simple question <em>"how should we make decisions in the face of uncertainty?"</em>.</p>
<p class="indent" style="text-align: justify; text-indent: 2em">Descriptive statistics gives us the tools to summarize, organize, and present data. By distilling observations into meaningful summaries, we find the first sense of order in apparent chaos. Descriptive statistics revolves around a few fundamental objects: data points, distributions, and summaries. Measures of central tendency—mean, median, mode—capture the “typical” observation, while measures of spread—variance, standard deviation, interquartile range—reveal how much observations fluctuate. Graphical tools such as histograms, box plots, and scatterplots turn abstract numbers into visible patterns, making insights immediate and intuitive. The development of measures like mean, median, and standard deviation emerged from this drive to quantify central tendencies and variability, giving rise to a shared language for describing empirical reality. The practical problems of descriptive statistics are deceptively simple: How do we summarize hundreds or thousands of observations? How do we detect patterns or outliers? How can we compare datasets rigorously? Each question demands careful choice of measure, sensitivity to data structure, and awareness of limitations. The challenge lies not in calculation alone, but in interpretation: a summary is meaningful only if it faithfully represents the underlying phenomena [...].</p>


<!-- 1.1. Sampling and data types -->
<h3 style="text-align: justify">1.1. Sampling and data types</h3>
<p class="indent" style="text-align: justify; text-indent: 2em">All statistical inquiries begin with observations and measurements, which we normally refer to as <em>data</em>. And data begins with the act of selection, or <em>sampling</em>. The natural world overflows with phenomena, offering endless opportunities for observation, but only a finite subset can ever be recorded. This distinction gives rise to two central notions: the <em>population</em>, which we will write as \(\mathcal{P}\), and the <em>sample</em>, denoted by \(\mathcal{S}\). By <em>population</em> we mean the complete set of all possible observations under study, normally written as
\[
\mathcal{P} = \{x_1, x_2, \dots, x_N\} \; .
\]</p>
<p class="indent" style="text-align: justify; text-indent: 2em">The <em>sample</em>, on the other hand, is the finite subset actually collected. For a series of \(N\) observations \(x_1\), \(x_2\), ..., \(x_N\), a sample of just \(n\) elements—less than the total, which is normally denoted by the upper case \(N\)—is defined as
\[
\mathcal{S} = \{\chi_1, \chi_2, \dots, \chi_n\}, \quad n &lt; N \; ,
\]</p>
<p class="indent" style="text-align: justify; text-indent: 2em">where the \(\chi_i\)-labels remind us that the sample consists of selected observations from the population, not necessarily consecutive or all of them. The population represents the ideal object of inference, while the sample is the concrete, finite evidence available to us. This distinction is far from trivial; a poorly chosen sample often misrepresents the population and may induce bias, whereas a carefully constructed one mirrors its essential features, and can be used to describe the underlying nature.</p>
<p class="indent" style="text-align: justify; text-indent: 2em">Equally important is the recognition that not all data are of the same kind. A common distinction is to consider <em>categorical</em> and <em>numerical</em> data. Categorical—or <em>qualitative—</em>data describes qualities or labels such as the eye colour of students in a classroom (blue, brown, green), the brand of a smartphone, etc. Sometimes they are further divided into <em>nominal</em> categories, with no natural order, like the eye colour or the smartphone brand, and <em>ordinal</em> categories with a meaningful order. Examples of these would be the finishing places in a race (first, second, third), survey responses ranging from <em>strongly disagree</em> to <em>strongly agree</em>, etc.</p>
<p class="indent" style="text-align: justify; text-indent: 2em">The other big group is normally referred to as numerical—or <em>quantitative—</em>data. These measure numerical quantities and are often subdivided into <em>discrete</em>, countable numbers, such as the number of books on a shelf (4, 5, 6) or the number of goals scored in a match, and <em>continuous</em> values that can take any number within a range, such as the time a sprinter takes to run 100 meters, or the height of a person measured with some arbitrary precision.</p>
<p class="indent" style="text-align: justify; text-indent: 2em">Distinguishing between these types is no mere slang; different types of observations require different mathematical tools, and will be described in different ways. For example, it would not make sense to compute a mean out of smartphone brands, but to compute the mean of their prices is informative. Similarly, the distribution of finishing places after a race might be summarized by a median position, whereas heights of athletes could be studied with averages and measures of spread. A correct classification of data is thus a safeguard against misuse and a guide toward insight.</p>
<p class="indent" style="text-align: justify; text-indent: 2em">As a summary, sampling and proper description of data establish the ground upon which statistics is built. Before calculating, summarizing, or diving into inference, one must ensure that the information collected is both representative and properly understood. Without these foundations, descriptive measures risk floating unmoored, detached from the reality they claim to represent. Accurate sampling and rigorous description will lead to a faithful representation of the phenomena under study and their relationships, detecting anomalies, and even building accurate predictions.</p>
<p class="indent" style="text-align: justify; text-indent: 2em">In the recent decades, experimental scientists have become aware of a certain tendency to use statistics as a crutch, relying on them for validation rather than seeking genuine understanding. Scottish poet Andrew Lang marvellously summarizes this tren in his famous quote <em>"most people use statistics as a drunken man uses lamp-posts—for support rather than illumination". </em>Lang's observation serves as a cautionary reminder to approach statistical data with critical thinking and not merely as a tool to bolster preconceived notions.</p>


<!-- 1.2. Central tendency and variation -->
<h3>1.2 Central tendency and variation</h3>
<p class="indent" style="text-align: justify; text-indent: 2em">Once observations have been collected, a natural question arises: what is the <em>central</em>, or <em>typical</em> value of this data set? Mathematical quantities that measure the central tendency will be useful to summarize our data with a single representative number, providing an immediate sense of location within the distribution.</p>
<p class="indent" style="text-align: justify; text-indent: 2em">The <em>mean</em>, or <em>average,</em> is perhaps the most familiar measure of central tendency. Imagine we are doing an experiment where we measure some variable, and let's call it \(x\) for simplicity. \(x\) can be anything we could measure, like number of tomatoes in a bag, position at a given time, energy of some system, concentration of a specific substance, etc. Let's imagine we repeat the measurement \(n\) times, and we obtain the values \(x_1, x_2, \dots, x_n\). That will be our set of observations, or our <em>sample</em> \(\mathcal{S}\). We could simply write it as a list—or a <em>vector—</em>in the following way:</p>
\[
\mathcal{S} = \{x_1, x_2, \dots, x_n\} \; .
\]
<p style="text-align: justify">Keep in mind that from the mathematics perspective the word <em>vector</em> has a slightly different meaning, with subtleties related to algebraic operations and relations they should satisfy, but for the purpose of this course, where we prioritize above all simplicity, a vector and a list of numbers will be essentially the same thing. We can define a quantity called the <em>mean—</em>or <em>average</em>—of an arbitrary large sample of \(n\) observations, as the sum of all elements divided by the total. We will write it as \(\bar{x}\), and define it as follows:\[
\bar{x} = \frac{1}{n} (x_{1} + x_{2} + ... + x_{n}) \; .
\]</p>
We can write this in a slightly more compact way as a <em>summation</em>, as follows:
\[
\bar{x} = \frac{1}{n} \sum_{i = 1}^{n} x_i \; .
\]
<p style="text-align: justify">Here we denote the sum of all elements \(x_{i}\) with the greek letter \(\Sigma\), starting with the first one (\(x_1\), for \(i = 1\)) and until the last one (\(x_n\), for \(i = n\)). Both definitions for \(\bar{x}\) mean <em>exactly</em> the same thing, just written in different ways.</p>
<p style="text-align: justify;text-indent: 2em">Let's pause here for a second, and give a note about notation. Remember the difference we made at the very beginning between sample and population, as notations may differ between different books and literature sources. Normally, the sample mean is written just as \(\bar{x}\) just as we did, while for the full population of \(N\) elements \(x_1, x_2, \dots, x_N\)—before any sampling—the <em>population mean</em> is normally denoted as \(\mu\), and defined accordingly,</p>
\[
\mu = \frac{1}{N} \sum_{i = 1}^{N} x_{i} \; .
\]
<p style="text-align: justify">We will see more about the difference between sample mean and population mean when we discuss parameter estimation in Chapter 3. For now just keep in mind that \(\bar{x}\) is the mean of our sample of just \(n\) drawn observations, while \(\mu\) refers to the mean of the idealized, complete population.</p>
<p class="indent" style="text-align: justify; text-indent: 2em">Let's illustrate with an example. Suppose we repeat a measurement three times, obtaining the results \(x_1 = 1\), \(x_2 = 2\), and \(x_3 = 3\). Our sample is then \(\mathcal{S} = \{1, 2, 3\}\), and the sample mean is computed by applying our definition,
\[
\bar{x} = \frac{1}{3} \sum_{i = 1}^{3} x_{i} = \frac{1}{3} (1 + 2 + 3) = 2 \; .
\]
As a warm-up exercise, try computing the same mean value for a second sample, let's say \(\mathcal{S} = \{4, 5, 6\}\). Substituting into the general expression we obtain
\[
\bar{x} = \frac{1}{3} \sum_{i = 1}^{3} x_{i} = \frac{1}{3} (4 + 5 + 6) = 5 \; .
\]
The mean captures information about the "central" value, where most events cluster. Although useful, it is sensitive to extreme values or <em>outliers</em>, which motivates the definition of additional, more robust measures of central tendency.</p>
<p style="text-align: justify;text-indent: 2em">The <em>median</em> represent similar information, as the value that splits the ordered data set in half. For an ordered sample \(x_{(1)} \le x_{(2)} \le \dots \le x_{(n)}\), the median \(M\) is defined in a different way if the list contains an even or odd number of observations. If \(n\) is even, meaning we can write it in terms of an arbitrary integer as \(n = 2k\), the median is just the middle-point
\[
M=x_{(k+1)} \; ,
\]
while if \(n\) is even, meaning that it can write it in terms of an arbitrary integer as \(n = 2k + 1\), the median is computed as the average of the two middle points
\[
M=\dfrac{x_{(k)} + x_{(k+1)}}{2} \; .
\]
This shifting definition we just wrote for \(M\) may seem a bit unnatural at first, so let's navigate it with a couple of examples. Consider the sample \(\mathcal{S} = \{1, 2, 3, 5, 3, 2, 7\}\). First, we order the data:
\[
\mathcal{S}_{\text{ordered}} = \{1, 2, 2, 3, 3, 5, 7\} \; .
\]
Since the sample has an odd number of elements \(n = 7\), the median is just the middle value:
\[
M = x_{(4)} = 3 \; .
\]
Now consider an even-sized sample \(\mathcal{S} = \{1, 2, 3, 5, 4, 3, 2, 7\}\). Ordering the data gives
\[
\mathcal{S}_{\text{ordered}} = \{1, 2, 2, 3, 3, 4, 5, 7\}.
\]
With has an even number of elements now, \(n = 8\). Hence, applying such case in our definition of \(M\), the median is the average of the two middle values:
\[
M = \frac{x_{(4)} + x_{(5)}}{2} = \frac{3 + 3}{2} = 3 \; .
\]
Unlike the mean, the median is robust to outliers and skewed data, capturing the central position of the dataset even with repeated values. For instance, the data represented in LHS of Figure \ref{fig:histogram_comparison1} will be accurately described by computing the mean, given its symmetric behaviour, while the one in the RHS will be better addressed with a median, accounting for the skewness and the presence of outliers.</p>
<p style="text-align: justify;text-indent: 2em">The <em>mode</em> is the value—or values—that appear most frequently in the observation set, which is quite a straightforward measure. For the first sample \(\mathcal{S} = \{1, 2, 3, 5, 3, 2, 7\}\) we just count the frequency of each value, and conclude that since both \(2\) and \(3\) occur most frequently, the dataset is <em>bimodal</em>, with modes \(2\) and \(3\). In the case of categorical data, such as eye colour or smartphone brands, the mode corresponds to the most common category.</p>
<p style="text-align: justify;text-indent: 2em">Beyond central location, it is important to understand the <em>spread</em> of the data. We can define the <em>variance</em> of a set, normally denoted by \(s^2\), as a quantity that captures how far are the elements from the mean value,
\[
s^2 = \frac{1}{n - 1} \sum_{i = 1}^{N} (x_{i} - \bar{x})^{2} \;
\]
and again, we will use a different notation for the <em>population variance</em>,
\[
\sigma^2 = \frac{1}{N} \sum_{i=1}^{N} (x_i - \mu)^2 \; .
\]
If we pay close attention, we see that the definitions of \(s^2\) and \(\sigma^{2}\) are not identical. The \(n-1\) in the denominator of the sample variance is called the <em>Bessel correction factor</em>, and it arises from the fact that treating finite samples is not the same as referring to the complete population. We will return to this topic in Chapter 3, when we discuss the concept of estimators and Maximum Likelihood Estimation.</p>
<p style="text-align: justify;text-indent: 2em">Note that the variance is just a sum of differences, and squared just so that we obtain a positive value. It is a measure starting with the first element (\(x_1\), for \(i = 1\)) and until the last one (\(x_N\), for \(i = N\)), of how far is each element from the mean value. If all elements in our sample are very close to the mean, then the sum of differences will be a small number, and we would get a variance \(s^2\) close to zero. Meanwhile, if the elements are very different, with large variation within the set, we would obtain a larger variance.</p>
<p style="text-align: justify;text-indent: 2em">Again, let's illustrate with an example. If we compute the variance of our very first example set \(\mathcal{S} = \{1, 2, 3\}\), which has just \(n = 3\) observations, we get
\[
s^2 = \frac{1}{3 - 1} \sum_{i = 1}^{3} (x_{i} - \bar{x})^{2} = \frac{1}{2} \big((1 - 2)^{2} + (2 - 2)^{2} + (2 - 3)^{2}\big) = \frac{1}{2} (1 + 0 + 1) = 1 \; ,
\]
which we could interpret as, on average, the elements of the list being \textit{one unit} away from the mean. As a warm up exercise, try to compute the variance for a second sample, let's say \(\mathcal{S} = \{4, 5, 6\}\). By substituting in the general expression of the sample variance you should get the following result
\[
s^2 = \frac{1}{3 - 1} \sum_{i = 1}^{3} (x_{i} - \bar{x})^{2} = \frac{1}{2} \big((4 - 5)^{2} + (5 - 5)^{2} + (6 - 5)^{2}\big) = \frac{1}{2} (1 + 0 + 1) = 1 \; .
\]
We obtain again a variance \(s^2 = 1\), indicating as in the previous example, that the elements of this sample \(\mathcal{S}\) are also <em>one unit</em> away from the mean.</p>
<p style="text-align: justify;text-indent: 2em">Another useful quantity used to characterize variability is the so called <em>standard deviation</em>, which is just the square root of the variance,
\[
s = \sqrt{\frac{1}{n - 1} \sum_{i = 1}^{n} (x_{i} - \bar{x})^{2}} \; ,
\]
and for the entire population,
\[
\sigma = \sqrt{\frac{1}{N } \sum_{i = 1}^{N} (x_{i} - \bar{x})^{2}} \; .
\]
The historical development of variance and standard deviation has roots in early statistical and error analysis work. The concept of variance was defined by Ronald Fisher in the early 1920s, though related ideas of squared deviations appeared earlier in Gauss’s work on error distributions when measuring star positions. The standard deviation, on the other hand, was named by Karl Pearson in 1890s, prior to which it was referred to as <em>root mean square error</em>. Pearson's naming made the concept more accessible and allowed widespread use for measuring data spread around the mean, upon which Fisher later built the idea of variance, as estimator of the variation within a sample.</p>
<p style="text-align: justify;text-indent: 2em">Finally, <em>quantiles</em> divide the ordered data into equal proportions. Mathematically, the \(p\)<sup>th</sup> quantile \(Q_p\) is the value below which a fraction \(p\) of the data lies. Special cases include the <em>first quartile</em> (\(Q_1\), or 25<sup>th</sup> percentile), the <em>median</em> (\(Q_2\), or 50<sup>th</sup> percentile), and the <em>third quartile</em> (\(Q_3\), or 75<sup>th</sup> percentile). Formally, for a continuous cumulative distribution function (CDF) \(F(x)\), the \(p\)<sup>th</sup> quantile satisfies
\[
Q_p = \inf \{ x : F(x) \ge p \}.
\]</p>
<p style="text-align: justify;text-indent: 2em">In summary, mean, median, mode, variance, standard deviation, and quantiles provide a rich, complementary view of the dataset’s central tendency and variability, allowing for both numerical and graphical summaries that capture the essence of the data.</p>
<p style="text-align: justify;text-indent: 2em">Variation is not merely a technicality; it is the very essence of uncertainty. Without spread, probability would be trivial, for every outcome would be the same. It is in the differences among observations that statistical inquiry finds its substance. Hence, central tendency and variation together provide the complementary lenses through which data becomes intelligible. They allow us to say whether two groups are alike or unlike, whether a new result is ordinary or surprising, whether the observed variation is too great to be dismissed as chance. In this sense, descriptive statistics foreshadows the inferential methods to come, hinting at deeper laws beneath the numbers.</p>


<!-- 1.3. Data visualization -->
<h3>1.3 Data visualization</h3>
<p class="indent" style="text-align: justify; text-indent: 2em">While numerical summaries are useful, the human mind often understands patterns much faster through vision than calculation. By <em>data visualization</em> we mean a series of techniques used to transform numbers and sequences into shapes, colours and structures that are easier to interpret, and that can be grasped at a glance. It turns abstraction into perception and often reveals regularities invisible to formulas alone. Nowadays, a broad series of fields falling under the name of data visualization—or data representation—have become among the pillars of any scientific or data related topic.</p>

<div class="figure-group" style="justify-content: center; align-items: flex-start; margin-top: 10px">
  <figure>
    <img src="https://pressbooks.pub/app/uploads/sites/24882/2025/08/histogram_1-300x191.png" alt="" width="300" height="191" />
    <figcaption>Fig. 1.5 (a) Violin representation of symmetric Gaussian</figcaption>
  </figure>
  <figure>
    <img src="https://pressbooks.pub/app/uploads/sites/24882/2025/08/histogram_2-300x191.png" alt="" width="300" height="191" />
    <figcaption>Fig. 1.5 (b) Violin representation of skewed Gaussian</figcaption>
  </figure>
</div>

<p class="indent" style="text-align: justify; text-indent: 2em">The <em>histogram</em> is found among the oldest and most fundamental visualization tools. The concept of dividing data into intervals to visualize frequency dates back to Karl Pearson in the late 19<sup>th</sup> century, who formalized it as a graphical representation of probability distributions \cite{pearson1892}. A histogram divides the range of a dataset into consecutive intervals, or <em>bins</em>, and represents the amount—or relative <em>frequency</em>—of observations falling within each bin as the height of a bar. This simple yet powerful plot provides an immediate visual impression of the dataset’s distribution, allowing one to identify symmetry, skewness, concentration of values, and potential gaps. For example, a symmetric histogram, like the one in LHS of Figure \ref{fig:histogram_comparison1} suggests a roughly balanced distribution around the mean, while a right-skewed histogram, like the one displayed in the RHS of Figure \ref{fig:histogram_comparison1}, indicates that higher values are less frequent—or less <em>probable</em>—but can yet influence measures like the mean. This is the state of the art in physical sciences and whenever observations are supposed to fit a mathematical prediction.</p>

<div class="figure-group" style="justify-content: center; align-items: flex-start; margin-top: 10px">
  <figure>
    <img src="https://pressbooks.pub/app/uploads/sites/24882/2025/08/histogram1_bins_small-300x191.png" alt="" width="300" height="191" />
    <figcaption>Fig. 1.5 (a) Violin representation of symmetric Gaussian</figcaption>
  </figure>
  <figure>
    <img src="https://pressbooks.pub/app/uploads/sites/24882/2025/08/histogram2_bins_small-300x191.png" alt="" width="300" height="191" />
    <figcaption>Fig. 1.5 (b) Violin representation of skewed Gaussian</figcaption>
  </figure>
</div>

<p class="indent" style="text-align: justify; text-indent: 2em">Building a histogram in an informative way is extremely powerful, and there are some subtleties to consider. As a rule of thumb, look for natural divisions in the data, and keep all bins the same size, covering the whole range under study. Outliers can skew, so they must be treated carefully. Figures \ref{fig:histogram_comparison2} and \ref{fig:histogram_comparison3} show how the binning size can affect the distribution of data. Smaller binning leads to more resolution but can be easily distorted in the presence of outliers, while few large bins are robust agains though losing the accuracy in resolution. For skewed distributions it is normally better to use the median and the IQR.</p>

<div class="figure-group" style="justify-content: center; align-items: flex-start; margin-top: 10px">
  <figure>
    <img src="https://pressbooks.pub/app/uploads/sites/24882/2025/08/histogram1_bins_large-300x191.png" alt="" width="300" height="191" />
    <figcaption>Fig. 1.5 (a) Violin representation of symmetric Gaussian</figcaption>
  </figure>
  <figure>
    <img src="https://pressbooks.pub/app/uploads/sites/24882/2025/08/histogram2_bins_large-300x191.png" alt="" width="300" height="191" />
    <figcaption>Fig. 1.5 (b) Violin representation of skewed Gaussian</figcaption>
  </figure>
</div>

<p class="indent" style="text-align: justify; text-indent: 2em">The box plot, also known as the <em>box-and-whisker</em> plot, was introduced by John Tukey in 1970 as part of his work on exploratory data analysis \cite{tukey1977}. The box plot offers a compact summary of a dataset’s central tendency, spread, and potential outliers. Constructed from five key statistics—the minimum, first quartile (\(Q_1\)), median (\(Q_2\)), third quartile (\(Q_3\)), and maximum—it clearly shows the <em>interquartile range</em> (IQR = \(Q_3 - Q_1\)) and highlights points that fall outside 1.5 times the IQR as outliers. This representation allows for quick comparisons across multiple groups, and it is particularly useful for detecting asymmetry and variability without being overly influenced by extreme values. It is widely used in biological and clinical sciences where an experiment can be repeated many times with relatively small sample sizes. Figure \ref{fig:box_comparison} displays the same data represented as histograms in Figures \ref{fig:histogram_comparison1} as box plots.</p>

<div class="figure-group" style="justify-content: center; align-items: flex-start; margin-top: 10px">
  <figure>
    <img src="https://pressbooks.pub/app/uploads/sites/24882/2025/08/box_1-300x191.png" alt="" width="300" height="191" />
    <figcaption>Fig. 1.5 (a) Violin representation of symmetric Gaussian</figcaption>
  </figure>
  <figure>
    <img src="https://pressbooks.pub/app/uploads/sites/24882/2025/08/box_2-300x191.png" alt="" width="300" height="191" />
    <figcaption>Fig. 1.5 (b) Violin representation of skewed Gaussian</figcaption>
  </figure>
</div>

<p class="indent" style="text-align: justify; text-indent: 2em">Finally, the <em>violin</em> plot is a more recent innovation, combining the box plot with a series of mathematical tools that represent as well the shape of the distribution. While the precise origin is less formally documented, it gained prominence in the late 20th century in statistical software environments, such as R, during the 1990s \cite{hintze1997}. Essentially, the violin plot extends the concept of the box plot by combining it with a kernel density estimate of the data. This plot not only displays the median and quartiles but also provides a smooth depiction of the distribution’s shape, revealing features such as multimodality or skewness that might be obscured in a simple box plot. By showing both summary statistics and the underlying density, the violin plot gives a richer, more nuanced view of the dataset, particularly when comparing several groups side by side. As an example of such comparison, see Figure [...].</p>

<div class="figure-group" style="justify-content: center; align-items: flex-start; margin-top: 10px">
  <figure>
    <img src="https://pressbooks.pub/app/uploads/sites/24882/2025/08/violin_1-300x191.png" alt="" width="300" height="191" />
    <figcaption>Fig. 1.5 (a) Violin representation of symmetric Gaussian</figcaption>
  </figure>
  <figure>
    <img src="https://pressbooks.pub/app/uploads/sites/24882/2025/08/violin_2-300x191.png" alt="" width="300" height="191" />
    <figcaption>Fig. 1.5 (b) Violin representation of skewed Gaussian</figcaption>
  </figure>
</div>

<p class="indent" style="text-align: justify; text-indent: 2em">To conclude, let us emphasize that graphs and diagrams are not mere decoration but deeply useful instruments of analysis. They allow patterns to leap from obscurity in shallow data, invite hypotheses, and sometimes contradict assumptions and expectation. In practice, visualization is both a beginning and a test: a first impression of data, and a final check on the reasonableness of results derived through calculation.</p>

<!-- 1.4. Dependency and correlation -->
<h3>1.4 Dependency and correlation</h3>
<p class="indent" style="text-align: justify; text-indent: 2em">Data rarely lives in isolation. Often, even in the simplest case, one variable depends upon another. Rainfall influences crop yields, study hours affect exam results, in the same way the brightness of a star relates to its temperature, and atmospheric carbon levels affect global temperatures, just to list some examples. Hence, recognizing and describing such dependencies lies at the heart of descriptive statistics and prepares the way for predictive models.</p>

<div style="justify-content: center;align-items: flex-start;gap: 20px;margin-top: 10px">
<figure style="text-align: center"><img src="https://pressbooks.pub/app/uploads/sites/24882/2025/08/mean_std_hist-300x191.png" alt="" width="300" height="191" /><figcaption>Fig. 1.6 (a) Histogram of three samples drawn from a Gaussian distribution</figcaption></figure>
<figure style="text-align: center"><img src="https://pressbooks.pub/app/uploads/sites/24882/2025/08/mean_std_box-300x191.png" alt="" width="300" height="191" /><figcaption>Fig. 1.6 (b) Box plot of three samples drawn from a Gaussian distribution</figcaption></figure>
<figure style="text-align: center"><img src="https://pressbooks.pub/app/uploads/sites/24882/2025/08/mean_std_violin-300x191.png" alt="" width="300" height="191" /><figcaption>Fig. 1.6 (c) Violin of three samples drawn from a Gaussian distribution</figcaption></figure>
</div>

<p class="indent" style="text-align: justify; text-indent: 2em">The simplest and most widely studied form of dependency is \textit{linearity}. When one variable $y$ tends to increase in proportion to another $x$, the relation can be sketched as a straight line. In the language of calculus - also called some times \textit{analysis}, or \textit{regression} - the best-fitting line is expressed as
\[
y(x) = a x + b \; ,
\]

<p class="indent" style="text-align: justify; text-indent: 2em">The idea of correlation is a fundamental measure of association between two random variables, quantifying how strongly they vary together. The most widely used mathematical description is the \textit{Pearson correlation coefficient}, introduced by Karl Pearson in the 1890s, which is built upon the idea of covariance normalized by variability. For two random variables $x$ and $y$, the population correlation $\rho_{x, y}$ is defined as </p>
\[
y(x) = a x^2 + b x + c \; ,
\]
